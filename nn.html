<!doctype html>
<html class="no-js">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><link rel="index" title="Index" href="genindex.html" /><link rel="search" title="Search" href="search.html" /><link rel="next" title="oneflow.nn.functional" href="functional.html" /><link rel="prev" title="oneflow.Tensor" href="tensor.html" />

    <meta name="generator" content="sphinx-3.5.4, furo 2021.04.11.beta34"/>
        <title>oneflow.nn - OneFlow documentation</title>
      <link rel="stylesheet" href="_static/styles/furo.css?digest=59ab60ac09ea94ccfe6deddff6d715cce948a6fc">
    <link rel="stylesheet" href="_static/pygments.css">
    <link media="(prefers-color-scheme: dark)" rel="stylesheet" href="_static/pygments_dark.css">
    


<style>
  :root {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media (prefers-color-scheme: dark) {
    :root {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
  }

  /* For allowing end-user-specific overrides */
  .override-light {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  .override-dark {
    --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
  }
</style><link rel="stylesheet" href="_static/styles/furo-extensions.css?digest=d391b54134226e4196576da3bdb6dddb7e05ba2b"></head>
  <body dir="">
    
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
      stroke-width="1.5" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z"/>
      <line x1="4" y1="6" x2="20" y2="6" />
      <line x1="10" y1="12" x2="20" y2="12" />
      <line x1="6" y1="18" x2="20" y2="18" />
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
      stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
      class="feather feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
      stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
      class="feather feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="index.html"><div class="brand">OneFlow  documentation</div></a>
    </div>
    <div class="header-right">
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="index.html">
  
  
  <span class="sidebar-brand-text">OneFlow  documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="search.html">
  <input class="sidebar-search" placeholder=Search name="q">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul>
<li class="toctree-l1"><a class="reference internal" href="troubleshooting.html">Troubleshooting</a></li>
</ul>
<p class="caption"><span class="caption-text">OneFlow Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="oneflow.html">oneflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">oneflow.Tensor</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">oneflow.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="functional.html">oneflow.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="module.html">oneflow.nn.Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="graph.html">oneflow.nn.Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">oneflow.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="image.html">oneflow.nn.image</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">oneflow.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">oneflow.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">oneflow.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">oneflow.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="comm.html">oneflow.comm</a></li>
<li class="toctree-l1"><a class="reference internal" href="placement.html">oneflow.placement</a></li>
<li class="toctree-l1"><a class="reference internal" href="sbp.html">oneflow.sbp.sbp</a></li>
</ul>

</div>
</div>
      </div>
      
    </div>
  </aside>
  <main class="main">
    <div class="content">
      <article role="main">
        <label class="toc-overlay-icon toc-content-icon" for="__toc">
          <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
        </label>
        <div class="section" id="oneflow-nn">
<h1>oneflow.nn<a class="headerlink" href="#oneflow-nn" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-oneflow.nn">
<span id="operators-for-neural-networks"></span><h2>Operators for neural networks<a class="headerlink" href="#module-oneflow.nn" title="Permalink to this headline">¶</a></h2>
<p>Copyright 2020 The OneFlow Authors. All rights reserved.</p>
<p>Licensed under the Apache License, Version 2.0 (the “License”);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at</p>
<blockquote>
<div><p><a class="reference external" href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></p>
</div></blockquote>
<p>Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an “AS IS” BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.</p>
<dl class="py class">
<dt id="oneflow.nn.AdaptiveAvgPool1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">AdaptiveAvgPool1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.AdaptiveAvgPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个平面组成的输入信号 <cite>input</cite> 上应用 1D 自适应平均池化。</p>
<p>对于任何大小的输入，输出大小都是 H。</p>
<p>输出的数量等于输入平面的数量。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>output_size</strong> (Int64List[1]): 目标输出大小 H（单个整数）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 64, 5])</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.AdaptiveAvgPool2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">AdaptiveAvgPool2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.AdaptiveAvgPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个平面组成的的信号 <cite>input</cite> 上应用 2D 自适应平均池化。</p>
<p>对于任何大小的输入，输出大小都是 H x W 。</p>
<p>输出的数量等于输入平面的数量。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>output_size</strong> (Int64List[2]): 目标输出大小（单个整数 H 或包含两个整数的元组 (H, W) ）。 H 和 W 可以是 <code class="docutils literal notranslate"><span class="pre">int</span></code> 也可以是 <code class="docutils literal notranslate"><span class="pre">None</span></code> ，如果为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 则大小将和输入大小一致。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 64, 5, 7])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 64, 7, 7])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="bp">None</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 64, 10, 7])</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.AdaptiveAvgPool3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">AdaptiveAvgPool3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.AdaptiveAvgPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个平面组成的信号 <cite>input</cite> 上应用 3D 自适应平均池化。</p>
<p>对于任何大小的输入，输出大小都是 D x H x W 。</p>
<p>输出的数量等于输入平面的数量。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>output_size</strong> (Int64List[3]): 目标输出大小（单个整数 D 则为 D x D x D 或包含三个整数的元组 (D, H, W) ）。 H 、 W 和 D 可以是 <code class="docutils literal notranslate"><span class="pre">int</span></code> 也可以是 <code class="docutils literal notranslate"><span class="pre">None</span></code> ，如果为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 则大小将和输入大小一致。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool3d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">9</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 64, 5, 7, 9])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool3d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 64, 7, 7, 7])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool3d</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 64, 7, 9, 8])</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.AvgPool1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">AvgPool1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">count_include_pad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.AvgPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个平面组成的信号 <cite>input</cite> 上执行 1D 平均池化。
在最简单的情况下，输出值是输入大小为 <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> 的层。
输出 <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> 和 <cite>kernel_size</cite> ， <span class="math notranslate nohighlight">\(k\)</span> 可以被精确地描述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out(N_i, C_j, l)  = \frac{1}{k} \sum_{m=0}^{k-1}
                    input(N_i, C_j, stride[0] \times h + m, stride*l + m)\]</div></div>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 非零，则输入在两侧隐式填充 0 以填充点数。</p>
<p>参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 可以为 int 或者单元素元组。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>当 <code class="xref py py-attr docutils literal notranslate"><span class="pre">ceil_mode</span></code> 为 True 时，如果滑动窗口在 left padding 或输入内开始，则允许滑动窗口出界。忽略在右侧填充区域开始的滑动窗口。</p>
</div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>kernel_size</strong> (Union[int, Tuple[int, int]]): 窗口的大小</p></li>
<li><p><strong>strides</strong> (Union[int, Tuple[int, int]], 可选): 窗口的 stride 。默认值为 None</p></li>
<li><p><strong>padding</strong> (Union[int, Tuple[int, int]]): 如果非 0 ，在两侧添加隐式填充 0 。默认为 0</p></li>
<li><p><strong>ceil_mode</strong> (bool): 如果为 True ，将使用 ceil 而不是 floor 来计算输出形状。默认为 False</p></li>
<li><p><strong>count_include_pad</strong> (bool): 如果为 True ，将在平均计算中填充 0 ，默认为 True</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">AvgPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="n">oneflow</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.AvgPool1d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.AvgPool1d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.AvgPool2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">AvgPool2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">count_include_pad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">divisor_override</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.AvgPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个平面组成的信号 <cite>input</cite> 上执行 2D 平均池化。</p>
<p>在最简单的情况下，输出值是输入大小为 <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> 的层。</p>
<p>输出 <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> 和 <cite>kernel_size</cite> ， <span class="math notranslate nohighlight">\((kH, kW)\)</span> 可以被精确地描述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out(N_i, C_j, h, w)  = \frac{1}{kH * kW} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1}
                       input(N_i, C_j, stride[0] \times h + m, stride[1] \times w + n)\]</div></div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>kernel_size</strong> (Union[int, Tuple[int, int]]): 整数或长度为 1 或 2 的整数列表。输入张量的每个维度的窗口大小</p></li>
<li><p><strong>strides</strong> (Union[int, Tuple[int, int]]): 整数或长度为 1 或 2 的整数列表。输入张量的每个维度的滑动窗口的 stride 。默认为 None</p></li>
<li><p><strong>padding</strong> (Tuple[int, int]): 整数或长度为 1 或 2 的整数列表。在两侧添加隐式填充 0 。默认为 0</p></li>
<li><p><strong>ceil_mode</strong> (bool, default to False): 如果为 True 。将使用 ceil 而不是 floor 来计算输出形状。默认为 False</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="n">oneflow</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.AvgPool2d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.AvgPool2d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.AvgPool3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">AvgPool3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">count_include_pad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">divisor_override</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.AvgPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个平面组成的信号 <cite>input</cite> 上执行 3D 平均池化。在最简单的情况下，输出值是输入大小为 <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span> 的层。</p>
<p>输出 <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span> 和 <cite>kernel_size</cite> ， <span class="math notranslate nohighlight">\((kD, kH, kW)\)</span> 可以被精确地描述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out(N_i, C_j, d, h, w)  = \frac{1}{kD * kH * kW } \sum_{k=0}^{kD-1} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1}
                       input(N_i, C_j, stride[0] \times d + k, stride[1] \times h + m, stride[2] \times w + n)\]</div></div>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 非零，则输入在三侧隐式填充 0 以填充点数。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>当 <code class="xref py py-attr docutils literal notranslate"><span class="pre">ceil_mode</span></code> 为 True 时，如果滑动窗口在 left padding 或输入内开始，则允许滑动窗口出界。忽略在右侧填充区域开始的滑动窗口。</p>
</div>
<dl>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>kernel_size</strong> (Union[int, Tuple[int, int, int]]): 窗口的大小</p></li>
<li><p><strong>strides</strong> (Union[int, Tuple[int, int, int]], 可选): 窗口的 stride 。默认值为 None</p></li>
<li><p><strong>padding</strong> (Union[int, Tuple[int, int, int]]):  如果非 0 ，在三侧添加隐式填充 0 。默认为 0</p></li>
<li><p><strong>ceil_mode</strong> (bool): 如果为 True ，将使用 ceil 而不是 floor 来计算输出形状。默认为 False</p></li>
<li><p><strong>count_include_pad</strong> (bool): 如果为 True ，将在平均计算中填充 0 ，默认为 True</p></li>
<li><p><strong>divisor_override</strong> (int): 如果设定了 attr:<cite>divisor_override</cite> ，它将用作除数，否则 attr:<cite>kernel_size</cite> 将作为除数。默认为 0</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span></p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{kernel_size}[0]}{\text{stride}[0]} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{kernel_size}[1]}{\text{stride}[1]} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{kernel_size}[2]}{\text{stride}[2]} + 1\right\rfloor\]</div></div>
</li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">AvgPool3d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="n">oneflow</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">19</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.AvgPool3d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.AvgPool3d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.BCELoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">BCELoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span> → <span class="pre">Tensor</span><a class="headerlink" href="#oneflow.nn.BCELoss" title="Permalink to this definition">¶</a></dt>
<dd><p>计算二值交叉熵损失 (binary cross-entropy loss)。</p>
<p>公式为：</p>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = “none” ：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = -(Target_i*log(Input_i) + (1-Target_i)*log(1-Input_i))\]</div></div>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = “mean”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = -\frac{1}{n}\sum_{i=1}^n(Target_i*log(Input_i) + (1-Target_i)*log(1-Input_i))\]</div></div>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = “sum”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = -\sum_{i=1}^n(Target_i*log(Input_i) + (1-Target_i)*log(1-Input_i))\]</div></div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>weight</strong> (oneflow.Tensor, 可选的): 手动重新调整损失的权重。默认为 <code class="docutils literal notranslate"><span class="pre">None</span></code> ，对应的权重值为 1</p></li>
<li><p><strong>reduction</strong> (str, 可选的): reduce 的方式，可以是 “none” 、 “mean” 、 “sum” 中的一种。默认为 “mean”</p></li>
</ul>
</dd>
</dl>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>输入值必须在区间 (0, 1) 内。否则此损失函数可能返回 <cite>nan</cite> 值。</p>
</div>
<dl class="simple">
<dt>返回类型：</dt><dd><p>oneflow.tensor</p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">activation</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sigmoid_input</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">sigmoid_input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[2.9266, 1.1963, 1.1087],</span>
<span class="go">        [0.8064, 2.0750, 4.2539]], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m_sum</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m_sum</span><span class="p">(</span><span class="n">sigmoid_input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(12.3668, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m_mean</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m_mean</span><span class="p">(</span><span class="n">sigmoid_input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(2.0611, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m_none</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m_none</span><span class="p">(</span><span class="n">sigmoid_input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(1.0306, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.BCEWithLogitsLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">BCEWithLogitsLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> → <span class="pre">Tensor</span><a class="headerlink" href="#oneflow.nn.BCEWithLogitsLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>此运算将 <cite>Sigmoid</cite> 和 <cite>BCELoss</cite> 组合在一起。为了数据的稳定性，我们用了一些数学技巧，而不是将 <cite>Sigmoid</cite> 作用于 <cite>BCELoss</cite> 层。</p>
<p>公式为：</p>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = <code class="docutils literal notranslate"><span class="pre">"none"</span></code>:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = -weight*[Pos\_weight*y*log\sigma({x}) + (1-y)*log(1-\sigma(x))]\]</div></div>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = <code class="docutils literal notranslate"><span class="pre">"mean"</span></code>:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = -\frac{weight}{n}\sum_{i=1}^n[Pos\_weight*y*log\sigma({x}) + (1-y)*log(1-\sigma(x))]\]</div></div>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = <code class="docutils literal notranslate"><span class="pre">"sum"</span></code>:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = -weight*\sum_{i=1}^n[Pos\_weight*y*log\sigma({x}) + (1-y)*log(1-\sigma(x))]\]</div></div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>weight</strong> (Tensor, 可选的): 手动重新调整损失的权重。默认为 <code class="docutils literal notranslate"><span class="pre">None</span></code></p></li>
<li><p><strong>reduction</strong> (str, 可选的): reduce 的方式，可以是 <code class="docutils literal notranslate"><span class="pre">"none"</span></code> 、 <code class="docutils literal notranslate"><span class="pre">"mean"</span></code> 、 <code class="docutils literal notranslate"><span class="pre">"sum"</span></code> 中的一种。默认为 “mean” 。如果为 <code class="docutils literal notranslate"><span class="pre">'none'</span></code> 则不进行 reduce 。如果为 <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> ，输出的值的和除以元素数。如果为 <code class="docutils literal notranslate"><span class="pre">'sum'</span></code> ，输出将被求和。默认为 <code class="docutils literal notranslate"><span class="pre">"mean"</span></code></p></li>
<li><p><strong>pos_weight</strong> (Tensor, 可选的): 手动重新调整正例的权重。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N,*)\)</span> 其中 <cite>*</cite> 的意思是，可以增加任意维度</p></li>
<li><p><strong>Target</strong> : <span class="math notranslate nohighlight">\((N,*)\)</span> 与输入形状一样</p></li>
<li><p><strong>Output</strong> : 标量。如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> 为 <code class="docutils literal notranslate"><span class="pre">"none"</span></code> ，则 <span class="math notranslate nohighlight">\((N,*)\)</span> 和输入形状一样</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pos_weight</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">pos_weight</span><span class="o">=</span><span class="n">pos_weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[2.9266, 1.5552, 1.1087],</span>
<span class="go">        [0.9676, 2.0750, 5.9554],</span>
<span class="go">        [0.9676, 2.0750, 5.9554]], dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">pos_weight</span><span class="o">=</span><span class="n">pos_weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(2.6207, dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">pos_weight</span><span class="o">=</span><span class="n">pos_weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(23.5865, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.BatchNorm1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">BatchNorm1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.BatchNorm1d" title="Permalink to this definition">¶</a></dt>
<dd><p>在 2D 或 3D 输入（具有可选附加通道维度的小批量 1D 输入）上应用批归一化 (Batch Normalization) 。行为与论文 <a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift</a> 一致。</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>按小批量逐维度求平均值和标准差， <span class="math notranslate nohighlight">\(\gamma\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span> 是大小为 <cite>C</cite> 的可学习参数向量（ <cite>C</cite> 是输入的大小）。
默认情况下，<span class="math notranslate nohighlight">\(\gamma\)</span> 的元素均为 1 而 <span class="math notranslate nohighlight">\(\beta\)</span> 的元素均为 0 。标准差的计算等价于 <cite>torch.var(input, unbiased=False)</cite> 。</p>
<p>此外，默认情况下，在训练期间，该层不断估计计算的均值和方差，然后评估时将其归一化。运行估计默认 <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 为 0.1 。</p>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> 被设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> ，则该层不会继续进行估计，并且在评估时也使用批处理统计信息。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 参数不同于优化器 (optimizer) 类中使用的参数或传统的动量概念。数学上，这里的更新规则是：
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span> ，其中 <span class="math notranslate nohighlight">\(\hat{x}\)</span> 是估计的统计量， <span class="math notranslate nohighlight">\(x_t\)</span> 是新的观察值。</p>
</div>
<p>因为批归一化 (Batch Normalization) 是在 <cite>C</cite> 维度上完成的，计算 <cite>(N, L)</cite> 切片的统计数据，所以常称其为 Temporal Batch Normalization 。</p>
<dl>
<dt>参数：</dt><dd><ul>
<li><p><strong>num_features</strong> : <span class="math notranslate nohighlight">\(C\)</span> 来自于大小为 <span class="math notranslate nohighlight">\((N, C, L)\)</span> 的预期输入或 <span class="math notranslate nohighlight">\(L\)</span> 来自大小为 <span class="math notranslate nohighlight">\((N, L)\)</span> 的输入</p></li>
<li><p><strong>eps</strong> : 为数值稳定性而为分母加的值。默认为：1e-5</p></li>
<li><p><strong>momentum</strong> : 用于 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 计算的值。设定为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 则计算移动平均 (Moving average) ，默认：0.1</p></li>
<li><p><strong>affine</strong> : 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，该模块具有可学习的仿射参数。默认为 <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><dl>
<dt><strong>track_running_stats</strong><span class="classifier">当设置为 <code class="docutils literal notranslate"><span class="pre">True</span></code> 时，该模块跟踪运行均值和方差，当设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> 时，此模块不会跟踪此类统计信息，</span></dt><dd><p>并将统计缓冲区 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 初始化为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 。当这些缓冲区为“无”时，此模块在训练和评估模式中始终使用批处理统计信息。默认值： <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C)\)</span> 或 <span class="math notranslate nohighlight">\((N, C, L)\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C)\)</span> 或 <span class="math notranslate nohighlight">\((N, C, L)\)</span> （与输入形状相同）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.BatchNorm2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">BatchNorm2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.BatchNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><p>在 4D 输入（具有可选附加通道维度的小批量 2D 输入）上应用批归一化 (Batch Normalization) 。行为与论文 <a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift</a> 一致。</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>按小批量逐维度求平均值和标准差， <span class="math notranslate nohighlight">\(\gamma\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span> 是大小为 <cite>C</cite> 的可学习参数向量（ <cite>C</cite> 是输入的大小）。
默认情况下，<span class="math notranslate nohighlight">\(\gamma\)</span> 的元素均为 1 而 <span class="math notranslate nohighlight">\(\beta\)</span> 的元素均为 0 。标准差的计算等价于 <cite>torch.var(input, unbiased=False)</cite> 。</p>
<p>此外，默认情况下，在训练期间，该层不断估计计算的均值和方差，然后评估时将其归一化。运行估计默认 <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 为 0.1 。</p>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> 被设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> ，则该层不会继续进行估计，并且在评估时也使用批处理统计信息。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 参数不同于优化器 (optimizer) 类中使用的参数或传统的动量概念。数学上，这里的更新规则是：
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span> ，其中 <span class="math notranslate nohighlight">\(\hat{x}\)</span> 是估计的统计量， <span class="math notranslate nohighlight">\(x_t\)</span> 是新的观察值。</p>
</div>
<p>因为批归一化 (Batch Normalization) 是在 <cite>C</cite> 维度上完成的，计算 <cite>(N, H, W)</cite> 切片的统计数据，所以常称其为 Spatial Batch Normalization 。</p>
<dl>
<dt>参数：</dt><dd><ul>
<li><p><strong>num_features</strong> : <span class="math notranslate nohighlight">\(C\)</span> 来自于大小为 <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> 的预期输入</p></li>
<li><p><strong>eps</strong> : 为数值稳定性而为分母加的值。默认为：1e-5</p></li>
<li><p><strong>momentum</strong> : 用于 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 计算的值。设定为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 则计算移动平均 (Moving average) ，默认：0.1</p></li>
<li><p><strong>affine</strong> : 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，该模块具有可学习的仿射参数。默认为 <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><dl>
<dt><strong>track_running_stats</strong><span class="classifier">当设置为 <code class="docutils literal notranslate"><span class="pre">True</span></code> 时，该模块跟踪运行均值和方差，当设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> 时，此模块不会跟踪此类统计信息，</span></dt><dd><p>并将统计缓冲区 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 初始化为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 。当这些缓冲区为“无”时，此模块在训练和评估模式中始终使用批处理统计信息。默认值： <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C, H, W)\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> （与输入形状相同）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.BatchNorm3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">BatchNorm3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.BatchNorm3d" title="Permalink to this definition">¶</a></dt>
<dd><p>在 5D 输入（具有可选附加通道维度的小批量 3D 输入）上应用批归一化 (Batch Normalization) 。行为与论文 <a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift</a> 一致。</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>按小批量逐维度求平均值和标准差， <span class="math notranslate nohighlight">\(\gamma\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span> 是大小为 <cite>C</cite> 的可学习参数向量（ <cite>C</cite> 是输入的大小）。
默认情况下，<span class="math notranslate nohighlight">\(\gamma\)</span> 的元素均为 1 而 <span class="math notranslate nohighlight">\(\beta\)</span> 的元素均为 0 。标准差的计算等价于 <cite>torch.var(input, unbiased=False)</cite> 。</p>
<p>此外，默认情况下，在训练期间，该层不断估计计算的均值和方差，然后评估时将其归一化。运行估计默认 <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 为 0.1 。</p>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> 被设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> ，则该层不会继续进行估计，并且在评估时也使用批处理统计信息。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 参数不同于优化器 (optimizer) 类中使用的参数或传统的动量概念。数学上，这里的更新规则是：
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span> ，其中 <span class="math notranslate nohighlight">\(\hat{x}\)</span> 是估计的统计量， <span class="math notranslate nohighlight">\(x_t\)</span> 是新的观察值。</p>
</div>
<p>因为批归一化 (Batch Normalization) 是在 <cite>C</cite> 维度上完成的，计算 <cite>(N, H, W)</cite> 切片的统计数据，所以常称其为 Spatial Batch Normalization 。</p>
<dl>
<dt>参数：</dt><dd><ul>
<li><p><strong>num_features</strong> : <span class="math notranslate nohighlight">\(C\)</span> 来自于大小为 <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span> 的预期输入</p></li>
<li><p><strong>eps</strong> : 为数值稳定性而为分母加的值。默认为：1e-5</p></li>
<li><p><strong>momentum</strong> : 用于 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 计算的值。设定为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 则计算移动平均 (Moving average) ，默认：0.1</p></li>
<li><p><strong>affine</strong> : 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，该模块具有可学习的仿射参数。默认为 <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><dl>
<dt><strong>track_running_stats</strong><span class="classifier">当设置为 <code class="docutils literal notranslate"><span class="pre">True</span></code> 时，该模块跟踪运行均值和方差，当设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> 时，此模块不会跟踪此类统计信息，</span></dt><dd><p>并将统计缓冲区 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 初始化为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 。当这些缓冲区为“无”时，此模块在训练和评估模式中始终使用批处理统计信息。默认值： <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span> （与输入形状相同）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([3, 2, 5, 8, 4])</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.CELU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">CELU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.CELU" title="Permalink to this definition">¶</a></dt>
<dd><p>应用逐元素方程：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\text{CELU}(x, \alpha) = \begin{cases}
                        x &amp; \text{ if } x \ge 0  \\
        \alpha*(exp(\frac{x}{\alpha})-1) &amp; \text{ otherwise } \\
            \end{cases}\end{split}\]</div></div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>alpha</strong> (float): CELU 公式中的 <span class="math notranslate nohighlight">\(\alpha\)</span> 。默认值：1.0</p></li>
<li><p><strong>inplace</strong> (bool): 是否执行 place 操作。默认： <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N,*)\)</span> 其中 <cite>*</cite> 的意思是，可以增加任意维度</p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, *)\)</span>, 与输入相同</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">celu</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CELU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">celu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([-0.3161,  0.0000,  0.5000], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.CELU.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.CELU.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.COCOReader">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">COCOReader</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">annotation_file</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_dir</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_seed</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_by_aspect_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_images_without_annotations</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride_partition</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.device</span><span class="p"><span class="pre">,</span> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">placement</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.placement</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sbp</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.sbp.sbp</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.sbp.sbp</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.COCOReader" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.CTCLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">CTCLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">blank</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zero_infinity</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.CTCLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>The Connectionist Temporal Classification loss.
The interface is consistent with PyTorch.
The documentation is referenced from:
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html#torch.nn.CTCLoss">https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html#torch.nn.CTCLoss</a></p>
<p>Calculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the
probability of possible alignments of input to target, producing a loss value which is differentiable
with respect to each input node. The alignment of input to target is assumed to be “many-to-one”, which
limits the length of the target sequence such that it must be <span class="math notranslate nohighlight">\(\leq\)</span> the input length.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>blank</strong> (<em>int</em><em>, </em><em>optional</em>) – blank label. Default <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the output losses will be divided by the target lengths and
then the mean over the batch is taken. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
<li><p><strong>zero_infinity</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to zero infinite losses and the associated gradients.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>
Infinite losses mainly occur when the inputs are too short
to be aligned to the targets.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Log_probs: Tensor of size <span class="math notranslate nohighlight">\((T, N, C)\)</span>,
where <span class="math notranslate nohighlight">\(T = \text{input length}\)</span>,
<span class="math notranslate nohighlight">\(N = \text{batch size}\)</span>, and
<span class="math notranslate nohighlight">\(C = \text{number of classes (including blank)}\)</span>.</p></li>
<li><p>Targets: Tensor of size <span class="math notranslate nohighlight">\((N, S)\)</span> or
<span class="math notranslate nohighlight">\((\operatorname{sum}(\text{target_lengths}))\)</span>,
where <span class="math notranslate nohighlight">\(N = \text{batch size}\)</span> and
<span class="math notranslate nohighlight">\(S = \text{max target length, if shape is } (N, S)\)</span>.
It represent the target sequences. Each element in the target
sequence is a class index. And the target index cannot be blank (default=0).
In the <span class="math notranslate nohighlight">\((N, S)\)</span> form, targets are padded to the
length of the longest sequence, and stacked.
In the <span class="math notranslate nohighlight">\((\operatorname{sum}(\text{target_lengths}))\)</span> form,
the targets are assumed to be un-padded and
concatenated within 1 dimension.</p></li>
<li><p>Input_lengths: Tuple or tensor of size <span class="math notranslate nohighlight">\((N)\)</span>,
where <span class="math notranslate nohighlight">\(N = \text{batch size}\)</span>. It represent the lengths of the
inputs (must each be <span class="math notranslate nohighlight">\(\leq T\)</span>). And the lengths are specified
for each sequence to achieve masking under the assumption that sequences
are padded to equal lengths.</p></li>
<li><p>Target_lengths: Tuple or tensor of size <span class="math notranslate nohighlight">\((N)\)</span>,
where <span class="math notranslate nohighlight">\(N = \text{batch size}\)</span>. It represent lengths of the targets.
Lengths are specified for each sequence to achieve masking under the
assumption that sequences are padded to equal lengths. If target shape is
<span class="math notranslate nohighlight">\((N,S)\)</span>, target_lengths are effectively the stop index
<span class="math notranslate nohighlight">\(s_n\)</span> for each target sequence, such that <code class="docutils literal notranslate"><span class="pre">target_n</span> <span class="pre">=</span> <span class="pre">targets[n,0:s_n]</span></code> for
each target in a batch. Lengths must each be <span class="math notranslate nohighlight">\(\leq S\)</span>
If the targets are given as a 1d tensor that is the concatenation of individual
targets, the target_lengths must add up to the total length of the tensor.</p></li>
</ul>
</dd>
<dt>Reference:</dt><dd><p>A. Graves et al.: Connectionist Temporal Classification:
Labelling Unsegmented Sequence Data with Recurrent Neural Networks:
<a class="reference external" href="https://www.cs.toronto.edu/~graves/icml_2006.pdf">https://www.cs.toronto.edu/~graves/icml_2006.pdf</a></p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">log_probs</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>   <span class="p">[</span>
<span class="gp">... </span>       <span class="p">[[</span><span class="o">-</span><span class="mf">1.1031</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7998</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5200</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.9808</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1363</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1908</span><span class="p">]],</span>
<span class="gp">... </span>       <span class="p">[[</span><span class="o">-</span><span class="mf">1.2258</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0665</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0153</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.1135</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2331</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9671</span><span class="p">]],</span>
<span class="gp">... </span>       <span class="p">[[</span><span class="o">-</span><span class="mf">1.3348</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6611</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5118</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.9823</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2355</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0941</span><span class="p">]],</span>
<span class="gp">... </span>       <span class="p">[[</span><span class="o">-</span><span class="mf">1.3850</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3273</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7247</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.8235</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4783</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0994</span><span class="p">]],</span>
<span class="gp">... </span>       <span class="p">[[</span><span class="o">-</span><span class="mf">0.9049</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8867</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6962</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.4938</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3630</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6547</span><span class="p">]],</span>
<span class="gp">... </span>   <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">targets</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_lengths</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_lengths</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_mean</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CTCLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">loss_mean</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(1.1376, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_sum</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CTCLoss</span><span class="p">(</span><span class="n">blank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">loss_sum</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(6.8257, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.CoinFlip">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">CoinFlip</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_seed</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">probability</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.device</span><span class="p"><span class="pre">,</span> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">placement</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.placement</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sbp</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.sbp.sbp</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.sbp.sbp</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.CoinFlip" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.CombinedMarginLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">CombinedMarginLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">m1</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">m2</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">m3</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.CombinedMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>The operation implements “margin_softmax” in InsightFace:
<a class="reference external" href="https://github.com/deepinsight/insightface/blob/master/recognition/arcface_mxnet/train.py">https://github.com/deepinsight/insightface/blob/master/recognition/arcface_mxnet/train.py</a>
The implementation of margin_softmax in InsightFace is composed of multiple operators.
We fuse them for speed up.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor"><em>oneflow.Tensor</em></a>) – A Tensor</p></li>
<li><p><strong>label</strong> (<a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor"><em>oneflow.Tensor</em></a>) – label with integer data type</p></li>
<li><p><strong>m1</strong> (<em>float</em>) – loss m1 parameter</p></li>
<li><p><strong>m2</strong> (<em>float</em>) – loss m2 parameter</p></li>
<li><p><strong>m3</strong> (<em>float</em>) – loss m3 parameter</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Tensor</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">oneflow.Tensor</a></p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.7027179</span><span class="p">,</span> <span class="mf">0.0230609</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.02721931</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.16056311</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.4565852</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.64471215</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np_label</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np_x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np_label</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_func</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CombinedMarginLoss</span><span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[-0.0423,  0.0231],</span>
<span class="go">        [-0.0272,  0.1237],</span>
<span class="go">        [-0.4566, -0.0204]], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ConstantPad1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ConstantPad1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">tuple</span><span class="p"><span class="pre">,</span> </span><span class="pre">list</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ConstantPad1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor boundaries with a constant value.
The interface is consistent with PyTorch, and referenced from:
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad1d.html?highlight=constantpad1d#torch.nn.ConstantPad1d">https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad1d.html?highlight=constantpad1d#torch.nn.ConstantPad1d</a></p>
<p>For <cite>N</cite>-dimensional padding, use <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>list</em><em>, </em><em>tuple</em>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in both boundaries. If a 2-<cite>tuple</cite>, uses
(<span class="math notranslate nohighlight">\(\text{padding_left}\)</span>, <span class="math notranslate nohighlight">\(\text{padding_right}\)</span>)</p></li>
<li><p><strong>value</strong> (<em>int</em><em>, </em><em>float</em>) – The constant value used for padding. Defaults to 0.</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, W_{out})\)</span> where</p>
<p><span class="math notranslate nohighlight">\(W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}\)</span></p>
</li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad1d</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">value</span><span class="o">=</span><span class="mf">9.9999</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([[[9.9999, 0.0000, 1.0000, 9.9999, 9.9999],</span>
<span class="go">         [9.9999, 2.0000, 3.0000, 9.9999, 9.9999]],</span>

<span class="go">        [[9.9999, 4.0000, 5.0000, 9.9999, 9.9999],</span>
<span class="go">         [9.9999, 6.0000, 7.0000, 9.9999, 9.9999]]], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ConstantPad2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ConstantPad2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">tuple</span><span class="p"><span class="pre">,</span> </span><span class="pre">list</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ConstantPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from:
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad2d.html?highlight=constantpad2d#torch.nn.ConstantPad2d">https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad2d.html?highlight=constantpad2d#torch.nn.ConstantPad2d</a></p>
<p>This operator pads the input with constant value that user specifies.
User can set the amount of padding by setting the parameter <cite>paddings</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>tuple</em><em>, </em><em>list</em>) – the size of the padding.
If is <cite>int</cite>, uses the same padding in all boundaries.
If a 4-<cite>tuple</cite>, uses
(<span class="math notranslate nohighlight">\(\mathrm{padding_{left}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{right}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{top}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{bottom}}\)</span>)</p></li>
<li><p><strong>value</strong> (<em>int</em><em>, </em><em>float</em>) – The constant value used for padding. Defaults to 0.</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> where</p>
<p><span class="math notranslate nohighlight">\(H_{out} = H_{in} + \mathrm{padding_{top}} + \mathrm{padding_{bottom}}\)</span>
<span class="math notranslate nohighlight">\(W_{out} = W_{in} + \mathrm{padding_{left}} + \mathrm{padding_{right}}\)</span></p>
</li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad2d</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">18</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="go">oneflow.Size([1, 2, 5, 7])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([[[[ 1.,  1.,  1.,  1.,  1.,  1.,  1.],</span>
<span class="go">          [ 1.,  1.,  0.,  1.,  2.,  1.,  1.],</span>
<span class="go">          [ 1.,  1.,  3.,  4.,  5.,  1.,  1.],</span>
<span class="go">          [ 1.,  1.,  6.,  7.,  8.,  1.,  1.],</span>
<span class="go">          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.]],</span>

<span class="go">         [[ 1.,  1.,  1.,  1.,  1.,  1.,  1.],</span>
<span class="go">          [ 1.,  1.,  9., 10., 11.,  1.,  1.],</span>
<span class="go">          [ 1.,  1., 12., 13., 14.,  1.,  1.],</span>
<span class="go">          [ 1.,  1., 15., 16., 17.,  1.,  1.],</span>
<span class="go">          [ 1.,  1.,  1.,  1.,  1.,  1.,  1.]]]], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ConstantPad3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ConstantPad3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">tuple</span><span class="p"><span class="pre">,</span> </span><span class="pre">list</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ConstantPad3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor boundaries with a constant value.
The interface is consistent with PyTorch, and referenced from:
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad3d.html?highlight=constantpad3d#torch.nn.ConstantPad3d">https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad3d.html?highlight=constantpad3d#torch.nn.ConstantPad3d</a></p>
<p>For <cite>N</cite>-dimensional padding, use <code class="xref py py-func docutils literal notranslate"><span class="pre">flow.nn.functional.pad()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>list</em><em>, </em><em>tuple</em>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 6-<cite>tuple</cite>, uses
(<span class="math notranslate nohighlight">\(\text{padding_left}\)</span>, <span class="math notranslate nohighlight">\(\text{padding_right}\)</span>,
<span class="math notranslate nohighlight">\(\text{padding_top}\)</span>, <span class="math notranslate nohighlight">\(\text{padding_bottom}\)</span>,
<span class="math notranslate nohighlight">\(\text{padding_front}\)</span>, <span class="math notranslate nohighlight">\(\text{padding_back}\)</span>)</p></li>
<li><p><strong>value</strong> (<em>int</em><em>, </em><em>float</em>) – The constant value used for padding. Defaults to 0.</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span> where</p>
<p><span class="math notranslate nohighlight">\(D_{out} = D_{in} + \text{padding_front} + \text{padding_back}\)</span></p>
<p><span class="math notranslate nohighlight">\(H_{out} = H_{in} + \text{padding_top} + \text{padding_bottom}\)</span></p>
<p><span class="math notranslate nohighlight">\(W_{out} = W_{in} + \text{padding_left} + \text{padding_right}\)</span></p>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="k">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad3d</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([[[[[9, 9, 9, 9],</span>
<span class="go">           [9, 9, 9, 9],</span>
<span class="go">           [9, 9, 9, 9],</span>
<span class="go">           [9, 9, 9, 9]],</span>

<span class="go">          [[9, 9, 9, 9],</span>
<span class="go">           [9, 0, 1, 9],</span>
<span class="go">           [9, 2, 3, 9],</span>
<span class="go">           [9, 9, 9, 9]],</span>

<span class="go">          [[9, 9, 9, 9],</span>
<span class="go">           [9, 4, 5, 9],</span>
<span class="go">           [9, 6, 7, 9],</span>
<span class="go">           [9, 9, 9, 9]],</span>

<span class="go">          [[9, 9, 9, 9],</span>
<span class="go">           [9, 9, 9, 9],</span>
<span class="go">           [9, 9, 9, 9],</span>
<span class="go">           [9, 9, 9, 9]]]]], dtype=oneflow.int32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Conv1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Conv1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Conv1d" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from: <a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Conv1d.html#conv1d">https://pytorch.org/docs/master/generated/torch.nn.Conv1d.html#conv1d</a></p>
<p>Applies a 1D convolution over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size
<span class="math notranslate nohighlight">\((N, C_{\text{in}}, L)\)</span> and output <span class="math notranslate nohighlight">\((N, C_{\text{out}}, L_{\text{out}})\)</span> can be
precisely described as:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +
\sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{\text{out}_j}, k)
\star \text{input}(N_i, k)\]</div></div>
<p>where <span class="math notranslate nohighlight">\(\star\)</span> is the valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator,
<span class="math notranslate nohighlight">\(N\)</span> is a batch size, <span class="math notranslate nohighlight">\(C\)</span> denotes a number of channels,
<span class="math notranslate nohighlight">\(L\)</span> is a length of signal sequence.</p>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation, a single
number or a one-element tuple.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of padding applied to the input. It
can be either a string {{‘valid’, ‘same’}} or a tuple of ints giving the
amount of implicit padding applied on both sides.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also
known as the à trous algorithm. It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a>
has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">padding='valid'</span></code> is the same as no padding. <code class="docutils literal notranslate"><span class="pre">padding='same'</span></code> pads
the input so the output has the shape as the input. However, this mode
doesn’t support any stride values other than 1.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>tuple</em><em> or </em><em>str</em><em>, </em><em>optional</em>) – Padding added to both sides of
the input. Default: 0</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code></p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Spacing between kernel
elements. Default: 1</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of blocked connections from input
channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the
output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C_{in}, L_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C_{out}, L_{out})\)</span> where</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[L_{out} = \left\lfloor\frac{L_{in} + 2 \times \text{padding} - \text{dilation}
          \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor\]</div></div>
</li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="oneflow.nn.Conv1d.weight">
<code class="sig-name descname"><span class="pre">weight</span></code><a class="headerlink" href="#oneflow.nn.Conv1d.weight" title="Permalink to this definition">¶</a></dt>
<dd><p>the learnable weights of the module of shape
<span class="math notranslate nohighlight">\((\text{out\_channels},
\frac{\text{in\_channels}}{\text{groups}}, \text{kernel\_size})\)</span>.
The values of these weights are sampled from
<span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{in} * \text{kernel\_size}}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.Conv1d.bias">
<code class="sig-name descname"><span class="pre">bias</span></code><a class="headerlink" href="#oneflow.nn.Conv1d.bias" title="Permalink to this definition">¶</a></dt>
<dd><p>the learnable bias of the module of shape
(out_channels). If <a class="reference internal" href="#oneflow.nn.Conv1d.bias" title="oneflow.nn.Conv1d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the values of these weights are
sampled from <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{in} * \text{kernel\_size}}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Conv1d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Conv1d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
<dl class="py method">
<dt id="oneflow.nn.Conv1d.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.Conv1d.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Conv2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Conv2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Conv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from: <a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html#conv2d">https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html#conv2d</a></p>
<p>Applies a 2D convolution over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size
<span class="math notranslate nohighlight">\((N, C_{\text{in}}, H, W)\)</span> and output <span class="math notranslate nohighlight">\((N, C_{\text{out}}, H_{\text{out}}, W_{\text{out}})\)</span>
can be precisely described as:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +
\sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)\]</div></div>
<p>where <span class="math notranslate nohighlight">\(\star\)</span> is the valid 2D <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator,
<span class="math notranslate nohighlight">\(N\)</span> is a batch size, <span class="math notranslate nohighlight">\(C\)</span> denotes a number of channels,
<span class="math notranslate nohighlight">\(H\)</span> is a height of input planes in pixels, and <span class="math notranslate nohighlight">\(W\)</span> is
width in pixels.</p>
<ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation, a single
number or a tuple.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit padding on both
sides for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points for each dimension.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also
known as the à trous algorithm. It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a>
has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>. For example,</p>
<blockquote>
<div><ul class="simple">
<li><p>At groups=1, all inputs are convolved to all outputs.</p></li>
<li><p>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels
and producing half the output channels, and both subsequently
concatenated.</p></li>
<li><p>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters (of size
<span class="math notranslate nohighlight">\(\frac{\text{out_channels}}{\text{in_channels}}\)</span>).,</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimension</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When <cite>groups == in_channels</cite> and <cite>out_channels == K * in_channels</cite>,
where <cite>K</cite> is a positive integer, this operation is also known as a “depthwise convolution”.</p>
<p>In other words, for an input of size <span class="math notranslate nohighlight">\((N, C_{in}, L_{in})\)</span>,
a depthwise convolution with a depthwise multiplier <cite>K</cite> can be performed with the arguments
<span class="math notranslate nohighlight">\((C_\text{in}=C_\text{in}, C_\text{out}=C_\text{in} \times \text{K}, ..., \text{groups}=C_\text{in})\)</span>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Zero-padding added to both sides of
the input. Default: 0</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code></p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of blocked connections from input
channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the
output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span> where</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] - \text{dilation}[0]
          \times (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] - \text{dilation}[1]
          \times (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor\]</div></div>
</li>
</ul>
</dd>
<dt>Attr:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>weight (Tensor): the learnable weights of the module of shape</dt><dd><p><span class="math notranslate nohighlight">\((\text{out_channels}, \frac{\text{in_channels}}{\text{groups}},\)</span>
<span class="math notranslate nohighlight">\(\text{kernel_size[0]}, \text{kernel_size[1]})\)</span>.
The values of these weights are sampled from
<span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{in} * \prod_{i=0}^{1}\text{kernel_size}[i]}\)</span></p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>bias (Tensor):   the learnable bias of the module of shape</dt><dd><p>(out_channels). If <code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>,
then the values of these weights are
sampled from <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{in} * \prod_{i=0}^{1}\text{kernel_size}[i]}\)</span></p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Conv2d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Conv2d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
<dl class="py method">
<dt id="oneflow.nn.Conv2d.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.Conv2d.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Conv3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Conv3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Conv3d" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from: <a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Conv3d.html#conv3d">https://pytorch.org/docs/master/generated/torch.nn.Conv3d.html#conv3d</a></p>
<p>Applies a 3D convolution over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math notranslate nohighlight">\((N, C_{in}, D, H, W)\)</span>
and output <span class="math notranslate nohighlight">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> can be precisely described as:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out(N_i, C_{out_j}) = bias(C_{out_j}) +
                        \sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \star input(N_i, k)\]</div></div>
<p>where <span class="math notranslate nohighlight">\(\star\)</span> is the valid 3D <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator</p>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of padding applied to the input. It
can be either a string {{‘valid’, ‘same’}} or a tuple of ints giving the
amount of implicit padding applied on both sides.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p></li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the depth, height and width dimension</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of three ints – in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the height dimension and the third <cite>int</cite> for the width dimension</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">padding='valid'</span></code> is the same as no padding. <code class="docutils literal notranslate"><span class="pre">padding='same'</span></code> pads
the input so the output has the shape as the input. However, this mode
doesn’t support any stride values other than 1.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<em>int</em><em>, </em><em>tuple</em><em> or </em><em>str</em><em>, </em><em>optional</em>) – Padding added to all six sides of
the input. Default: 0</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code></p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> where</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0]
      \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1]
      \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2]
      \times (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor\]</div></div>
</li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="oneflow.nn.Conv3d.weight">
<code class="sig-name descname"><span class="pre">weight</span></code><a class="headerlink" href="#oneflow.nn.Conv3d.weight" title="Permalink to this definition">¶</a></dt>
<dd><p>the learnable weights of the module of shape
<span class="math notranslate nohighlight">\((\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}},\)</span>
<span class="math notranslate nohighlight">\(\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})\)</span>.
The values of these weights are sampled from
<span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.Conv3d.bias">
<code class="sig-name descname"><span class="pre">bias</span></code><a class="headerlink" href="#oneflow.nn.Conv3d.bias" title="Permalink to this definition">¶</a></dt>
<dd><p>the learnable bias of the module of shape (out_channels). If <a class="reference internal" href="#oneflow.nn.Conv3d.bias" title="oneflow.nn.Conv3d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> is <code class="docutils literal notranslate"><span class="pre">True</span></code>,
then the values of these weights are
sampled from <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Conv3d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Conv3d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
<dl class="py method">
<dt id="oneflow.nn.Conv3d.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.Conv3d.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ConvTranspose1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ConvTranspose1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ConvTranspose1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D transposed convolution operator over an input image
composed of several input planes.</p>
<p>This module can be seen as the gradient of Conv1d with respect to its input.
It is also known as a fractionally-strided convolution or
a deconvolution (although it is not an actual deconvolution operation).</p>
<p>This module supports TensorFloat32.</p>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero padding on both
sides for <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> number of points. See note
below for details.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> controls the additional size added to one side
of the output shape. See note below for details.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> argument effectively adds <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code>
amount of zero padding to both sizes of the input. This is set so that
when a <code class="xref py py-class docutils literal notranslate"><span class="pre">Conv1d</span></code> and a <code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose1d</span></code>
are initialized with same parameters, they are inverses of each other in
regard to the input and output shapes. However, when <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>,
<code class="xref py py-class docutils literal notranslate"><span class="pre">Conv1d</span></code> maps multiple input shapes to the same output
shape. <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is provided to resolve this ambiguity by
effectively increasing the calculated output shape on one side. Note
that <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is only used to find output shape, but does
not actually add zero-padding to output.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on randomness for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding
will be added to both sides of the input. Default: 0</p></li>
<li><p><strong>output_padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Additional size added to one side
of the output shape. Default: 0</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C_{in}, L_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C_{out}, L_{out})\)</span> where</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[L_{out} = (L_{in} - 1) \times \text{stride} - 2 \times \text{padding} + \text{dilation}
          \times (\text{kernel_size} - 1) + \text{output_padding} + 1\]</div></div>
</li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="oneflow.nn.ConvTranspose1d.weight">
<code class="sig-name descname"><span class="pre">weight</span></code><a class="headerlink" href="#oneflow.nn.ConvTranspose1d.weight" title="Permalink to this definition">¶</a></dt>
<dd><p>the learnable weights of the module of shape
<span class="math notranslate nohighlight">\((\\text{in\_channels}, \frac{\\text{out\\_channels}}{\text{groups}},\)</span>
<span class="math notranslate nohighlight">\(\\text{kernel\\_size})\)</span>.
The values of these weights are sampled from
<span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{out} * \\text{kernel\\_size}}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.ConvTranspose1d.bias">
<code class="sig-name descname"><span class="pre">bias</span></code><a class="headerlink" href="#oneflow.nn.ConvTranspose1d.bias" title="Permalink to this definition">¶</a></dt>
<dd><p>the learnable bias of the module of shape (out_channels).
If <a class="reference internal" href="#oneflow.nn.ConvTranspose1d.bias" title="oneflow.nn.ConvTranspose1d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the values of these weights are
sampled from <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{out} * \\text{kernel\\_size}}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="oneflow.nn.ConvTranspose1d.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.ConvTranspose1d.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ConvTranspose2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ConvTranspose2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ConvTranspose2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D transposed convolution operator over an input image composed of several input planes.</p>
<p>This module can be seen as the gradient of Conv2d with respect to its input.
It is also known as a fractionally-strided convolution or
a deconvolution (although it is not an actual deconvolution operation).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding
will be added to both sides of each dimension in the input. Default: 0</p></li>
<li><p><strong>output_padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Additional size added to one side
of each dimension in the output shape. Default: 0</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span> where</p></li>
</ul>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]\\          \times (\text{kernel_size}[0] - 1) + \text{output_padding}[0] + 1\end{aligned}\end{align} \]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}W_{out} = (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]\\          \times (\text{kernel_size}[1] - 1) + \text{output_padding}[1] + 1\end{aligned}\end{align} \]</div></div>
</dd>
</dl>
<dl class="py attribute">
<dt id="oneflow.nn.ConvTranspose2d.weight">
<code class="sig-name descname"><span class="pre">weight</span></code><a class="headerlink" href="#oneflow.nn.ConvTranspose2d.weight" title="Permalink to this definition">¶</a></dt>
<dd><p>the learnable weights of the module of shape
<span class="math notranslate nohighlight">\((\text{in_channels}, \frac{\text{out_channels}}{\text{groups}},\)</span>
<span class="math notranslate nohighlight">\(\text{kernel_size[0]}, \text{kernel_size[1]})\)</span>.
The values of these weights are sampled from
<span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{out} * \prod_{i=0}^{1}\text{kernel_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.ConvTranspose2d.bias">
<code class="sig-name descname"><span class="pre">bias</span></code><a class="headerlink" href="#oneflow.nn.ConvTranspose2d.bias" title="Permalink to this definition">¶</a></dt>
<dd><p>the learnable bias of the module of shape (out_channels)
If <a class="reference internal" href="#oneflow.nn.ConvTranspose2d.bias" title="oneflow.nn.ConvTranspose2d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the values of these weights are
sampled from <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{out} * \prod_{i=0}^{1}\text{kernel_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="k">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([20, 33, 93, 100])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.ConvTranspose2d.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.ConvTranspose2d.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ConvTranspose3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ConvTranspose3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ConvTranspose3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D transposed convolution operator over an input image composed of several input
planes.
The transposed convolution operator multiplies each input value element-wise by a learnable kernel,
and sums over the outputs from all input feature planes.</p>
<p>This module can be seen as the gradient of Conv3d with respect to its input.
It is also known as a fractionally-strided convolution or
a deconvolution (although it is not an actual deconvolution operation).</p>
<p>This module supports TensorFloat32.</p>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero padding on both
sides for <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> number of points. See note
below for details.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> controls the additional size added to one side
of the output shape. See note below for details.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p></li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code>
can either be:</p>
<blockquote>
<div><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the depth, height and width dimensions</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of three ints – in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the height dimension and the third <cite>int</cite> for the width dimension</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> argument effectively adds <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code>
amount of zero padding to both sizes of the input. This is set so that
when a <code class="xref py py-class docutils literal notranslate"><span class="pre">Conv3d</span></code> and a <code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose3d</span></code>
are initialized with same parameters, they are inverses of each other in
regard to the input and output shapes. However, when <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>,
<code class="xref py py-class docutils literal notranslate"><span class="pre">Conv3d</span></code> maps multiple input shapes to the same output
shape. <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is provided to resolve this ambiguity by
effectively increasing the calculated output shape on one side. Note
that <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is only used to find output shape, but does
not actually add zero-padding to output.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple</em>) – Size of the convolving kernel</p></li>
<li><p><strong>stride</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding
will be added to both sides of each dimension in the input. Default: 0</p></li>
<li><p><strong>output_padding</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Additional size added to one side
of each dimension in the output shape. Default: 0</p></li>
<li><p><strong>groups</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> where</p></li>
</ul>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[D_{out} = (D_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]
          \times (\text{kernel_size}[0] - 1) + \text{output_padding}[0] + 1\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = (H_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]
          \times (\text{kernel_size}[1] - 1) + \text{output_padding}[1] + 1\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = (W_{in} - 1) \times \text{stride}[2] - 2 \times \text{padding}[2] + \text{dilation}[2]
          \times (\text{kernel_size}[2] - 1) + \text{output_padding}[2] + 1\]</div></div>
</dd>
</dl>
<dl class="py attribute">
<dt id="oneflow.nn.ConvTranspose3d.weight">
<code class="sig-name descname"><span class="pre">weight</span></code><a class="headerlink" href="#oneflow.nn.ConvTranspose3d.weight" title="Permalink to this definition">¶</a></dt>
<dd><p>the learnable weights of the module of shape
<span class="math notranslate nohighlight">\((\text{in_channels}, \frac{\text{out_channels}}{\text{groups}},\)</span>
<span class="math notranslate nohighlight">\(\text{kernel_size[0]}, \text{kernel_size[1]}, \text{kernel_size[2]})\)</span>.
The values of these weights are sampled from
<span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{out} * \prod_{i=0}^{2}\text{kernel_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.ConvTranspose3d.bias">
<code class="sig-name descname"><span class="pre">bias</span></code><a class="headerlink" href="#oneflow.nn.ConvTranspose3d.bias" title="Permalink to this definition">¶</a></dt>
<dd><p>the learnable bias of the module of shape (out_channels)
If <a class="reference internal" href="#oneflow.nn.ConvTranspose3d.bias" title="oneflow.nn.ConvTranspose3d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the values of these weights are
sampled from <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{out} * \prod_{i=0}^{2}\text{kernel_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="k">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.ConvTranspose3d.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.ConvTranspose3d.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.CropMirrorNormalize">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">CropMirrorNormalize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">color_space</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'BGR'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_layout</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'NCHW'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crop_h</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crop_w</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crop_pos_y</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crop_pos_x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">[0.0]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">[1.0]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dtype</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">oneflow._oneflow_internal.dtype</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">oneflow.float32</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.CropMirrorNormalize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.CrossEntropyLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">CrossEntropyLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_index</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">-</span> <span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.CrossEntropyLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>This criterion combines <code class="xref py py-class docutils literal notranslate"><span class="pre">LogSoftmax</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">NLLLoss</span></code> in one single class.</p>
<p>It is useful when training a classification problem with <cite>C</cite> classes.</p>
<p>The <cite>input</cite> is expected to contain raw, unnormalized scores for each class.</p>
<p><cite>input</cite> has to be a Tensor of size either <span class="math notranslate nohighlight">\((minibatch, C)\)</span> or
<span class="math notranslate nohighlight">\((minibatch, C, d_1, d_2, ..., d_K)\)</span>
with <span class="math notranslate nohighlight">\(K \geq 1\)</span> for the <cite>K</cite>-dimensional case (described later).</p>
<p>This criterion expects a class index in the range <span class="math notranslate nohighlight">\([0, C-1]\)</span> as the
<cite>target</cite> for each value of a 1D tensor of size <cite>minibatch</cite>;</p>
<p>The loss can be described as:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right)
               = -x[class] + \log\left(\sum_j \exp(x[j])\right)\]</div></div>
<p>Can also be used for higher dimension inputs, such as 2D images, by providing
an input of size <span class="math notranslate nohighlight">\((minibatch, C, d_1, d_2, ..., d_K)\)</span> with <span class="math notranslate nohighlight">\(K \geq 1\)</span>,
where <span class="math notranslate nohighlight">\(K\)</span> is the number of dimensions, and a target of appropriate shape
(see below).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will
be applied, <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the weighted mean of the output is taken,
<code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>   <span class="p">[[</span><span class="o">-</span><span class="mf">0.1664078</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7256707</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.14690138</span><span class="p">],</span>
<span class="gp">... </span>       <span class="p">[</span><span class="o">-</span><span class="mf">0.21474946</span><span class="p">,</span> <span class="mf">0.53737473</span><span class="p">,</span> <span class="mf">0.99684894</span><span class="p">],</span>
<span class="gp">... </span>       <span class="p">[</span><span class="o">-</span><span class="mf">1.135804</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.50371903</span><span class="p">,</span> <span class="mf">0.7645404</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.8020, 1.1167, 0.3583], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_sum</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_sum</span>
<span class="go">tensor(2.2769, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_mean</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_mean</span>
<span class="go">tensor(0.7590, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Dropout">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Dropout</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>During training, randomly zeroes some of the elements of the input
tensor with probability <a class="reference internal" href="#oneflow.nn.Dropout.p" title="oneflow.nn.Dropout.p"><code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code></a> using samples from a Bernoulli
distribution. Each channel will be zeroed out independently on every forward
call.</p>
<p>This has proven to be an effective technique for regularization and
preventing the co-adaptation of neurons as described in the paper
“Improving neural networks by preventing co-adaptation of feature
detectors”.</p>
<p>Furthermore, the outputs are scaled by a factor of <span class="math notranslate nohighlight">\(\frac{1}{1-p}\)</span> during
training. This means that during evaluation the module simply computes an
identity function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> – probability of an element to be zeroed. Default: 0.5</p></li>
<li><p><strong>inplace</strong> – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((*)\)</span>. Input can be of any shape</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((*)\)</span>. Output is of the same shape as input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
<span class="gp">... </span>   <span class="p">[</span>
<span class="gp">... </span>       <span class="p">[</span><span class="o">-</span><span class="mf">0.7797</span><span class="p">,</span> <span class="mf">0.2264</span><span class="p">,</span> <span class="mf">0.2458</span><span class="p">,</span> <span class="mf">0.4163</span><span class="p">],</span>
<span class="gp">... </span>       <span class="p">[</span><span class="mf">0.4299</span><span class="p">,</span> <span class="mf">0.3626</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4892</span><span class="p">,</span> <span class="mf">0.4141</span><span class="p">],</span>
<span class="gp">... </span>       <span class="p">[</span><span class="o">-</span><span class="mf">1.4115</span><span class="p">,</span> <span class="mf">1.2183</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5503</span><span class="p">,</span> <span class="mf">0.6520</span><span class="p">],</span>
<span class="gp">... </span>   <span class="p">]</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> 
<span class="go">tensor([[-0.7797,  0.2264,  0.2458,  0.4163],</span>
<span class="go">        [ 0.4299,  0.3626, -0.4892,  0.4141],</span>
<span class="go">        [-1.4115,  1.2183, -0.5503,  0.6520]], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py attribute">
<dt id="oneflow.nn.Dropout.inplace">
<code class="sig-name descname"><span class="pre">inplace</span></code><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#oneflow.nn.Dropout.inplace" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.Dropout.p">
<code class="sig-name descname"><span class="pre">p</span></code><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><a class="headerlink" href="#oneflow.nn.Dropout.p" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ELU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ELU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\text{ELU}(x) = \begin{cases}
                        x &amp; \text{ if } x \gt 0  \\
        \alpha*(exp(x)-1) &amp; \text{ if } x \le 0 \\
            \end{cases}\end{split}\]</div></div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> – the <span class="math notranslate nohighlight">\(\alpha\)</span> value for the ELU formulation. Default: 1.0</p></li>
<li><p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">elu</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">elu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([-0.3935,  0.0000,  0.5000], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.ELU.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ELU.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Embedding">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Embedding</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_embeddings</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_idx</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_norm</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_grad_by_freq</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sparse</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_weight</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Embedding" title="Permalink to this definition">¶</a></dt>
<dd><p>A simple lookup table that stores embeddings of a fixed dictionary and size.</p>
<p>This module is often used to store word embeddings and retrieve them using indices.
The input to the module is a list of indices, and the output is the corresponding
word embeddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_embeddings</strong> (<em>int</em>) – size of the dictionary of embeddings</p></li>
<li><p><strong>embedding_dim</strong> (<em>int</em>) – the size of each embedding vector</p></li>
<li><p><strong>padding_idx</strong> (<em>int</em><em>, </em><em>optional</em>) – If specified, the entries at <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_idx</span></code> do not contribute to the gradient;
therefore, the embedding vector at <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_idx</span></code> is not updated during training,
i.e. it remains as a fixed “pad”. For a newly constructed Embedding,
the embedding vector at <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_idx</span></code> will default to all zeros,
but can be updated to another value to be used as the padding vector.</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Embedding.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.Embedding.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.FakeQuantization">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">FakeQuantization</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">quantization_formula</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'google'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_bit</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_scheme</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'symmetric'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.FakeQuantization" title="Permalink to this definition">¶</a></dt>
<dd><p>Simulate the quantize and dequantize operations in training time.</p>
<p>The output will be computed as:</p>
<blockquote>
<div><p>if quantization_scheme == “symmetric”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; quant\_max = 2^{quantization\_to\_bit - 1} - 1\\&amp; quant\_min = -quant\_max\\&amp; clamp(round(x / scale), quant\_min, quant\_max) * scale\end{aligned}\end{align} \]</div></div>
<p>elif quantization_scheme == “affine”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; quant\_max = 2^{quantization\_to\_bit} - 1\\&amp; quant\_min = 0\\&amp; (clamp(round(x / scale + zero\_point), quant\_min, quant\_max) - zero\_point) * scale\end{aligned}\end{align} \]</div></div>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>quantization_bit</strong> (<em>int</em>) – Quantize input to uintX / intX, X can be in range [2, 8]. Defaults to 8.</p></li>
<li><p><strong>quantization_scheme</strong> (<em>str</em>) – “symmetric” or “affine”, quantize to signed / unsigned integer. Defaults to “symmetric”.</p></li>
<li><p><strong>quantization_formula</strong> (<em>str</em>) – Support “google” or “cambricon”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Input tensor after quantize and dequantize operations.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">oneflow.Tensor</a></p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">weight</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_bit</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_scheme</span> <span class="o">=</span> <span class="s2">"symmetric"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_formula</span> <span class="o">=</span> <span class="s2">"google"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">per_layer_quantization</span> <span class="o">=</span> <span class="bp">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">min_max_observer</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MinMaxObserver</span><span class="p">(</span><span class="n">quantization_formula</span><span class="o">=</span><span class="n">quantization_formula</span><span class="p">,</span> <span class="n">quantization_bit</span><span class="o">=</span><span class="n">quantization_bit</span><span class="p">,</span>
<span class="gp">... </span><span class="n">quantization_scheme</span><span class="o">=</span><span class="n">quantization_scheme</span><span class="p">,</span> <span class="n">per_layer_quantization</span><span class="o">=</span><span class="n">per_layer_quantization</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fake_quantization</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">FakeQuantization</span><span class="p">(</span><span class="n">quantization_formula</span><span class="o">=</span><span class="n">quantization_formula</span><span class="p">,</span> <span class="n">quantization_bit</span><span class="o">=</span><span class="n">quantization_bit</span><span class="p">,</span>
<span class="gp">... </span><span class="n">quantization_scheme</span><span class="o">=</span><span class="n">quantization_scheme</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> <span class="n">min_max_observer</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">input_tensor</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">output_tensor</span> <span class="o">=</span> <span class="n">fake_quantization</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">input_tensor</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">scale</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">zero_point</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Flatten">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Flatten</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">start_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">end_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Flatten" title="Permalink to this definition">¶</a></dt>
<dd><p>Flattens a contiguous range of dims into a tensor. For use with: nn.Sequential.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start_dim</strong> – first dim to flatten (default = 1).</p></li>
<li><p><strong>end_dim</strong> – last dim to flatten (default = -1).</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="go">oneflow.Size([32, 25])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Flatten.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.Flatten.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.FusedBatchNorm1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">FusedBatchNorm1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.FusedBatchNorm1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Fused Batch Normalization over a 2D or 3D input, the formula is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = ReLU(BatchNorm(input) + addend)\]</div></div>
<p>The formula of Batch Normalization is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size). By default, the elements of <span class="math notranslate nohighlight">\(\gamma\)</span> are set
to 1 and the elements of <span class="math notranslate nohighlight">\(\beta\)</span> are set to 0. The standard-deviation is calculated
via the biased estimator, equivalent to <cite>torch.var(input, unbiased=False)</cite>.</p>
<p>Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code>
of 0.1.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, this layer then does not
keep running estimates, and batch statistics are instead used during
evaluation time as well.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span>,
where <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the estimated statistic and <span class="math notranslate nohighlight">\(x_t\)</span> is the
new observed value.</p>
</div>
<p>Because the Batch Normalization is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, L)</cite> slices, it’s common terminology to call this Temporal Batch Normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> – <span class="math notranslate nohighlight">\(C\)</span> from an expected input of size
<span class="math notranslate nohighlight">\((N, C, L)\)</span> or <span class="math notranslate nohighlight">\(L\)</span> from input of size <span class="math notranslate nohighlight">\((N, L)\)</span></p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability.
Default: 1e-5</p></li>
<li><p><strong>momentum</strong> – the value used for the running_mean and running_var
computation. Can be set to <code class="docutils literal notranslate"><span class="pre">None</span></code> for cumulative moving average
(i.e. simple average). Default: 0.1</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics, and initializes statistics
buffers <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> as <code class="docutils literal notranslate"><span class="pre">None</span></code>.
When these buffers are <code class="docutils literal notranslate"><span class="pre">None</span></code>, this module always uses batch statistics.
in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C)\)</span> or <span class="math notranslate nohighlight">\((N, C, L)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C)\)</span> or <span class="math notranslate nohighlight">\((N, C, L)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span> <span class="c1"># FusedBatchNorm support in GPU currently.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">FusedBatchNorm1d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">addend</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.FusedBatchNorm2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">FusedBatchNorm2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.FusedBatchNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Fused Batch Normalization over a 4D input, the formula is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = ReLU(BatchNorm(input) + addend)\]</div></div>
<p>The formula of Batch Normalization is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size). By default, the elements of <span class="math notranslate nohighlight">\(\gamma\)</span> are set
to 1 and the elements of <span class="math notranslate nohighlight">\(\beta\)</span> are set to 0. The standard-deviation is calculated
via the biased estimator, equivalent to <cite>torch.var(input, unbiased=False)</cite>.</p>
<p>Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code>
of 0.1.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, this layer then does not
keep running estimates, and batch statistics are instead used during
evaluation time as well.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span>,
where <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the estimated statistic and <span class="math notranslate nohighlight">\(x_t\)</span> is the
new observed value.</p>
</div>
<p>Because the Batch Normalization is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, H, W)</cite> slices, it’s common terminology to call this Spatial Batch Normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> – <span class="math notranslate nohighlight">\(C\)</span> from an expected input of size
<span class="math notranslate nohighlight">\((N, C, H, W)\)</span></p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability.
Default: 1e-5</p></li>
<li><p><strong>momentum</strong> – the value used for the running_mean and running_var
computation. Can be set to <code class="docutils literal notranslate"><span class="pre">None</span></code> for cumulative moving average
(i.e. simple average). Default: 0.1</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics, and initializes statistics
buffers <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> as <code class="docutils literal notranslate"><span class="pre">None</span></code>.
When these buffers are <code class="docutils literal notranslate"><span class="pre">None</span></code>, this module always uses batch statistics.
in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span> <span class="c1"># FusedBatchNorm support in GPU currently.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">FusedBatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">addend</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.FusedBatchNorm3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">FusedBatchNorm3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.FusedBatchNorm3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Fused Batch Normalization over a 5D input, the formula is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = ReLU(BatchNorm(input) + addend)\]</div></div>
<p>The formula of Batch Normalization is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}y = \\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\end{split}\]</div></div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size). By default, the elements of <span class="math notranslate nohighlight">\(\gamma\)</span> are set
to 1 and the elements of <span class="math notranslate nohighlight">\(\beta\)</span> are set to 0. The standard-deviation is calculated
via the biased estimator, equivalent to <cite>torch.var(input, unbiased=False)</cite>.</p>
<p>Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code>
of 0.1.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, this layer then does not
keep running estimates, and batch statistics are instead used during
evaluation time as well.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times     x_t\)</span>,
where <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the estimated statistic and <span class="math notranslate nohighlight">\(x_t\)</span> is the
new observed value.</p>
</div>
<p>Because the Batch Normalization is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, D, H, W)</cite> slices, it’s common terminology to call this Spatial Batch Normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> – <span class="math notranslate nohighlight">\(C\)</span> from an expected input of size
<span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span></p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability.
Default: 1e-5</p></li>
<li><p><strong>momentum</strong> – the value used for the running_mean and running_var
computation. Can be set to <code class="docutils literal notranslate"><span class="pre">None</span></code> for cumulative moving average
(i.e. simple average). Default: 0.1</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics, and initializes statistics
buffers <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> as <code class="docutils literal notranslate"><span class="pre">None</span></code>.
When these buffers are <code class="docutils literal notranslate"><span class="pre">None</span></code>, this module always uses batch statistics.
in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span> <span class="c1"># FusedBatchNorm support in GPU currently.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">FusedBatchNorm3d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">addend</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.GELU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">GELU</span></code><a class="headerlink" href="#oneflow.nn.GELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Gelu activation operator.</p>
<p>The equation is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = 0.5 * x * (1 + tanh(\sqrt{\frac{2}{\pi}} * (x + 0.044715x^{3})))\]</div></div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor"><em>oneflow.Tensor</em></a>) – Input Tensor</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">oneflow.Tensor</a></p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gelu</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">gelu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([-0.1543,  0.0000,  0.3457], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.GLU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">GLU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.GLU" title="Permalink to this definition">¶</a></dt>
<dd><p>The GLU activation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor"><em>Tensor</em></a><em>, </em><em>float</em>) – input tensor.</p></li>
<li><p><strong>dim</strong> (<em>int</em><em>, </em><em>optional</em>) – dimension on which to split the input. Default: -1</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((\ast_1, N, \ast_2)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((\ast_1, M, \ast_2)\)</span> where <span class="math notranslate nohighlight">\(M=N/2\)</span></p></li>
</ul>
</dd>
</dl>
<p>The formula is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[GLU(input) = GLU(a, b) = a \otimes sigmoid(b)\]</div></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>where input is split in half along dim to form a and b, ⊗ is the element-wise product between matrices.</p>
</div>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span>
<span class="go">tensor([[0.9526, 1.9640],</span>
<span class="go">        [4.9954, 5.9980]], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.GroupNorm">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">GroupNorm</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_groups</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_channels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.GroupNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from:
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html">https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html</a></p>
<p>Applies Group Normalization over a mini-batch of inputs as described in
the paper <a class="reference external" href="https://arxiv.org/abs/1803.08494">Group Normalization</a></p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>The input channels are separated into <code class="xref py py-attr docutils literal notranslate"><span class="pre">num_groups</span></code> groups, each containing
<code class="docutils literal notranslate"><span class="pre">num_channels</span> <span class="pre">/</span> <span class="pre">num_groups</span></code> channels. The mean and standard-deviation are calculated
separately over the each group. <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable
per-channel affine transform parameter vectors of size <code class="xref py py-attr docutils literal notranslate"><span class="pre">num_channels</span></code> if
<code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.
The standard-deviation is calculated via the biased estimator, equivalent to
<cite>torch.var(input, unbiased=False)</cite>.</p>
<p>This layer uses statistics computed from input data in both training and
evaluation modes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_groups</strong> (<em>int</em>) – number of groups to separate the channels into</p></li>
<li><p><strong>num_channels</strong> (<em>int</em>) – number of channels expected in input</p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module
has learnable per-channel affine parameters initialized to ones (for weights)
and zeros (for biases). Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, *)\)</span> where <span class="math notranslate nohighlight">\(C=\text{num_channels}\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, *)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Separate 6 channels into 3 groups</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Separate 6 channels into 6 groups (equivalent with InstanceNorm)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Put all 6 channels into a single group (equivalent with LayerNorm)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Activating the module</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.GroupNorm.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.GroupNorm.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
<dl class="py method">
<dt id="oneflow.nn.GroupNorm.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.GroupNorm.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Hardsigmoid">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Hardsigmoid</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Hardsigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\text{Hardsigmoid}(x) = \begin{cases}
    0 &amp; \text{ if } x \le -3  \\
    1 &amp; \text{ if } x \ge +3 \\
    \frac{x}{6} + \frac{1}{2} &amp; \text{ otherwise } \\
\end{cases}\end{split}\]</div></div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hardsigmoid</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Hardsigmoid</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">hardsigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.4167, 0.5000, 0.5833], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Hardsigmoid.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Hardsigmoid.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Hardswish">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Hardswish</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Hardswish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the hardswish function, element-wise, as described in the paper:
<a class="reference external" href="https://arxiv.org/abs/1905.02244">Searching for MobileNetV3</a>.</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\text{Hardswish}(x) = \begin{cases}
    0 &amp; \text{ if } x \le -3  \\
    x &amp; \text{ if } x \ge +3 \\
    x*(x+3)/6 &amp; \text{ otherwise } \\
\end{cases}\end{split}\]</div></div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hardswish</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Hardswish</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">hardswish</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([-0.2083,  0.0000,  0.2917], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Hardswish.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Hardswish.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Hardtanh">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Hardtanh</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">min_val</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_val</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_value</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_value</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Hardtanh" title="Permalink to this definition">¶</a></dt>
<dd><p>按照以下公式（HardTanh），进行 element-wise 操作：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\text{HardTanh}(x) = \begin{cases}
    1 &amp; \text{ if } x &gt; 1 \\
    -1 &amp; \text{ if } x &lt; -1 \\
    x &amp; \text{ otherwise } \\
\end{cases}\end{split}\]</div></div>
<p>默认的线性范围为 <span class="math notranslate nohighlight">\([-1, 1]\)</span>，可以通过设置参数
<code class="xref py py-attr docutils literal notranslate"><span class="pre">min_val</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_val</span></code> 改变。</p>
<dl class="simple">
<dt>参数:</dt><dd><ul class="simple">
<li><p>min_val: 线性范围的下界。 默认值: -1</p></li>
<li><p>max_val: 线性范围的上界。 默认值: 1</p></li>
<li><p>inplace: 是否做 in-place 操作。默认为 <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<p>因为有了参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">min_val</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_val</span></code>，原有的
参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">min_value</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_value</span></code> 已经被不再推荐使用。</p>
<dl class="simple">
<dt>形状:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> 其中 <cite>*</cite> 的意思是，可以指定任意维度</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span> 输入形状与输出形状一致</p></li>
</ul>
</dd>
</dl>
<p>示例:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Hardtanh</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.2000, 0.3000, 1.0000, 1.0000], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Hardtanh.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Hardtanh.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Identity">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Identity</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Identity" title="Permalink to this definition">¶</a></dt>
<dd><p>A placeholder identity operator that is argument-insensitive.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> – any argument (unused)</p></li>
<li><p><strong>kwargs</strong> – any keyword argument (unused)</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="c1"># output = input</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.InstanceNorm1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">InstanceNorm1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.InstanceNorm1d" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from: <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html">https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html</a></p>
<p>Applies Instance Normalization over a 3D input (a mini-batch of 1D
inputs with optional additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1607.08022">Instance Normalization: The Missing Ingredient for Fast Stylization</a>.</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>The mean and standard-deviation are calculated per-dimension separately
for each object in a mini-batch. <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size) if <code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.
The standard-deviation is calculated via the biased estimator, equivalent to
<cite>torch.var(input, unbiased=False)</cite>.</p>
<p>By default, this layer uses instance statistics computed from input data in
both training and evaluation modes.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, during training this
layer keeps running estimates of its computed mean and variance, which are
then used for normalization during evaluation. The running estimates are
kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> of 0.1.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span>,
where <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the estimated statistic and <span class="math notranslate nohighlight">\(x_t\)</span> is the
new observed value.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#oneflow.nn.InstanceNorm1d" title="oneflow.nn.InstanceNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a> and <a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> are very similar, but
have some subtle differences. <a class="reference internal" href="#oneflow.nn.InstanceNorm1d" title="oneflow.nn.InstanceNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a> is applied
on each channel of channeled data like multidimensional time series, but
<a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> is usually applied on entire sample and often in NLP
tasks. Additionally, <a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> applies elementwise affine
transform, while <a class="reference internal" href="#oneflow.nn.InstanceNorm1d" title="oneflow.nn.InstanceNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a> usually don’t apply affine
transform.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> – <span class="math notranslate nohighlight">\(C\)</span> from an expected input of size
<span class="math notranslate nohighlight">\((N, C, L)\)</span> or <span class="math notranslate nohighlight">\(L\)</span> from input of size <span class="math notranslate nohighlight">\((N, L)\)</span></p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</p></li>
<li><p><strong>momentum</strong> – the value used for the running_mean and running_var computation. Default: 0.1</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters, initialized the same way as done for batch normalization.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, L)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, L)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.InstanceNorm2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">InstanceNorm2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.InstanceNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from: <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html">https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html</a></p>
<p>Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs
with additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1607.08022">Instance Normalization: The Missing Ingredient for Fast Stylization</a>.</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>The mean and standard-deviation are calculated per-dimension separately
for each object in a mini-batch. <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size) if <code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.
The standard-deviation is calculated via the biased estimator, equivalent to
<cite>torch.var(input, unbiased=False)</cite>.</p>
<p>By default, this layer uses instance statistics computed from input data in
both training and evaluation modes.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, during training this
layer keeps running estimates of its computed mean and variance, which are
then used for normalization during evaluation. The running estimates are
kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> of 0.1.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span>,
where <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the estimated statistic and <span class="math notranslate nohighlight">\(x_t\)</span> is the
new observed value.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#oneflow.nn.InstanceNorm2d" title="oneflow.nn.InstanceNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a> and <a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> are very similar, but
have some subtle differences. <a class="reference internal" href="#oneflow.nn.InstanceNorm2d" title="oneflow.nn.InstanceNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a> is applied
on each channel of channeled data like RGB images, but
<a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> is usually applied on entire sample and often in NLP
tasks. Additionally, <a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> applies elementwise affine
transform, while <a class="reference internal" href="#oneflow.nn.InstanceNorm2d" title="oneflow.nn.InstanceNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a> usually don’t apply affine
transform.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> – <span class="math notranslate nohighlight">\(C\)</span> from an expected input of size
<span class="math notranslate nohighlight">\((N, C, H, W)\)</span></p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</p></li>
<li><p><strong>momentum</strong> – the value used for the running_mean and running_var computation. Default: 0.1</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters, initialized the same way as done for batch normalization.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.InstanceNorm3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">InstanceNorm3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.InstanceNorm3d" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from: <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm3d.html">https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm3d.html</a></p>
<p>Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs
with additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1607.08022">Instance Normalization: The Missing Ingredient for Fast Stylization</a>.</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>The mean and standard-deviation are calculated per-dimension separately
for each object in a mini-batch. <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameter vectors
of size C (where C is the input size) if <code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.
The standard-deviation is calculated via the biased estimator, equivalent to
<cite>torch.var(input, unbiased=False)</cite>.</p>
<p>By default, this layer uses instance statistics computed from input data in
both training and evaluation modes.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, during training this
layer keeps running estimates of its computed mean and variance, which are
then used for normalization during evaluation. The running estimates are
kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> of 0.1.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span>,
where <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the estimated statistic and <span class="math notranslate nohighlight">\(x_t\)</span> is the
new observed value.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#oneflow.nn.InstanceNorm3d" title="oneflow.nn.InstanceNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a> and <a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> are very similar, but
have some subtle differences. <a class="reference internal" href="#oneflow.nn.InstanceNorm3d" title="oneflow.nn.InstanceNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a> is applied
on each channel of channeled data like 3D models with RGB color, but
<a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> is usually applied on entire sample and often in NLP
tasks. Additionally, <a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> applies elementwise affine
transform, while <a class="reference internal" href="#oneflow.nn.InstanceNorm3d" title="oneflow.nn.InstanceNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a> usually don’t apply affine
transform.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> – <span class="math notranslate nohighlight">\(C\)</span> from an expected input of size
<span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span></p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</p></li>
<li><p><strong>momentum</strong> – the value used for the running_mean and running_var computation. Default: 0.1</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters, initialized the same way as done for batch normalization.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.KLDivLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">KLDivLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_target</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.KLDivLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from:
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html?highlight=kldivloss#torch.nn.KLDivLoss">https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html?highlight=kldivloss#torch.nn.KLDivLoss</a></p>
<p>The Kullback-Leibler divergence loss measure</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler divergence</a> is a useful distance measure for continuous
distributions and is often useful when performing direct regression over
the space of (discretely sampled) continuous output distributions.</p>
<p>As with <code class="xref py py-class docutils literal notranslate"><span class="pre">NLLLoss</span></code>, the <cite>input</cite> given is expected to contain
<em>log-probabilities</em> and is not restricted to a 2D Tensor.
The targets are interpreted as <em>probabilities</em> by default, but could be considered
as <em>log-probabilities</em> with <code class="xref py py-attr docutils literal notranslate"><span class="pre">log_target</span></code> set to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>This criterion expects a <cite>target</cite> <cite>Tensor</cite> of the same size as the
<cite>input</cite> <cite>Tensor</cite>.</p>
<p>The unreduced (i.e. with <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> set to <code class="docutils literal notranslate"><span class="pre">'none'</span></code>) loss can be described as:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[l(x,y) = L = \{ l_1,\dots,l_N \}, \quad
l_n = y_n \cdot \left( \log y_n - x_n \right)\]</div></div>
<p>where the index <span class="math notranslate nohighlight">\(N\)</span> spans all dimensions of <code class="docutils literal notranslate"><span class="pre">input</span></code> and <span class="math notranslate nohighlight">\(L\)</span> has the same
shape as <code class="docutils literal notranslate"><span class="pre">input</span></code>. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is not <code class="docutils literal notranslate"><span class="pre">'none'</span></code> (default <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>), then:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), &amp; \text{if reduction} = \text{`mean';} \\
    \operatorname{sum}(L),  &amp; \text{if reduction} = \text{`sum'.}
\end{cases}\end{split}\]</div></div>
<p>In default <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> mode <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>, the losses are averaged for each minibatch over observations
<strong>as well as</strong> over dimensions. <code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> mode gives the correct KL divergence where losses
are averaged over batch dimension only. <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> mode’s behavior will be changed to the same as
<code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> in the next major release.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied.
<code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code>: the sum of the output will be divided by batchsize.
<code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed.
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the output will be divided by the number of elements in the output.
Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
<li><p><strong>log_target</strong> (<em>bool</em><em>, </em><em>optional</em>) – Specifies whether <cite>target</cite> is passed in the log space.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> doesn’t return the true kl divergence value, please use
<code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = <code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> which aligns with KL math definition.
In the next major release, <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> will be changed to be the same as <code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code>.</p>
</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <span class="math notranslate nohighlight">\(*\)</span> means, any number of additional
dimensions</p></li>
<li><p>Target: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
<li><p>Output: scalar by default. If :attr:<code class="docutils literal notranslate"><span class="pre">reduction</span></code> is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, then <span class="math notranslate nohighlight">\((N, *)\)</span>,
the same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.9021705</span><span class="p">,</span> <span class="mf">0.08798598</span><span class="p">,</span> <span class="mf">1.04686249</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.22386942</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.89729659</span><span class="p">,</span> <span class="mf">0.01615712</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">,</span> <span class="n">log_target</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([ 1.3514,  0.0000, -0.0836], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">,</span> <span class="n">log_target</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(0.4226, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">,</span> <span class="n">log_target</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(5.7801, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.L1Loss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">L1Loss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.L1Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>This operator computes the L1 Loss between each element in <cite>input</cite> and <cite>target</cite>.</p>
<p>The equation is:</p>
<p>if reduction = “none”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[output = |Target - Input|\]</div></div>
<p>if reduction = “mean”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[output = \frac{1}{n}\sum_{i=1}^n|Target_i - Input_i|\]</div></div>
<p>if reduction = “sum”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[output = \sum_{i=1}^n|Target_i - Input_i|\]</div></div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor"><em>oneflow.Tensor</em></a>) – The input Tensor.</p></li>
<li><p><strong>target</strong> (<a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor"><em>oneflow.Tensor</em></a>) – The target Tensor.</p></li>
<li><p><strong>reduction</strong> (<em>str</em>) – The reduce type, it can be one of “none”, “mean”, “sum”. Defaults to “mean”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The result Tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">oneflow.Tensor</a></p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">]],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[3., 3., 3.],</span>
<span class="go">        [2., 2., 2.],</span>
<span class="go">        [3., 3., 3.]], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m_mean</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m_mean</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(2.6667, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m_mean</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m_mean</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(24., dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.LayerNorm">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">LayerNorm</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">normalized_shape</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">oneflow.Size</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">elementwise_affine</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.LayerNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Layer Normalization over a mini-batch of inputs as described in
the paper <a class="reference external" href="https://arxiv.org/abs/1607.06450">Layer Normalization</a></p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>The mean and standard-deviation are calculated separately over the last
certain number dimensions which have to be of the shape specified by
<a class="reference internal" href="#oneflow.nn.LayerNorm.normalized_shape" title="oneflow.nn.LayerNorm.normalized_shape"><code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized_shape</span></code></a>.
<span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable affine transform parameters of
<a class="reference internal" href="#oneflow.nn.LayerNorm.normalized_shape" title="oneflow.nn.LayerNorm.normalized_shape"><code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized_shape</span></code></a> if <a class="reference internal" href="#oneflow.nn.LayerNorm.elementwise_affine" title="oneflow.nn.LayerNorm.elementwise_affine"><code class="xref py py-attr docutils literal notranslate"><span class="pre">elementwise_affine</span></code></a> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.
The standard-deviation is calculated via the biased estimator.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unlike Batch Normalization and Instance Normalization, which applies
scalar scale and bias for each entire channel/plane with the
<code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> option, Layer Normalization applies per-element scale and
bias with <a class="reference internal" href="#oneflow.nn.LayerNorm.elementwise_affine" title="oneflow.nn.LayerNorm.elementwise_affine"><code class="xref py py-attr docutils literal notranslate"><span class="pre">elementwise_affine</span></code></a>.</p>
</div>
<p>This layer uses statistics computed from input data in both training and
evaluation modes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>normalized_shape</strong> (<em>int</em><em> or </em><em>list</em><em> or </em><em>oneflow.Size</em>) – <p>input shape from an expected input of size</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[[* \times \text{normalized_shape}[0] \times \text{normalized_shape}[1] \times \ldots \times \text{normalized_shape}[-1]]\]</div></div>
<p>If a single integer is used, it is treated as a singleton list, and this module will</p>
<p>normalize over the last dimension which is expected to be of that specific size.</p>
</p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</p></li>
<li><p><strong>elementwise_affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module
has learnable per-element affine parameters initialized to ones (for weights)
and zeros (for biases). Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">input_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span>
<span class="gp">... </span>        <span class="p">[</span>
<span class="gp">... </span>            <span class="p">[[</span><span class="o">-</span><span class="mf">0.16046895</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.03667831</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.34974465</span><span class="p">,</span> <span class="mf">0.26505867</span><span class="p">]],</span>
<span class="gp">... </span>            <span class="p">[[</span><span class="o">-</span><span class="mf">1.24111986</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.53806001</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.72426331</span><span class="p">,</span> <span class="mf">0.43572459</span><span class="p">]],</span>
<span class="gp">... </span>        <span class="p">],</span>
<span class="gp">... </span>        <span class="p">[</span>
<span class="gp">... </span>            <span class="p">[[</span><span class="o">-</span><span class="mf">0.77390957</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.42610624</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.16398858</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.35760343</span><span class="p">]],</span>
<span class="gp">... </span>            <span class="p">[[</span><span class="mf">1.07541728</span><span class="p">,</span> <span class="mf">0.11008703</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.26361224</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.48663723</span><span class="p">]],</span>
<span class="gp">... </span>        <span class="p">],</span>
<span class="gp">... </span>    <span class="p">],</span>
<span class="gp">... </span>    <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">input_arr</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span>
<span class="go">array([[[[ 0.99997395, -0.99997395],</span>
<span class="go">         [-0.999947  ,  0.999947  ]],</span>

<span class="go">        [[-0.99995965,  0.9999595 ],</span>
<span class="go">         [ 0.99998784, -0.99998784]]],</span>


<span class="go">       [[[-0.9998348 ,  0.99983466],</span>
<span class="go">         [ 0.9999914 , -0.9999914 ]],</span>

<span class="go">        [[ 0.9999785 , -0.9999785 ],</span>
<span class="go">         [ 0.9999646 , -0.9999646 ]]]], dtype=float32)</span>
</pre></div>
</div>
<dl class="py attribute">
<dt id="oneflow.nn.LayerNorm.elementwise_affine">
<code class="sig-name descname"><span class="pre">elementwise_affine</span></code><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#oneflow.nn.LayerNorm.elementwise_affine" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.LayerNorm.eps">
<code class="sig-name descname"><span class="pre">eps</span></code><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><a class="headerlink" href="#oneflow.nn.LayerNorm.eps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="oneflow.nn.LayerNorm.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.LayerNorm.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.LayerNorm.normalized_shape">
<code class="sig-name descname"><span class="pre">normalized_shape</span></code><em class="property"><span class="pre">:</span> <span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="p"><span class="pre">…</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#oneflow.nn.LayerNorm.normalized_shape" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="oneflow.nn.LayerNorm.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.LayerNorm.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.LeakyReLU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">LeakyReLU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">negative_slope</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.LeakyReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\text{LeakyRELU}(x) = \begin{cases}
    x, &amp; \text{ if } x \geq 0 \\
    \text{negative_slope} \times x, &amp; \text{ otherwise }
\end{cases}\end{split}\]</div></div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>negative_slope</strong> – Controls the angle of the negative slope. Default: 1e-2</p></li>
<li><p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.2000, 0.3000, 3.0000, 4.0000], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.LeakyReLU.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.LeakyReLU.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Linear">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Linear</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_features</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_features</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Linear" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a linear transformation to the incoming data: <span class="math notranslate nohighlight">\(y = xA^T + b\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_features</strong> (<em>-</em>) – size of each input sample</p></li>
<li><p><strong>out_features</strong> (<em>-</em>) – size of each output sample</p></li>
<li><p><strong>bias</strong> (<em>-</em>) – If set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the layer will not learn an additive bias. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *, H_{in})\)</span> where <span class="math notranslate nohighlight">\(*\)</span> means any number of
additional dimensions and <span class="math notranslate nohighlight">\(H_{in} = {in\_features}\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *, H_{out})\)</span> where all but the last dimension
are the same shape as the input and <span class="math notranslate nohighlight">\(H_{out} = {out\_features}\)</span>.</p></li>
</ul>
</dd>
<dt>Attr:</dt><dd><ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code>: the learnable weights of the module of shape <span class="math notranslate nohighlight">\(({out\_features}, {in\_features})\)</span>. The values are initialized from <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>, where <span class="math notranslate nohighlight">\((k = 1 / {in\_features})\)</span></p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code>: the learnable bias of the module of shape <span class="math notranslate nohighlight">\(({out\_features})\)</span>. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the values are initialized from <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where <span class="math notranslate nohighlight">\((k = 1 / {in\_features})\)</span></p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([128, 30])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Linear.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.Linear.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
<dl class="py method">
<dt id="oneflow.nn.Linear.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.Linear.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.LogSigmoid">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">LogSigmoid</span></code><a class="headerlink" href="#oneflow.nn.LogSigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{LogSigmoid}(x) = \log\left(\frac{ 1 }{ 1 + \exp(-x)}\right)\]</div></div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logsigmoid</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LogSigmoid</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">logsigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([-0.9741, -0.6931, -0.4741], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.LogSoftmax">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">LogSoftmax</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.LogSoftmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the LogSoftmax function to an n-dimensional
input Tensor.
The LogSoftmax formulation can be simplified as:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right) = x_i - \log({ \sum_j \exp(x_j)})\]</div></div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dim</strong> (<em>int</em>) – A dimension along which LogSoftmax will be computed.</p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
<span class="gp">... </span>       <span class="p">[[</span> <span class="mf">0.4296</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1957</span><span class="p">,</span>  <span class="mf">2.5463</span><span class="p">],</span>
<span class="gp">... </span>       <span class="p">[</span> <span class="mf">1.2552</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5747</span><span class="p">,</span>  <span class="mf">0.6923</span><span class="p">]]</span>
<span class="gp">... </span>   <span class="p">)</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[-2.2513, -3.8766, -0.1346],</span>
<span class="go">        [-0.4877, -3.3176, -1.0506]], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.LogSoftmax.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.LogSoftmax.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.MSELoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">MSELoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.MSELoss" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from:
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html?highlight=mseloss#torch.nn.MSELoss">https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html?highlight=mseloss#torch.nn.MSELoss</a></p>
<p>Creates a criterion that measures the mean squared error (squared L2 norm) between
each element in the input <span class="math notranslate nohighlight">\(x\)</span> and target <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>The unreduced (i.e. with <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> set to <code class="docutils literal notranslate"><span class="pre">'none'</span></code>) loss can be described as:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = \left( x_n - y_n \right)^2,\]</div></div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the batch size. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is not <code class="docutils literal notranslate"><span class="pre">'none'</span></code>
(default <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>), then:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\ell(x, y) =
\begin{cases}
    \operatorname{mean}(L), &amp;  \text{if reduction} = \text{`mean';}\\
    \operatorname{sum}(L),  &amp;  \text{if reduction} = \text{`sum'.}
\end{cases}\end{split}\]</div></div>
<p><span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are tensors of arbitrary shapes with a total
of <span class="math notranslate nohighlight">\(n\)</span> elements each.</p>
<p>The mean operation still operates over all the elements, and divides by <span class="math notranslate nohighlight">\(n\)</span>.</p>
<p>The division by <span class="math notranslate nohighlight">\(n\)</span> can be avoided if one sets <code class="docutils literal notranslate"><span class="pre">reduction</span> <span class="pre">=</span> <span class="pre">'sum'</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <span class="math notranslate nohighlight">\(*\)</span> means, any number of additional
dimensions</p></li>
<li><p>Target: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span><span class="p">[[</span><span class="o">-</span><span class="mf">0.02557137</span><span class="p">,</span> <span class="mf">0.03101675</span><span class="p">,</span> <span class="mf">1.37493674</span><span class="p">],</span>
<span class="gp">... </span><span class="p">[</span><span class="mf">0.25599439</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.08372561</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.21006816</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span><span class="p">[[</span><span class="o">-</span><span class="mf">1.53105064</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.68137555</span><span class="p">,</span> <span class="mf">0.5931354</span><span class="p">],</span>
<span class="gp">... </span><span class="p">[</span><span class="o">-</span><span class="mf">0.49158347</span><span class="p">,</span> <span class="mf">0.93673637</span><span class="p">,</span> <span class="mf">0.1324141</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[2.2665, 0.5075, 0.6112],</span>
<span class="go">        [0.5589, 4.0823, 0.1173]], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(1.3573, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(8.1436, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.MarginRankingLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">MarginRankingLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">margin</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.MarginRankingLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the loss given
inputs <span class="math notranslate nohighlight">\(x1\)</span>, <span class="math notranslate nohighlight">\(x2\)</span>, two 1D mini-batch <cite>Tensors</cite>,
and a label 1D mini-batch tensor <span class="math notranslate nohighlight">\(y\)</span> (containing 1 or -1).</p>
<p>If <span class="math notranslate nohighlight">\(y = 1\)</span> then it assumed the first input should be ranked higher
(have a larger value) than the second input, and vice-versa for <span class="math notranslate nohighlight">\(y = -1\)</span>.</p>
<p>The loss function for each sample in the mini-batch is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{loss}(x1, x2, y) = \max(0, -y * (x1 - x2) + \text{margin})\]</div></div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>margin</strong> (<em>float</em><em>, </em><em>optional</em>) – Has a default value of <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p><cite>x1</cite> : <span class="math notranslate nohighlight">\((N, D)\)</span> where <cite>N</cite> is the batch size and <cite>D</cite> is the size of a sample.</p></li>
<li><p><cite>x2</cite> : <span class="math notranslate nohighlight">\((N, D)\)</span> where <cite>N</cite> is the batch size and <cite>D</cite> is the size of a sample.</p></li>
<li><p>Target: <span class="math notranslate nohighlight">\((N)\)</span></p></li>
<li><p>Output: scalar. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, then <span class="math notranslate nohighlight">\((N)\)</span>.</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MarginRankingLoss</span><span class="p">(</span><span class="n">margin</span> <span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[2., 1., 0.],</span>
<span class="go">        [3., 0., 5.],</span>
<span class="go">        [0., 0., 0.]], dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MarginRankingLoss</span><span class="p">(</span><span class="n">margin</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(8.2000, dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MarginRankingLoss</span><span class="p">(</span><span class="n">margin</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(8.3333, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.MaxPool1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">MaxPool1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_indices</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.MaxPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from: <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d">https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d</a></p>
<p>Applies a 1D max pooling over an input signal composed of several input planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math notranslate nohighlight">\((N, C, L)\)</span>
and output <span class="math notranslate nohighlight">\((N, C, L_{out})\)</span> can be precisely described as:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out(N_i, C_j, k) = \max_{m=0, \ldots, \text{kernel\_size} - 1}
        input(N_i, C_j, stride \times k + m)\]</div></div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly padded with minimum value on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points. <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> is the stride between the elements within the
sliding window. This <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of the pooling parameters.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding
or the input. Sliding windows that would start in the right padded region are ignored.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> – The size of the sliding window, must be &gt; 0.</p></li>
<li><p><strong>stride</strong> – The stride of the sliding window, must be &gt; 0. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>.</p></li>
<li><p><strong>padding</strong> – Implicit negative infinity padding to be added on both sides, must be &gt;= 0 and &lt;= kernel_size / 2.</p></li>
<li><p><strong>dilation</strong> – The stride between elements within a sliding window, must be &gt; 0.</p></li>
<li><p><strong>return_indices</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the argmax along with the max values.
Useful for <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.MaxUnpool1d</span></code> later</p></li>
<li><p><strong>ceil_mode</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape. This
ensures that every element in the input tensor is covered by a sliding window.</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, L_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, L_{out})\)</span>, where</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[L_{out} = \left\lfloor \frac{L_{in} + 2 \times \text{padding} - \text{dilation}
      \times (\text{kernel_size} - 1) - 1}{\text{stride}} + 1\right\rfloor\]</div></div>
</li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">of_maxpool1d</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">of_maxpool1d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="n">oneflow</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.MaxPool1d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.MaxPool1d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.MaxPool2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">MaxPool2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_indices</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.MaxPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from: <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d">https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d</a></p>
<p>Applies a 2D max pooling over an input signal composed of several input planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math notranslate nohighlight">\((N, C, H, W)\)</span>,
output <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> <span class="math notranslate nohighlight">\((kH, kW)\)</span>
can be precisely described as:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    out(N_i, C_j, h, w) ={} &amp; \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\
                            &amp; \text{input}(N_i, C_j, \text{stride[0]} \times h + m,
                                           \text{stride[1]} \times w + n)
\end{aligned}\end{split}\]</div></div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly minimum value padded on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points. <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding
or the input. Sliding windows that would start in the right padded region are ignored.</p>
</div>
<dl class="simple">
<dt>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> can either be:</dt><dd><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimension</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> – the size of the window to take a max over</p></li>
<li><p><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></p></li>
<li><p><strong>padding</strong> – implicit minimum value padding to be added on both sides</p></li>
<li><p><strong>dilation</strong> – a parameter that controls the stride of elements in the window</p></li>
<li><p><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the max indices along with the outputs.
Useful for <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.MaxUnpool2d</span></code> later</p></li>
<li><p><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span>, where</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor\frac{H_{in} + 2 * \text{padding[0]} - \text{dilation[0]}
      \times (\text{kernel_size[0]} - 1) - 1}{\text{stride[0]}} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor\frac{W_{in} + 2 * \text{padding[1]} - \text{dilation[1]}
      \times (\text{kernel_size[1]} - 1) - 1}{\text{stride[1]}} + 1\right\rfloor\]</div></div>
</li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">of_maxpool2d</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">of_maxpool2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="n">oneflow</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.MaxPool2d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.MaxPool2d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.MaxPool3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">MaxPool3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_indices</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.MaxPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from: <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html#torch.nn.MaxPool3d">https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html#torch.nn.MaxPool3d</a></p>
<p>Applies a 3D max pooling over an input signal composed of several input planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span>,
output <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> <span class="math notranslate nohighlight">\((kD, kH, kW)\)</span>
can be precisely described as:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \text{out}(N_i, C_j, d, h, w) ={} &amp; \max_{k=0, \ldots, kD-1} \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\
                                      &amp; \text{input}(N_i, C_j, \text{stride[0]} \times d + k,
                                                     \text{stride[1]} \times h + m, \text{stride[2]} \times w + n)
\end{aligned}\end{split}\]</div></div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly minimum value on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points. <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding
or the input. Sliding windows that would start in the right padded region are ignored.</p>
</div>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the depth, height and width dimension</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of three ints – in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the height dimension and the third <cite>int</cite> for the width dimension</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> – the size of the window to take a max over</p></li>
<li><p><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></p></li>
<li><p><strong>padding</strong> – implicit minimum value padding to be added on all three sides</p></li>
<li><p><strong>dilation</strong> – a parameter that controls the stride of elements in the window</p></li>
<li><p><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the max indices along with the outputs.
Useful for <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.MaxUnpool3d</span></code> later</p></li>
<li><p><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span>, where</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times
  (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times
  (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2] \times
  (\text{kernel_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor\]</div></div>
</li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">of_maxpool3d</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">of_maxpool3d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="n">oneflow</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.MaxPool3d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.MaxPool3d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.MinMaxObserver">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">MinMaxObserver</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">quantization_formula</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'google'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_bit</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_scheme</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'symmetric'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_layer_quantization</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.MinMaxObserver" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the quantization parameters of the input tensor.</p>
<p>First compute the max and min values of input tensor:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; max\_value = max(input)\\&amp; min\_value = min(input)\end{aligned}\end{align} \]</div></div>
<p>Then compute the scale and zero_point with the following equations:</p>
<blockquote>
<div><p>if quantization_scheme == “symmetric”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; denom = 2^{quantization\_to\_bit - 1} - 1\\&amp; scale = max(|max\_value|,|min\_value|) / denom\\&amp; zero\_point = 0\end{aligned}\end{align} \]</div></div>
<p>elif quantization_scheme == “affine”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; denom = 2^{quantization\_to\_bit} - 1\\&amp; scale = (max\_value - min\_value) / denom\\&amp; zero\_point = -min\_value / scale\end{aligned}\end{align} \]</div></div>
</div></blockquote>
<p>If per_layer_quantization is False, then the shape of scale and zero_point will be (input.shape[0],).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>quantization_bit</strong> (<em>int</em>) – Quantize input to uintX / intX, X can be in range [2, 8]. Defaults to 8.</p></li>
<li><p><strong>quantization_scheme</strong> (<em>str</em>) – “symmetric” or “affine”, quantize to signed / unsigned integer. Defaults to “symmetric”.</p></li>
<li><p><strong>quantization_formula</strong> (<em>str</em>) – Support “google” or “cambricon”.</p></li>
<li><p><strong>per_layer_quantization</strong> (<em>bool</em>) – True or False, means per-layer / per-channel quantization. Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The scale and zero_point of input tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tuple[<a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">oneflow.Tensor</a>, <a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">oneflow.Tensor</a>]</p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">weight</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_bit</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_scheme</span> <span class="o">=</span> <span class="s2">"symmetric"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_formula</span> <span class="o">=</span> <span class="s2">"google"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">per_layer_quantization</span> <span class="o">=</span> <span class="bp">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">min_max_observer</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MinMaxObserver</span><span class="p">(</span><span class="n">quantization_formula</span><span class="o">=</span><span class="n">quantization_formula</span><span class="p">,</span> <span class="n">quantization_bit</span><span class="o">=</span><span class="n">quantization_bit</span><span class="p">,</span>
<span class="gp">... </span><span class="n">quantization_scheme</span><span class="o">=</span><span class="n">quantization_scheme</span><span class="p">,</span> <span class="n">per_layer_quantization</span><span class="o">=</span><span class="n">per_layer_quantization</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> <span class="n">min_max_observer</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">input_tensor</span><span class="p">,</span> <span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Mish">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Mish</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Mish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{Mish}(x) = x * \text{Tanh}(\text{Softplus}(x))\]</div></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference external" href="https://arxiv.org/abs/1908.08681">Mish: A Self Regularized Non-Monotonic Neural Activation Function</a></p>
</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mish</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Mish</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">mish</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.8651, 1.9440, 2.9865], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ModuleDict">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ModuleDict</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">modules</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">oneflow.nn.module.Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ModuleDict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ModuleList">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ModuleList</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">modules</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow.nn.module.Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ModuleList" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.MovingAverageMinMaxObserver">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">MovingAverageMinMaxObserver</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_formula</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'google'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_update_after_iters</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_bit</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_scheme</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'symmetric'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.MovingAverageMinMaxObserver" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the quantization parameters based on the moving average of the input tensor’s min and max values.</p>
<p>First compute the moving_max and moving_min value of input tensor:</p>
<blockquote>
<div><p>if quantization_scheme == “symmetric”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; moving\_max = moving\_max * momentum + |max(input)| * (1 - momentum)\\&amp; moving\_min = moving\_max\end{aligned}\end{align} \]</div></div>
<p>elif quantization_scheme == “affine”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; moving\_max = moving\_max * momentum + max(input) * (1 - momentum)\\&amp; moving\_min = moving\_min * momentum + min(input) * (1 - momentum)\end{aligned}\end{align} \]</div></div>
</div></blockquote>
<p>The moving average of min and max values are initialized as the first batch of input <cite>Blob</cite>’s min and max.</p>
<p>Then compute the scale and zero_point with the following equations:</p>
<blockquote>
<div><p>if quantization_scheme == “symmetric”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; denom = 2^{quantization\_to\_bit - 1} - 1\\&amp; scale = moving\_max / denom\\&amp; zero\_point = 0\end{aligned}\end{align} \]</div></div>
<p>elif quantization_scheme == “affine”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; denom = 2^{quantization\_to\_bit} - 1\\&amp; scale = (moving\_max - moving\_min) / denom\\&amp; zero\_point = -moving\_min / scale\end{aligned}\end{align} \]</div></div>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">current_train_step</span></code> can be directly assigned to an optimizer(eg.SGD) step.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>training</strong> (<em>bool</em>) – Is the model in training state. Defaults to False.</p></li>
<li><p><strong>quantization_bit</strong> (<em>int</em>) – Quantize input to uintX / intX, X can be in range [2, 8]. Defaults to 8.</p></li>
<li><p><strong>quantization_scheme</strong> (<em>str</em>) – “symmetric” or “affine”, quantize to signed / unsigned integer. Defaults to “symmetric”.</p></li>
<li><p><strong>quantization_formula</strong> (<em>str</em>) – Support “google” or “cambricon”.</p></li>
<li><p><strong>momentum</strong> (<em>float</em>) – Smoothing parameter for exponential moving average operation. Defaults to 0.95.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The scale and zero_point of input tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tuple[<a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">oneflow.Tensor</a>, <a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">oneflow.Tensor</a>]</p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">weight</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">current_train_step_tensor</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>  <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
<span class="gp">... </span>   <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_bit</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_scheme</span> <span class="o">=</span> <span class="s2">"symmetric"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_formula</span> <span class="o">=</span> <span class="s2">"google"</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">moving_average_min_max_observer</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">(</span><span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">quantization_formula</span><span class="o">=</span><span class="n">quantization_formula</span><span class="p">,</span>
<span class="gp">... </span>                                                                      <span class="n">stop_update_after_iters</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">quantization_bit</span><span class="o">=</span><span class="n">quantization_bit</span><span class="p">,</span>
<span class="gp">... </span>                                                                      <span class="n">quantization_scheme</span><span class="o">=</span><span class="n">quantization_scheme</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span>
<span class="gp">... </span>                                                                      <span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">)</span> <span class="o">=</span> <span class="n">moving_average_min_max_observer</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">input_tensor</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">current_train_step_tensor</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.MovingAverageMinMaxObserver.reset_running_stats">
<code class="sig-name descname"><span class="pre">reset_running_stats</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.MovingAverageMinMaxObserver.reset_running_stats" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.NLLLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">NLLLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_index</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">-</span> <span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.NLLLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>The negative log likelihood loss. It is useful to train a classification
problem with <cite>C</cite> classes.</p>
<p>The <cite>input</cite> given through a forward call is expected to contain
log-probabilities of each class. <cite>input</cite> has to be a Tensor of size either
<span class="math notranslate nohighlight">\((minibatch, C)\)</span> or <span class="math notranslate nohighlight">\((minibatch, C, d_1, d_2, ..., d_K)\)</span>
with <span class="math notranslate nohighlight">\(K \geq 1\)</span> for the <cite>K</cite>-dimensional case (described later).</p>
<p>Obtaining log-probabilities in a neural network is easily achieved by
adding a  <cite>LogSoftmax</cite>  layer in the last layer of your network.
You may use <cite>CrossEntropyLoss</cite> instead, if you prefer not to add an extra
layer.</p>
<p>The <cite>target</cite> that this loss expects should be a class index in the range <span class="math notranslate nohighlight">\([0, C-1]\)</span>
where <cite>C = number of classes</cite>;</p>
<p>The unreduced (i.e. with <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> set to <code class="docutils literal notranslate"><span class="pre">'none'</span></code>) loss can be described as:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - w_{y_n} x_{n,y_n}, \quad
w_{c} = \mathbb{1},\]</div></div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the input, <span class="math notranslate nohighlight">\(y\)</span> is the target, <span class="math notranslate nohighlight">\(w\)</span> is the weight, and
<span class="math notranslate nohighlight">\(N\)</span> is the batch size. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is not <code class="docutils literal notranslate"><span class="pre">'none'</span></code>
(default <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>), then</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\ell(x, y) = \begin{cases}
    \sum_{n=1}^N \frac{1}{N} l_n, &amp;
    \text{if reduction} = \text{`mean';}\\
    \sum_{n=1}^N l_n,  &amp;
    \text{if reduction} = \text{`sum'.}
\end{cases}\end{split}\]</div></div>
<p>Can also be used for higher dimension inputs, such as 2D images, by providing
an input of size <span class="math notranslate nohighlight">\((minibatch, C, d_1, d_2, ..., d_K)\)</span> with <span class="math notranslate nohighlight">\(K \geq 1\)</span>,
where <span class="math notranslate nohighlight">\(K\)</span> is the number of dimensions, and a target of appropriate shape
(see below). In the case of images, it computes NLL loss per-pixel.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will
be applied, <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the weighted mean of the output is taken,
<code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span><span class="p">[[</span><span class="o">-</span><span class="mf">0.1664078</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7256707</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.14690138</span><span class="p">],</span>
<span class="gp">... </span><span class="p">[</span><span class="o">-</span><span class="mf">0.21474946</span><span class="p">,</span> <span class="mf">0.53737473</span><span class="p">,</span> <span class="mf">0.99684894</span><span class="p">],</span>
<span class="gp">... </span><span class="p">[</span><span class="o">-</span><span class="mf">1.135804</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.50371903</span><span class="p">,</span> <span class="mf">0.7645404</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([ 0.1664, -0.5374, -0.7645], dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(-1.1355, dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(-0.3785, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.OFRecordBytesDecoder">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">OFRecordBytesDecoder</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">blob_name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.OFRecordBytesDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>This operator reads an tensor as bytes. The output might need</p>
<p>further decoding process like cv2.imdecode() for images and decode(“utf-8”)</p>
<p>for characters,depending on the downstream task.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>blob_name</strong> – The name of the target feature in OFRecord.</p></li>
<li><p><strong>name</strong> – The name for this component in the graph.</p></li>
<li><p><strong>input</strong> – the Tensor which might be provided by an OFRecordReader.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The result Tensor encoded with bytes.</p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">example</span><span class="p">():</span>
<span class="gp">... </span>     <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="gp">... </span>     <span class="n">record_reader</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">OFRecordReader</span><span class="p">(</span>
<span class="gp">... </span>        <span class="s2">"dataset/"</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">part_name_suffix_length</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="gp">... </span>     <span class="p">)</span>
<span class="gp">... </span>     <span class="n">val_record</span> <span class="o">=</span> <span class="n">record_reader</span><span class="p">()</span>

<span class="gp">... </span>     <span class="n">bytesdecoder_img</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">OFRecordBytesDecoder</span><span class="p">(</span><span class="s2">"encoded"</span><span class="p">)</span>

<span class="gp">... </span>     <span class="n">image_bytes_batch</span> <span class="o">=</span> <span class="n">bytesdecoder_img</span><span class="p">(</span><span class="n">val_record</span><span class="p">)</span>

<span class="gp">... </span>     <span class="n">image_bytes</span> <span class="o">=</span> <span class="n">image_bytes_batch</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="gp">... </span>     <span class="k">return</span> <span class="n">image_bytes</span>
<span class="gp">... </span><span class="n">example</span><span class="p">()</span>  
<span class="go">array([255 216 255 ...  79 255 217], dtype=uint8)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.OFRecordImageDecoder">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">OFRecordImageDecoder</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">blob_name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">color_space</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'BGR'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.OFRecordImageDecoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.OFRecordImageDecoderRandomCrop">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">OFRecordImageDecoderRandomCrop</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">blob_name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">color_space</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'BGR'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_attempts</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_seed</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_area</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">[0.08,</span> <span class="pre">1.0]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_aspect_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">[0.75,</span> <span class="pre">1.333333]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.OFRecordImageDecoderRandomCrop" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.OFRecordRawDecoder">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">OFRecordRawDecoder</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">blob_name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">oneflow._oneflow_internal.dtype</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim1_varying_length</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncate</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">auto_zero_padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.OFRecordRawDecoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.OFRecordReader">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">OFRecordReader</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ofrecord_dir</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_part_num</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">part_name_prefix</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'part-'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">part_name_suffix_length</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_shuffle</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle_buffer_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1024</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle_after_epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_seed</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.device</span><span class="p"><span class="pre">,</span> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">placement</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.placement</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sbp</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.sbp.sbp</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.sbp.sbp</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.OFRecordReader" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.PReLU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">PReLU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_parameters</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.PReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[PReLU(x) = \max(0,x) + a * \min(0,x)\]</div></div>
<p>Here <span class="math notranslate nohighlight">\(a\)</span> is a learnable parameter. When called without arguments, <cite>nn.PReLU()</cite> uses a single
parameter <span class="math notranslate nohighlight">\(a\)</span> across all input channels. If called with <cite>nn.PReLU(nChannels)</cite>,
a separate <span class="math notranslate nohighlight">\(a\)</span> is used for each input channel.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>weight decay should not be used when learning <span class="math notranslate nohighlight">\(a\)</span> for good performance.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Channel dim is the 2nd dim of input. When input has dims &lt; 2, then there is
no channel dim and the number of channels = 1.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_parameters</strong> (<em>int</em>) – number of <span class="math notranslate nohighlight">\(a\)</span> to learn.
Although it takes an int as input, there is only two values are legitimate:
1, or the number of channels at input. Default: 1</p></li>
<li><p><strong>init</strong> (<em>float</em>) – the initial value of <span class="math notranslate nohighlight">\(a\)</span>. Default: 0.25</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
<dt>Attr:</dt><dd><ul class="simple">
<li><p>weight (Tensor): the learnable weights of shape (<code class="xref py py-attr docutils literal notranslate"><span class="pre">num_parameters</span></code>).</p></li>
</ul>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[[[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]]]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="go">[[[[ 1.  -0.5]</span>
<span class="go">   [ 3.   4. ]]]]</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.PReLU.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.PReLU.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Parameter">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Parameter</span></code><a class="headerlink" href="#oneflow.nn.Parameter" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ParameterDict">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ParameterDict</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ParameterDict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ParameterList">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ParameterList</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ParameterList" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.PixelShuffle">
<code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">PixelShuffle</span></code><a class="headerlink" href="#oneflow.nn.PixelShuffle" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">oneflow.nn.modules.pixelshuffle.PixelShufflev2</span></code></p>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Quantization">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Quantization</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">quantization_formula</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'google'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_bit</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_scheme</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'symmetric'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Quantization" title="Permalink to this definition">¶</a></dt>
<dd><p>Simulate the quantize operation in inference time.</p>
<p>The output will be computed as:</p>
<blockquote>
<div><p>if quantization_scheme == “symmetric”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; quant\_max = 2^{quantization\_to\_bit - 1} - 1\\&amp; quant\_min = -quant\_max\\&amp; clamp(round(x / scale), quant\_min, quant\_max)\end{aligned}\end{align} \]</div></div>
<p>elif quantization_scheme == “affine”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; quant\_max = 2^{quantization\_to\_bit} - 1\\&amp; quant\_min = 0\\&amp; (clamp(round(x / scale + zero\_point), quant\_min, quant\_max) - zero\_point)\end{aligned}\end{align} \]</div></div>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>quantization_bit</strong> (<em>int</em>) – Quantize input to uintX / intX, X can be in range [2, 8]. Defaults to 8.</p></li>
<li><p><strong>quantization_scheme</strong> (<em>str</em>) – “symmetric” or “affine”, quantize to signed / unsigned integer. Defaults to “symmetric”.</p></li>
<li><p><strong>quantization_formula</strong> (<em>str</em>) – Support “google” or “cambricon”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Input tensor after quantize operation.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">oneflow.Tensor</a></p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">weight</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_bit</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_scheme</span> <span class="o">=</span> <span class="s2">"symmetric"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_formula</span> <span class="o">=</span> <span class="s2">"google"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">per_layer_quantization</span> <span class="o">=</span> <span class="bp">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">min_max_observer</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MinMaxObserver</span><span class="p">(</span><span class="n">quantization_formula</span><span class="o">=</span><span class="n">quantization_formula</span><span class="p">,</span> <span class="n">quantization_bit</span><span class="o">=</span><span class="n">quantization_bit</span><span class="p">,</span>
<span class="gp">... </span><span class="n">quantization_scheme</span><span class="o">=</span><span class="n">quantization_scheme</span><span class="p">,</span> <span class="n">per_layer_quantization</span><span class="o">=</span><span class="n">per_layer_quantization</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Quantization</span><span class="p">(</span><span class="n">quantization_formula</span><span class="o">=</span><span class="n">quantization_formula</span><span class="p">,</span> <span class="n">quantization_bit</span><span class="o">=</span><span class="n">quantization_bit</span><span class="p">,</span>
<span class="gp">... </span><span class="n">quantization_scheme</span><span class="o">=</span><span class="n">quantization_scheme</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> <span class="n">min_max_observer</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">input_tensor</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">output_tensor</span> <span class="o">=</span> <span class="n">quantization</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">input_tensor</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">scale</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">zero_point</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ReLU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ReLU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>ReLU 激活函数，对张量中的每一个元素做 element-wise 运算，公式如下:</p>
<p><span class="math notranslate nohighlight">\(\text{ReLU}(x) = (x)^+ = \max(0, x)\)</span></p>
<dl class="simple">
<dt>参数:</dt><dd><p>inplace: 是否做 in-place 操作。 默认为 <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
</dd>
<dt>形状:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> 其中 <cite>*</cite> 的意思是，可以指定任意维度</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span> 输入形状与输出形状一致</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relu</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">tensor([1., 0., 3.], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.ReLU.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ReLU.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ReLU6">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ReLU6</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ReLU6" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\text{Relu6}(x) = \begin{cases}
    6 &amp; \text{ if } x &gt; 6 \\
    0 &amp; \text{ if } x &lt; 0 \\
    x &amp; \text{ otherwise } \\
\end{cases}\end{split}\]</div></div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relu6</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">relu6</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.0000, 0.0000, 0.5000], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.ReLU6.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ReLU6.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ReflectionPad2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ReflectionPad2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ReflectionPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from:
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad2d.html">https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad2d.html</a></p>
<p>This operator pads the input tensor using the reflection of the input boundary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>padding</strong> (<em>Union</em><em>[</em><em>int</em><em>,</em><em>tuple</em><em>]</em>) – The size or bundary of padding, if is <cite>int</cite> uses the same padding in all dimension; if 4-dims <cite>tuple</cite>, uses <span class="math notranslate nohighlight">\((\text{padding}_{\text{left}}, \text{padding}_{\text{right}}, \text{padding}_{\text{top}}, \text{padding}_{\text{bottom}} )\)</span></p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns a new tensor which is result of the reflection padding of the input tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H_{\text{in}}, W_{\text{in}})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H_{\text{out}}, W_{\text{out}})\)</span> where</p>
<p><span class="math notranslate nohighlight">\(H_{\text{out}} = H_{\text{in}} + \text{padding}_{\text{top}} + \text{padding}_{\text{bottom}}\)</span></p>
<p><span class="math notranslate nohighlight">\(W_{\text{out}} = W_{\text{in}} + \text{padding}_{\text{left}} + \text{padding}_{\text{right}}\)</span></p>
</li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">18</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReflectionPad2d</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[[[ 5.,  4.,  3.,  4.,  5.,  4.,  3.],</span>
<span class="go">          [ 2.,  1.,  0.,  1.,  2.,  1.,  0.],</span>
<span class="go">          [ 5.,  4.,  3.,  4.,  5.,  4.,  3.],</span>
<span class="go">          [ 8.,  7.,  6.,  7.,  8.,  7.,  6.],</span>
<span class="go">          [ 5.,  4.,  3.,  4.,  5.,  4.,  3.]],</span>

<span class="go">         [[14., 13., 12., 13., 14., 13., 12.],</span>
<span class="go">          [11., 10.,  9., 10., 11., 10.,  9.],</span>
<span class="go">          [14., 13., 12., 13., 14., 13., 12.],</span>
<span class="go">          [17., 16., 15., 16., 17., 16., 15.],</span>
<span class="go">          [14., 13., 12., 13., 14., 13., 12.]]]], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.ReflectionPad2d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.ReflectionPad2d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ReplicationPad2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ReplicationPad2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ReplicationPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from:
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad2d.html?highlight=replicationpad2d#torch.nn.ReplicationPad2d">https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad2d.html?highlight=replicationpad2d#torch.nn.ReplicationPad2d</a></p>
<p>Pads the input tensor using the replication of the input boundary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>padding</strong> (<em>Union</em><em>[</em><em>int</em><em>, </em><em>tuple</em><em>, </em><em>list</em><em>]</em>) – the size of the padding. If is <cite>int</cite>, uses the same padding in all boundaries. If a 4-<cite>tuple</cite>, uses (<span class="math notranslate nohighlight">\(\mathrm{padding_{left}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{right}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{top}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{bottom}}\)</span>)</p>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> where</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(H_{out} = H_{in} + \mathrm{padding_{top}} + \mathrm{padding_{bottom}}\)</span></p>
<p><span class="math notranslate nohighlight">\(W_{out} = W_{in} + \mathrm{padding_{left}} + \mathrm{padding_{right}}\)</span></p>
</div></blockquote>
</li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad2d</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">18</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_int</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">18</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="go">oneflow.Size([1, 2, 5, 7])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([[[[ 0.,  0.,  0.,  1.,  2.,  2.,  2.],</span>
<span class="go">          [ 0.,  0.,  0.,  1.,  2.,  2.,  2.],</span>
<span class="go">          [ 3.,  3.,  3.,  4.,  5.,  5.,  5.],</span>
<span class="go">          [ 6.,  6.,  6.,  7.,  8.,  8.,  8.],</span>
<span class="go">          [ 6.,  6.,  6.,  7.,  8.,  8.,  8.]],</span>

<span class="go">         [[ 9.,  9.,  9., 10., 11., 11., 11.],</span>
<span class="go">          [ 9.,  9.,  9., 10., 11., 11., 11.],</span>
<span class="go">          [12., 12., 12., 13., 14., 14., 14.],</span>
<span class="go">          [15., 15., 15., 16., 17., 17., 17.],</span>
<span class="go">          [15., 15., 15., 16., 17., 17., 17.]]]], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.ReplicationPad2d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.ReplicationPad2d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.SELU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">SELU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.SELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<p>The formula is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{SELU}(x) = \text{scale} * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))\]</div></div>
<p>with <span class="math notranslate nohighlight">\(\alpha = 1.6732632423543772848170429916717\)</span> and</p>
<p><span class="math notranslate nohighlight">\(\text{scale} = 1.0507009873554804934193349852946\)</span>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When using <code class="docutils literal notranslate"><span class="pre">kaiming_normal</span></code> or <code class="docutils literal notranslate"><span class="pre">kaiming_normal_</span></code> for initialisation,
<code class="docutils literal notranslate"><span class="pre">nonlinearity='linear'</span></code> should be used instead of <code class="docutils literal notranslate"><span class="pre">nonlinearity='selu'</span></code>
in order to get <a class="reference external" href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a>.
See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.init.calculate_gain()</span></code> for more information.</p>
</div>
<p>More details can be found in the paper <a class="reference external" href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a>.</p>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selu</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SELU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">selu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([1.0507, 2.1014, 3.1521], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Sequential">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Sequential</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Sequential" title="Permalink to this definition">¶</a></dt>
<dd><p>A sequential container.
Modules will be added to it in the order they are passed in the constructor.
Alternatively, an ordered dict of modules can also be passed in.</p>
<p>To make it easier to understand, here is a small example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span> 
<span class="go">Sequential(</span>
<span class="go">  (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span>
<span class="go">  (1): ReLU()</span>
<span class="go">  (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span>
<span class="go">  (3): ReLU()</span>
<span class="go">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
<span class="gp">... </span>   <span class="p">(</span><span class="s1">'conv1'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">)),</span>
<span class="gp">... </span>   <span class="p">(</span><span class="s1">'relu1'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
<span class="gp">... </span>   <span class="p">(</span><span class="s1">'conv2'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">5</span><span class="p">)),</span>
<span class="gp">... </span>   <span class="p">(</span><span class="s1">'relu2'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
<span class="gp">... </span><span class="p">]))</span> 
<span class="go">Sequential(</span>
<span class="go">  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span>
<span class="go">  (relu1): ReLU()</span>
<span class="go">  (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span>
<span class="go">  (relu2): ReLU()</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.SiLU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">SiLU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.SiLU" title="Permalink to this definition">¶</a></dt>
<dd><p>SiLU(Swish) activation:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{SiLU}(x) = x * sigmoid(x)\]</div></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference external" href="https://arxiv.org/abs/1606.08415">Gaussian Error Linear Units (GELUs)</a>
where the SiLU (Sigmoid Linear Unit) was originally coined, and see
<a class="reference external" href="https://arxiv.org/abs/1702.03118">Sigmoid-Weighted Linear Units for Neural Network Function Approximation
in Reinforcement Learning</a> and <a class="reference external" href="https://arxiv.org/abs/1710.05941v1">Swish:
a Self-Gated Activation Function</a>
where the SiLU was experimented with later.</p>
</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">silu</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SiLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">silu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.7311, 1.7616, 2.8577], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Sigmoid">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Sigmoid</span></code><a class="headerlink" href="#oneflow.nn.Sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>应用逐元素函数：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)}\]</div></div>
<dl class="simple">
<dt>图型：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> 其中 <cite>*</cite> 表示任意数量的附加维度</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span> 与输入相同的形状</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.81733328</span><span class="p">,</span> <span class="mf">0.43621480</span><span class="p">,</span> <span class="mf">0.10351428</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.6937, 0.6074, 0.5259], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.SmoothL1Loss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">SmoothL1Loss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.SmoothL1Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that uses a squared term if the absolute
element-wise error falls below beta and an L1 term otherwise.
The interface is consistent with PyTorch.
The documentation is referenced from:
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html">https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html</a></p>
<p>It is less sensitive to outliers than <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.MSELoss</span></code> and in some cases
prevents exploding gradients (e.g. see the paper <a class="reference external" href="https://openaccess.thecvf.com/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf">Fast R-CNN</a> by Ross Girshick)..</p>
<p>For a batch of size <span class="math notranslate nohighlight">\(N\)</span>, the unreduced loss can be described as:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\ell(x, y) = L = \{l_1, ..., l_N\}^T\]</div></div>
<p>with</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}l_n = \begin{cases}
0.5 (x_n - y_n)^2 / beta, &amp; \text{if } |x_n - y_n| &lt; beta \\
|x_n - y_n| - 0.5 * beta, &amp; \text{otherwise }
\end{cases}\end{split}\]</div></div>
<p>If <cite>reduction</cite> is not <cite>none</cite>, then:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\ell(x, y) =
\begin{cases}
    \operatorname{mean}(L), &amp;  \text{if reduction} = \text{`mean';}\\
    \operatorname{sum}(L),  &amp;  \text{if reduction} = \text{`sum'.}
\end{cases}\end{split}\]</div></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Smooth L1 loss can be seen as exactly <a class="reference internal" href="#oneflow.nn.L1Loss" title="oneflow.nn.L1Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">L1Loss</span></code></a>, but with the <span class="math notranslate nohighlight">\(|x - y| &lt; beta\)</span>
portion replaced with a quadratic function such that its slope is 1 at <span class="math notranslate nohighlight">\(|x - y| = beta\)</span>.
The quadratic segment smooths the L1 loss near <span class="math notranslate nohighlight">\(|x - y| = 0\)</span>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Smooth L1 loss is closely related to <code class="xref py py-class docutils literal notranslate"><span class="pre">HuberLoss</span></code>, being
equivalent to <span class="math notranslate nohighlight">\(huber(x, y) / beta\)</span> (note that Smooth L1’s beta hyper-parameter is
also known as delta for Huber). This leads to the following differences:</p>
<ul class="simple">
<li><p>As beta -&gt; 0, Smooth L1 loss converges to <a class="reference internal" href="#oneflow.nn.L1Loss" title="oneflow.nn.L1Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">L1Loss</span></code></a>, while <code class="xref py py-class docutils literal notranslate"><span class="pre">HuberLoss</span></code>
converges to a constant 0 loss.</p></li>
<li><p>As beta -&gt; <span class="math notranslate nohighlight">\(+\infty\)</span>, Smooth L1 loss converges to a constant 0 loss, while
<code class="xref py py-class docutils literal notranslate"><span class="pre">HuberLoss</span></code> converges to <a class="reference internal" href="#oneflow.nn.MSELoss" title="oneflow.nn.MSELoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MSELoss</span></code></a>.</p></li>
<li><p>For Smooth L1 loss, as beta varies, the L1 segment of the loss has a constant slope of 1.
For <code class="xref py py-class docutils literal notranslate"><span class="pre">HuberLoss</span></code>, the slope of the L1 segment is beta.</p></li>
</ul>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size_average</strong> (<em>bool</em><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there are multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduce</strong> (<em>bool</em><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
<li><p><strong>beta</strong> (<em>float</em><em>, </em><em>optional</em>) – Specifies the threshold at which to change between L1 and L2 loss.
The value must be non-negative. Default: 1.0</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <span class="math notranslate nohighlight">\(*\)</span> means any number of additional dimensions</p></li>
<li><p>Target: <span class="math notranslate nohighlight">\((N, *)\)</span>; same shape as the input</p></li>
<li><p>Output: scalar. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, then <span class="math notranslate nohighlight">\((N, *)\)</span>; same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.0200, 0.1250, 1.7000, 0.0050, 0.1800], dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(0.4060, dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(2.0300, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Softmax">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Softmax</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Softmax function to an n-dimensional input Tensor
rescaling them so that the elements of the n-dimensional output Tensor
lie in the range [0,1] and sum to 1.</p>
<p>Softmax is defined as:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}\]</div></div>
<p>When the input Tensor is a sparse tensor then the unspecifed
values are treated as <code class="docutils literal notranslate"><span class="pre">-inf</span></code>.</p>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((*)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((*)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Tensor of the same dimension and shape as the input with
values in the range [0, 1]</p>
</dd>
<dt class="field-even">Parameters</dt>
<dd class="field-even"><p><strong>dim</strong> (<em>int</em>) – A dimension along which Softmax will be computed (so every slice
along dim will sum to 1).</p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
<span class="gp">... </span>       <span class="p">[[[</span><span class="o">-</span><span class="mf">0.46716809</span><span class="p">,</span>  <span class="mf">0.40112534</span><span class="p">,</span>  <span class="mf">0.61984003</span><span class="p">],</span>
<span class="gp">... </span>       <span class="p">[</span><span class="o">-</span><span class="mf">1.31244969</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.42528763</span><span class="p">,</span>  <span class="mf">1.47953856</span><span class="p">]]]</span>
<span class="gp">... </span>   <span class="p">)</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[[0.1575, 0.3754, 0.4671],</span>
<span class="go">         [0.0507, 0.1230, 0.8263]]], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Softmax.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Softmax.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Softplus">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Softplus</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">20</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Softplus" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))\]</div></div>
<p>SoftPlus is a smooth approximation to the ReLU function and can be used
to constrain the output of a machine to always be positive.</p>
<p>For numerical stability the implementation reverts to the linear function
when <span class="math notranslate nohighlight">\(input \times \beta &gt; threshold\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>beta</strong> – the <span class="math notranslate nohighlight">\(\beta\)</span> value for the Softplus formulation. Default: 1</p></li>
<li><p><strong>threshold</strong> – values above this revert to a linear function. Default: 20</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softplus</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Softplus</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.4741, 0.6931, 0.9741], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Softplus.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Softplus.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Softsign">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Softsign</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Softsign" title="Permalink to this definition">¶</a></dt>
<dd><p>The SoftSign activation.</p>
<p>The formula is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[SoftSign(x) = \frac{x}{1 + |x|}\]</div></div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softsign</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Softsign</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">softsign</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.5000, 0.6667, 0.7500], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Tanh">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Tanh</span></code><a class="headerlink" href="#oneflow.nn.Tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>This operator computes the hyperbolic tangent value of Tensor.</p>
<p>The equation is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = \frac{e^x-e^{-x}}{e^x+e^{-x}}\]</div></div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input</strong> (<a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor"><em>oneflow.Tensor</em></a>) – A Tensor</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The result Tensor</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">oneflow.Tensor</a></p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tanh</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([-0.7616,  0.0000,  0.7616], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Upsample">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Upsample</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="p"><span class="pre">…</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_factor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="p"><span class="pre">…</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'nearest'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">align_corners</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Upsample" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.</p>
<p>The documentation is referenced from: <a class="reference external" href="https://pytorch.org/docs/1.9.0/_modules/torch/nn/modules/upsampling.html#Upsample">https://pytorch.org/docs/1.9.0/_modules/torch/nn/modules/upsampling.html#Upsample</a></p>
<p>Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.</p>
<p>The input data is assumed to be of the form
<cite>minibatch x channels x [optional depth] x [optional height] x width</cite>.
Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor.</p>
<p>The algorithms available for upsampling are nearest neighbor and linear,
bilinear, bicubic and trilinear for 3D, 4D and 5D input Tensor,
respectively.</p>
<p>One can either give a <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code> or the target output <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> to
calculate the output size. (You cannot give both, as it is ambiguous)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>int</em><em> or </em><em>Tuple</em><em>[</em><em>int</em><em>] or </em><em>Tuple</em><em>[</em><em>int</em><em>, </em><em>int</em><em>] or </em><em>Tuple</em><em>[</em><em>int</em><em>, </em><em>int</em><em>, </em><em>int</em><em>]</em><em>, </em><em>optional</em>) – output spatial sizes</p></li>
<li><p><strong>scale_factor</strong> (<em>float</em><em> or </em><em>Tuple</em><em>[</em><em>float</em><em>] or </em><em>Tuple</em><em>[</em><em>float</em><em>, </em><em>float</em><em>] or </em><em>Tuple</em><em>[</em><em>float</em><em>, </em><em>float</em><em>, </em><em>float</em><em>]</em><em>, </em><em>optional</em>) – multiplier for spatial size. Has to match input size if it is a tuple.</p></li>
<li><p><strong>mode</strong> (<em>str</em><em>, </em><em>optional</em>) – the upsampling algorithm: one of <code class="docutils literal notranslate"><span class="pre">'nearest'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'linear'</span></code>, <code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code>, <code class="docutils literal notranslate"><span class="pre">'bicubic'</span></code> and <code class="docutils literal notranslate"><span class="pre">'trilinear'</span></code>.
Default: <code class="docutils literal notranslate"><span class="pre">'nearest'</span></code></p></li>
<li><p><strong>align_corners</strong> (<em>bool</em><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, the corner pixels of the input
and output tensors are aligned, and thus preserving the values at
those pixels. This only has effect when <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code> is
<code class="docutils literal notranslate"><span class="pre">'linear'</span></code>, <code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'trilinear'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, W_{in})\)</span>, <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span> or <span class="math notranslate nohighlight">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, W_{out})\)</span>, <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span>
or <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span>, where</p></li>
</ul>
</dd>
</dl>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[D_{out} = \left\lfloor D_{in} \times \text{scale_factor} \right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor H_{in} \times \text{scale_factor} \right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor W_{in} \times \text{scale_factor} \right\rfloor\]</div></div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>With <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">True</span></code>, the linearly interpolating modes
(<cite>linear</cite>, <cite>bilinear</cite>, <cite>bicubic</cite>, and <cite>trilinear</cite>) don’t proportionally
align the output and input pixels, and thus the output values can depend
on the input size. This was the default behavior for these modes up to
version 0.3.1. Since then, the default behavior is
<code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">False</span></code>. See below for concrete examples on how this
affects the outputs.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want downsampling/general resizing, you should use <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code>.</p>
</div>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"nearest"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> 
<span class="go">tensor([[[[1., 1., 2., 2.],</span>
<span class="go">          ...</span>
<span class="go">          [3., 3., 4., 4.]]]], device='cuda:0', dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Upsample.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.Upsample.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.UpsamplingBilinear2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">UpsamplingBilinear2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_factor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.UpsamplingBilinear2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D bilinear upsampling to an input signal composed of several input
channels.</p>
<p>To specify the scale, it takes either the <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code>
as it’s constructor argument.</p>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> is given, it is the output size of the image <cite>(h, w)</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>int</em><em> or </em><em>Tuple</em><em>[</em><em>int</em><em>, </em><em>int</em><em>]</em><em>, </em><em>optional</em>) – output spatial sizes</p></li>
<li><p><strong>scale_factor</strong> (<em>float</em><em> or </em><em>Tuple</em><em>[</em><em>float</em><em>, </em><em>float</em><em>]</em><em>, </em><em>optional</em>) – multiplier for
spatial size.</p></li>
</ul>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This class is deprecated in favor of <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code>. It is
equivalent to <code class="docutils literal notranslate"><span class="pre">nn.functional.interpolate(...,</span> <span class="pre">mode='bilinear',</span> <span class="pre">align_corners=True)</span></code>.</p>
</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> where</p></li>
</ul>
</dd>
</dl>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor H_{in} \times \text{scale_factor} \right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor W_{in} \times \text{scale_factor} \right\rfloor\]</div></div>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">UpsamplingBilinear2d</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> 
<span class="go">tensor([[[[1.0000, 1.3333, 1.6667, 2.0000],</span>
<span class="go">          ...</span>
<span class="go">          [3.0000, 3.3333, 3.6667, 4.0000]]]], device='cuda:0',</span>
<span class="go">       dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.UpsamplingNearest2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">UpsamplingNearest2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_factor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.UpsamplingNearest2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D nearest neighbor upsampling to an input signal composed of several input
channels.</p>
<p>To specify the scale, it takes either the <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code>
as it’s constructor argument.</p>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> is given, it is the output size of the image <cite>(h, w)</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>int</em><em> or </em><em>Tuple</em><em>[</em><em>int</em><em>, </em><em>int</em><em>]</em><em>, </em><em>optional</em>) – output spatial sizes</p></li>
<li><p><strong>scale_factor</strong> (<em>float</em><em> or </em><em>Tuple</em><em>[</em><em>float</em><em>, </em><em>float</em><em>]</em><em>, </em><em>optional</em>) – multiplier for
spatial size.</p></li>
</ul>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This class is deprecated in favor of <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code>.</p>
</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> where</p></li>
</ul>
</dd>
</dl>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor H_{in} \times \text{scale_factor} \right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor W_{in} \times \text{scale_factor} \right\rfloor\]</div></div>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">UpsamplingNearest2d</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> 
<span class="go">tensor([[[[1., 1., 2., 2.],</span>
<span class="go">          ...</span>
<span class="go">          [3., 3., 4., 4.]]]], device='cuda:0', dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ZeroPad2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ZeroPad2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">tuple</span><span class="p"><span class="pre">,</span> </span><span class="pre">list</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ZeroPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from:
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad2d.html">https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad2d.html</a></p>
<p>Pads the input tensor boundaries with zero. User can set the amount of padding by setting the parameter <cite>paddings</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>padding</strong> (<em>Union</em><em>[</em><em>int</em><em>, </em><em>tuple</em><em>]</em>) – the size of the padding. If is <cite>int</cite>, uses the same padding in all boundaries. If a 4-<cite>tuple</cite>, uses (<span class="math notranslate nohighlight">\(\mathrm{padding_{left}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{right}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{top}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{bottom}}\)</span>)</p>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> where</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(H_{out} = H_{in} + \mathrm{padding_{top}} + \mathrm{padding_{bottom}}\)</span></p>
<p><span class="math notranslate nohighlight">\(W_{out} = W_{in} + \mathrm{padding_{left}} + \mathrm{padding_{right}}\)</span></p>
</div></blockquote>
</li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m1</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ZeroPad2d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m2</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ZeroPad2d</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">18</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m1</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="go">oneflow.Size([1, 2, 7, 7])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([[[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  1.,  2.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  3.,  4.,  5.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  6.,  7.,  8.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]],</span>

<span class="go">         [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  9., 10., 11.,  0.,  0.],</span>
<span class="go">          [ 0.,  0., 12., 13., 14.,  0.,  0.],</span>
<span class="go">          [ 0.,  0., 15., 16., 17.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]]]], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m2</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([[[[ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  1.,  2.,  0.,  0.],</span>
<span class="go">          [ 0.,  3.,  4.,  5.,  0.,  0.],</span>
<span class="go">          [ 0.,  6.,  7.,  8.,  0.,  0.]],</span>

<span class="go">         [[ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  9., 10., 11.,  0.,  0.],</span>
<span class="go">          [ 0., 12., 13., 14.,  0.,  0.],</span>
<span class="go">          [ 0., 15., 16., 17.,  0.,  0.]]]], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py function">
<dt id="oneflow.nn.parallel.DistributedDataParallel">
<code class="sig-prename descclassname"><span class="pre">oneflow.nn.parallel.</span></code><code class="sig-name descname"><span class="pre">DistributedDataParallel</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">oneflow.nn.module.Module</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">broadcast_buffers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.parallel.DistributedDataParallel" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py function">
<dt id="oneflow.nn.utils.clip_grad_norm_">
<code class="sig-prename descclassname"><span class="pre">oneflow.nn.utils.</span></code><code class="sig-name descname"><span class="pre">clip_grad_norm_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_norm</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">error_if_nonfinite</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> → <span class="pre">oneflow._oneflow_internal.Tensor</span><a class="headerlink" href="#oneflow.nn.utils.clip_grad_norm_" title="Permalink to this definition">¶</a></dt>
<dd><p>Clips gradient norm of an iterable of parameters.
The norm is computed over all gradients together, as if they were
concatenated into a single vector.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parameters</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor"><em>Tensor</em></a><em>] or </em><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor"><em>Tensor</em></a>) – an iterable of Tensors or a
single Tensor that will have gradients normalized</p></li>
<li><p><strong>max_norm</strong> (<em>float</em><em> or </em><em>int</em>) – max norm of the gradients</p></li>
<li><p><strong>norm_type</strong> (<em>float</em><em> or </em><em>int</em>) – type of the used p-norm. Can be <code class="docutils literal notranslate"><span class="pre">'inf'</span></code> for
infinity norm.</p></li>
<li><p><strong>error_if_nonfinite</strong> (<em>bool</em>) – if True, an error is thrown if the total
norm of the gradients from :attr:<code class="docutils literal notranslate"><span class="pre">parameters</span></code> is <code class="docutils literal notranslate"><span class="pre">nan</span></code>,
<code class="docutils literal notranslate"><span class="pre">inf</span></code>, or <code class="docutils literal notranslate"><span class="pre">-inf</span></code>. Default: True</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Parameters after cliping gradient norm
Total norm of the parameters (viewed as a single vector).</p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">3.7</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m1</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out1</span> <span class="o">=</span> <span class="n">m1</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out1</span> <span class="o">=</span> <span class="n">out1</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out1</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">norm1</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">norm1</span>
<span class="go">tensor(6., dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([[0.1000, 0.1000, 0.1000],</span>
<span class="go">        [0.1000, 0.1000, 0.1000]], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out2</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">atan</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out2</span> <span class="o">=</span> <span class="n">out2</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out2</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">norm2</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">norm2</span>
<span class="go">tensor(1.0394, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([[0.0962, 0.0481, 0.0283],</span>
<span class="go">        [0.0663, 0.4810, 0.0428]], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py function">
<dt id="oneflow.nn.utils.weight_norm">
<code class="sig-prename descclassname"><span class="pre">oneflow.nn.utils.</span></code><code class="sig-name descname"><span class="pre">weight_norm</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">T_module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'weight'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> → <span class="pre">T_module</span><a class="headerlink" href="#oneflow.nn.utils.weight_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies weight normalization to a parameter in the given module.</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\mathbf{w}=g \frac{\mathbf{v}}{\|\mathbf{v}\|}\]</div></div>
<p>Weight normalization is a reparameterization that decouples the magnitude
of a weight tensor from its direction. This replaces the parameter specified
by <code class="xref py py-attr docutils literal notranslate"><span class="pre">name</span></code> (e.g. <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>) with two parameters: one specifying the magnitude
(e.g. <code class="docutils literal notranslate"><span class="pre">'weight_g'</span></code>) and one specifying the direction (e.g. <code class="docutils literal notranslate"><span class="pre">'weight_v'</span></code>).
Weight normalization is implemented via a hook that recomputes the weight
tensor from the magnitude and direction before every <code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code>
call.</p>
<p>By default, with <code class="docutils literal notranslate"><span class="pre">dim=0</span></code>, the norm is computed independently per output
channel/plane. To compute a norm over the entire weight tensor, use
<code class="docutils literal notranslate"><span class="pre">dim=None</span></code>.</p>
<p>See <a class="reference external" href="https://arxiv.org/abs/1602.07868">https://arxiv.org/abs/1602.07868</a></p>
<p>This document description is refereced to the Pytorch document.
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html">https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<a class="reference internal" href="module.html#oneflow.nn.Module" title="oneflow.nn.Module"><em>Module</em></a>) – containing module</p></li>
<li><p><strong>name</strong> (<em>str</em><em>, </em><em>optional</em>) – name of weight parameter</p></li>
<li><p><strong>dim</strong> (<em>int</em><em>, </em><em>optional</em>) – dimension over which to compute the norm</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The original module with the weight norm hook</p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">weight_norm</span><span class="p">(</span><span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">'weight'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span>
<span class="go">Linear(in_features=20, out_features=40, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">weight_g</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([40, 1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">weight_v</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([40, 20])</span>
</pre></div>
</div>
</dd></dl>
<dl class="py function">
<dt id="oneflow.nn.utils.remove_weight_norm">
<code class="sig-prename descclassname"><span class="pre">oneflow.nn.utils.</span></code><code class="sig-name descname"><span class="pre">remove_weight_norm</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">T_module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'weight'</span></span></em><span class="sig-paren">)</span> → <span class="pre">T_module</span><a class="headerlink" href="#oneflow.nn.utils.remove_weight_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes the weight normalization reparameterization from a module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<a class="reference internal" href="module.html#oneflow.nn.Module" title="oneflow.nn.Module"><em>Module</em></a>) – containing module</p></li>
<li><p><strong>name</strong> (<em>str</em><em>, </em><em>optional</em>) – name of weight parameter</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">weight_norm</span><span class="p">(</span><span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">remove_weight_norm</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="go">Linear(in_features=20, out_features=40, bias=True)</span>
</pre></div>
</div>
</dd></dl>
</div>
</div>

      </article>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="functional.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">oneflow.nn.functional</div>
              </div>
              <svg><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="tensor.html">
              <svg><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">oneflow.Tensor</div>
                
              </div>
            </a>
        </div>

        <div class="related-information">
              Copyright &#169; 2020, OneFlow
            |
            Built with <a href="https://www.sphinx-doc.org/">Sphinx</a>
              and
              <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
              <a href="https://github.com/pradyunsg/furo">Furo theme</a>.
            |
            <a class="muted-link" href="_sources/nn.rst.txt"
               rel="nofollow">
              Show Source
            </a>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            Contents
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">oneflow.nn</a><ul>
<li><a class="reference internal" href="#module-oneflow.nn">Operators for neural networks</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </main>
</div>
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="_static/scripts/main.js?digest=e931d09b2a40c1bb82b542effe772014573baf67"></script></body>
</html>