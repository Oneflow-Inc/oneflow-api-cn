<!doctype html>
<html class="no-js">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><link rel="index" title="Index" href="genindex.html" /><link rel="search" title="Search" href="search.html" /><link rel="next" title="oneflow.nn.functional" href="functional.html" /><link rel="prev" title="Tensor Attributes" href="tensor_attributes.html" />

    <meta name="generator" content="sphinx-3.5.4, furo 2021.04.11.beta34"/>
        <title>oneflow.nn - OneFlow documentation</title>
      <link rel="stylesheet" href="_static/styles/furo.css?digest=59ab60ac09ea94ccfe6deddff6d715cce948a6fc">
    <link rel="stylesheet" href="_static/pygments.css">
    <link media="(prefers-color-scheme: dark)" rel="stylesheet" href="_static/pygments_dark.css">
    


<style>
  :root {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media (prefers-color-scheme: dark) {
    :root {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
  }

  /* For allowing end-user-specific overrides */
  .override-light {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  .override-dark {
    --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
  }
</style><link rel="stylesheet" href="_static/styles/furo-extensions.css?digest=d391b54134226e4196576da3bdb6dddb7e05ba2b"></head>
  <body dir="">
    
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
      stroke-width="1.5" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z"/>
      <line x1="4" y1="6" x2="20" y2="6" />
      <line x1="10" y1="12" x2="20" y2="12" />
      <line x1="6" y1="18" x2="20" y2="18" />
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
      stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
      class="feather feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
      stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
      class="feather feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="index.html"><div class="brand">OneFlow  documentation</div></a>
    </div>
    <div class="header-right">
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="index.html">
  
  
  <span class="sidebar-brand-text">OneFlow  documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="search.html">
  <input class="sidebar-search" placeholder=Search name="q">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul>
<li class="toctree-l1"><a class="reference internal" href="troubleshooting.html">Troubleshooting</a></li>
</ul>
<p class="caption"><span class="caption-text">OneFlow Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="oneflow.html">oneflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">oneflow.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">oneflow.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="functional.html">oneflow.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">oneflow.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">oneflow.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">oneflow.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">oneflow.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">oneflow.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">oneflow.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="module.html">oneflow.nn.Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="graph.html">oneflow.nn.Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="image.html">oneflow.nn.image</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">oneflow.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="env.html">oneflow.env</a></li>
<li class="toctree-l1"><a class="reference internal" href="comm.html">oneflow.comm</a></li>
</ul>

</div>
</div>
      </div>
      
    </div>
  </aside>
  <main class="main">
    <div class="content">
      <article role="main">
        <label class="toc-overlay-icon toc-content-icon" for="__toc">
          <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
        </label>
        <div class="section" id="oneflow-nn">
<h1>oneflow.nn<a class="headerlink" href="#oneflow-nn" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-oneflow.nn">
<span id="operators-for-neural-networks"></span><h2>Operators for neural networks<a class="headerlink" href="#module-oneflow.nn" title="Permalink to this headline">¶</a></h2>
<p>Copyright 2020 The OneFlow Authors. All rights reserved.</p>
<p>Licensed under the Apache License, Version 2.0 (the “License”);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at</p>
<blockquote>
<div><p><a class="reference external" href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></p>
</div></blockquote>
<p>Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an “AS IS” BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.</p>
<dl class="py class">
<dt id="oneflow.nn.AdaptiveAvgPool1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">AdaptiveAvgPool1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.AdaptiveAvgPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个平面组成的输入信号 <cite>input</cite> 上应用 1D 自适应平均池化。</p>
<p>对于任何大小的输入，输出大小都是 H。</p>
<p>输出的数量等于输入平面的数量。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>output_size</strong> (Int64List[1]): 目标输出大小 H（单个整数）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 64, 5])</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.AdaptiveAvgPool2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">AdaptiveAvgPool2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.AdaptiveAvgPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个平面组成的的信号 <cite>input</cite> 上应用 2D 自适应平均池化。</p>
<p>对于任何大小的输入，输出大小都是 H x W 。</p>
<p>输出的数量等于输入平面的数量。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>output_size</strong> (Int64List[2]): 目标输出大小（单个整数 H 或包含两个整数的元组 (H, W) ）。 H 和 W 可以是 <code class="docutils literal notranslate"><span class="pre">int</span></code> 也可以是 <code class="docutils literal notranslate"><span class="pre">None</span></code> ，如果为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 则大小将和输入大小一致。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 64, 5, 7])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 64, 7, 7])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="bp">None</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 64, 10, 7])</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.AdaptiveAvgPool3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">AdaptiveAvgPool3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.AdaptiveAvgPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个平面组成的信号 <cite>input</cite> 上应用 3D 自适应平均池化。</p>
<p>对于任何大小的输入，输出大小都是 D x H x W 。</p>
<p>输出的数量等于输入平面的数量。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>output_size</strong> (Int64List[3]): 目标输出大小（单个整数 D 则为 D x D x D 或包含三个整数的元组 (D, H, W) ）。 H 、 W 和 D 可以是 <code class="docutils literal notranslate"><span class="pre">int</span></code> 也可以是 <code class="docutils literal notranslate"><span class="pre">None</span></code> ，如果为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 则大小将和输入大小一致。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool3d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">9</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 64, 5, 7, 9])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool3d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 64, 7, 7, 7])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool3d</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 64, 7, 9, 8])</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.AvgPool1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">AvgPool1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">count_include_pad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.AvgPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个平面组成的信号 <cite>input</cite> 上执行 1D 平均池化。
在最简单的情况下，输出值是输入大小为 <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> 的层。
输出 <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> 和 <cite>kernel_size</cite> ， <span class="math notranslate nohighlight">\(k\)</span> 可以被精确地描述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out(N_i, C_j, l)  = \frac{1}{k} \sum_{m=0}^{k-1}
                    input(N_i, C_j, stride[0] \times h + m, stride*l + m)\]</div></div>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 非零，则输入在两侧隐式填充 0 以填充点数。</p>
<p>参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 可以为 int 或者单元素元组。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>当 <code class="xref py py-attr docutils literal notranslate"><span class="pre">ceil_mode</span></code> 为 True 时，如果滑动窗口在 left padding 或输入内开始，则允许滑动窗口出界。忽略在右侧填充区域开始的滑动窗口。</p>
</div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>kernel_size</strong> (Union[int, Tuple[int, int]]): 窗口的大小</p></li>
<li><p><strong>strides</strong> (Union[int, Tuple[int, int]], 可选): 窗口的 stride 。默认值为 None</p></li>
<li><p><strong>padding</strong> (Union[int, Tuple[int, int]]): 如果非 0 ，在两侧添加隐式填充 0 。默认为 0</p></li>
<li><p><strong>ceil_mode</strong> (bool): 如果为 True ，将使用 ceil 而不是 floor 来计算输出形状。默认为 False</p></li>
<li><p><strong>count_include_pad</strong> (bool): 如果为 True ，将在平均计算中填充 0 ，默认为 True</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">AvgPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="n">oneflow</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.AvgPool1d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.AvgPool1d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.AvgPool2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">AvgPool2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">count_include_pad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">divisor_override</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.AvgPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个平面组成的信号 <cite>input</cite> 上执行 2D 平均池化。</p>
<p>在最简单的情况下，输出值是输入大小为 <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> 的层。</p>
<p>输出 <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> 和 <cite>kernel_size</cite> ， <span class="math notranslate nohighlight">\((kH, kW)\)</span> 可以被精确地描述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out(N_i, C_j, h, w)  = \frac{1}{kH * kW} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1}
                       input(N_i, C_j, stride[0] \times h + m, stride[1] \times w + n)\]</div></div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>kernel_size</strong> (Union[int, Tuple[int, int]]): 整数或长度为 1 或 2 的整数列表。输入张量的每个维度的窗口大小</p></li>
<li><p><strong>strides</strong> (Union[int, Tuple[int, int]]): 整数或长度为 1 或 2 的整数列表。输入张量的每个维度的滑动窗口的 stride 。默认为 None</p></li>
<li><p><strong>padding</strong> (Tuple[int, int]): 整数或长度为 1 或 2 的整数列表。在两侧添加隐式填充 0 。默认为 0</p></li>
<li><p><strong>ceil_mode</strong> (bool, default to False): 如果为 True 。将使用 ceil 而不是 floor 来计算输出形状。默认为 False</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="n">oneflow</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.AvgPool2d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.AvgPool2d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.AvgPool3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">AvgPool3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">count_include_pad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">divisor_override</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.AvgPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个平面组成的信号 <cite>input</cite> 上执行 3D 平均池化。在最简单的情况下，输出值是输入大小为 <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span> 的层。</p>
<p>输出 <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span> 和 <cite>kernel_size</cite> ， <span class="math notranslate nohighlight">\((kD, kH, kW)\)</span> 可以被精确地描述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out(N_i, C_j, d, h, w)  = \frac{1}{kD * kH * kW } \sum_{k=0}^{kD-1} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1}
                       input(N_i, C_j, stride[0] \times d + k, stride[1] \times h + m, stride[2] \times w + n)\]</div></div>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 非零，则输入在三侧隐式填充 0 以填充点数。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>当 <code class="xref py py-attr docutils literal notranslate"><span class="pre">ceil_mode</span></code> 为 True 时，如果滑动窗口在 left padding 或输入内开始，则允许滑动窗口出界。忽略在右侧填充区域开始的滑动窗口。</p>
</div>
<dl>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>kernel_size</strong> (Union[int, Tuple[int, int, int]]): 窗口的大小</p></li>
<li><p><strong>strides</strong> (Union[int, Tuple[int, int, int]], 可选): 窗口的 stride 。默认值为 None</p></li>
<li><p><strong>padding</strong> (Union[int, Tuple[int, int, int]]):  如果非 0 ，在三侧添加隐式填充 0 。默认为 0</p></li>
<li><p><strong>ceil_mode</strong> (bool): 如果为 True ，将使用 ceil 而不是 floor 来计算输出形状。默认为 False</p></li>
<li><p><strong>count_include_pad</strong> (bool): 如果为 True ，将在平均计算中填充 0 ，默认为 True</p></li>
<li><p><strong>divisor_override</strong> (int): 如果设定了 attr:<cite>divisor_override</cite> ，它将用作除数，否则 attr:<cite>kernel_size</cite> 将作为除数。默认为 0</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span></p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{kernel_size}[0]}{\text{stride}[0]} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{kernel_size}[1]}{\text{stride}[1]} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{kernel_size}[2]}{\text{stride}[2]} + 1\right\rfloor\]</div></div>
</li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">AvgPool3d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="n">oneflow</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">19</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.AvgPool3d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.AvgPool3d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.BCELoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">BCELoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span> → <span class="pre">Tensor</span><a class="headerlink" href="#oneflow.nn.BCELoss" title="Permalink to this definition">¶</a></dt>
<dd><p>计算二值交叉熵损失 (binary cross-entropy loss)。</p>
<p>公式为：</p>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = “none” ：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = -(Target_i*log(Input_i) + (1-Target_i)*log(1-Input_i))\]</div></div>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = “mean”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = -\frac{1}{n}\sum_{i=1}^n(Target_i*log(Input_i) + (1-Target_i)*log(1-Input_i))\]</div></div>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = “sum”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = -\sum_{i=1}^n(Target_i*log(Input_i) + (1-Target_i)*log(1-Input_i))\]</div></div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>weight</strong> (oneflow.Tensor, 可选的): 手动重新调整损失的权重。默认为 <code class="docutils literal notranslate"><span class="pre">None</span></code> ，对应的权重值为 1</p></li>
<li><p><strong>reduction</strong> (str, 可选的): reduce 的方式，可以是 “none” 、 “mean” 、 “sum” 中的一种。默认为 “mean”</p></li>
</ul>
</dd>
</dl>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>输入值必须在区间 (0, 1) 内。否则此损失函数可能返回 <cite>nan</cite> 值。</p>
</div>
<dl class="simple">
<dt>返回类型：</dt><dd><p>oneflow.tensor</p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">activation</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sigmoid_input</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">sigmoid_input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[2.9266, 1.1963, 1.1087],</span>
<span class="go">        [0.8064, 2.0750, 4.2539]], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m_sum</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m_sum</span><span class="p">(</span><span class="n">sigmoid_input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(12.3668, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m_mean</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m_mean</span><span class="p">(</span><span class="n">sigmoid_input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(2.0611, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m_none</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m_none</span><span class="p">(</span><span class="n">sigmoid_input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(1.0306, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.BCEWithLogitsLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">BCEWithLogitsLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> → <span class="pre">Tensor</span><a class="headerlink" href="#oneflow.nn.BCEWithLogitsLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>此运算将 <cite>Sigmoid</cite> 和 <cite>BCELoss</cite> 组合在一起。为了数据的稳定性，我们用了一些数学技巧，而不是将 <cite>Sigmoid</cite> 作用于 <cite>BCELoss</cite> 层。</p>
<p>公式为：</p>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = <code class="docutils literal notranslate"><span class="pre">"none"</span></code>:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = -weight*[Pos\_weight*y*log\sigma({x}) + (1-y)*log(1-\sigma(x))]\]</div></div>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = <code class="docutils literal notranslate"><span class="pre">"mean"</span></code>:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = -\frac{weight}{n}\sum_{i=1}^n[Pos\_weight*y*log\sigma({x}) + (1-y)*log(1-\sigma(x))]\]</div></div>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = <code class="docutils literal notranslate"><span class="pre">"sum"</span></code>:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = -weight*\sum_{i=1}^n[Pos\_weight*y*log\sigma({x}) + (1-y)*log(1-\sigma(x))]\]</div></div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>weight</strong> (Tensor, 可选的): 手动重新调整损失的权重。默认为 <code class="docutils literal notranslate"><span class="pre">None</span></code></p></li>
<li><p><strong>reduction</strong> (str, 可选的): reduce 的方式，可以是 <code class="docutils literal notranslate"><span class="pre">"none"</span></code> 、 <code class="docutils literal notranslate"><span class="pre">"mean"</span></code> 、 <code class="docutils literal notranslate"><span class="pre">"sum"</span></code> 中的一种。默认为 “mean” 。如果为 <code class="docutils literal notranslate"><span class="pre">'none'</span></code> 则不进行 reduce 。如果为 <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> ，输出的值的和除以元素数。如果为 <code class="docutils literal notranslate"><span class="pre">'sum'</span></code> ，输出将被求和。默认为 <code class="docutils literal notranslate"><span class="pre">"mean"</span></code></p></li>
<li><p><strong>pos_weight</strong> (Tensor, 可选的): 手动重新调整正例的权重。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N,*)\)</span> 其中 <cite>*</cite> 的意思是，可以增加任意维度</p></li>
<li><p><strong>Target</strong> : <span class="math notranslate nohighlight">\((N,*)\)</span> 与输入形状一样</p></li>
<li><p><strong>Output</strong> : 标量。如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> 为 <code class="docutils literal notranslate"><span class="pre">"none"</span></code> ，则 <span class="math notranslate nohighlight">\((N,*)\)</span> 和输入形状一样</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pos_weight</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">pos_weight</span><span class="o">=</span><span class="n">pos_weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[2.9266, 1.5552, 1.1087],</span>
<span class="go">        [0.9676, 2.0750, 5.9554],</span>
<span class="go">        [0.9676, 2.0750, 5.9554]], dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">pos_weight</span><span class="o">=</span><span class="n">pos_weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(2.6207, dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">pos_weight</span><span class="o">=</span><span class="n">pos_weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(23.5865, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.BatchNorm1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">BatchNorm1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.BatchNorm1d" title="Permalink to this definition">¶</a></dt>
<dd><p>在 2D 或 3D 输入（具有可选附加通道维度的小批量 1D 输入）上应用批归一化 (Batch Normalization) 。行为与论文 <a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift</a> 一致。</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>按小批量逐维度求平均值和标准差， <span class="math notranslate nohighlight">\(\gamma\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span> 是大小为 <cite>C</cite> 的可学习参数向量（ <cite>C</cite> 是输入的大小）。
默认情况下，<span class="math notranslate nohighlight">\(\gamma\)</span> 的元素均为 1 而 <span class="math notranslate nohighlight">\(\beta\)</span> 的元素均为 0 。标准差的计算等价于 <cite>torch.var(input, unbiased=False)</cite> 。</p>
<p>此外，默认情况下，在训练期间，该层不断估计计算的均值和方差，然后评估时将其归一化。运行估计默认 <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 为 0.1 。</p>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> 被设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> ，则该层不会继续进行估计，并且在评估时也使用批处理统计信息。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 参数不同于优化器 (optimizer) 类中使用的参数或传统的动量概念。数学上，这里的更新规则是：
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span> ，其中 <span class="math notranslate nohighlight">\(\hat{x}\)</span> 是估计的统计量， <span class="math notranslate nohighlight">\(x_t\)</span> 是新的观察值。</p>
</div>
<p>因为批归一化 (Batch Normalization) 是在 <cite>C</cite> 维度上完成的，计算 <cite>(N, L)</cite> 切片的统计数据，所以常称其为 Temporal Batch Normalization 。</p>
<dl>
<dt>参数：</dt><dd><ul>
<li><p><strong>num_features</strong> : <span class="math notranslate nohighlight">\(C\)</span> 来自于大小为 <span class="math notranslate nohighlight">\((N, C, L)\)</span> 的预期输入或 <span class="math notranslate nohighlight">\(L\)</span> 来自大小为 <span class="math notranslate nohighlight">\((N, L)\)</span> 的输入</p></li>
<li><p><strong>eps</strong> : 为数值稳定性而为分母加的值。默认为：1e-5</p></li>
<li><p><strong>momentum</strong> : 用于 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 计算的值。设定为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 则计算移动平均 (Moving average) ，默认：0.1</p></li>
<li><p><strong>affine</strong> : 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，该模块具有可学习的仿射参数。默认为 <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><dl>
<dt><strong>track_running_stats</strong><span class="classifier">当设置为 <code class="docutils literal notranslate"><span class="pre">True</span></code> 时，该模块跟踪运行均值和方差，当设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> 时，此模块不会跟踪此类统计信息，</span></dt><dd><p>并将统计缓冲区 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 初始化为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 。当这些缓冲区为“无”时，此模块在训练和评估模式中始终使用批处理统计信息。默认值： <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C)\)</span> 或 <span class="math notranslate nohighlight">\((N, C, L)\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C)\)</span> 或 <span class="math notranslate nohighlight">\((N, C, L)\)</span> （与输入形状相同）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.BatchNorm2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">BatchNorm2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.BatchNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><p>在 4D 输入（具有可选附加通道维度的小批量 2D 输入）上应用批归一化 (Batch Normalization) 。行为与论文 <a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift</a> 一致。</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>按小批量逐维度求平均值和标准差， <span class="math notranslate nohighlight">\(\gamma\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span> 是大小为 <cite>C</cite> 的可学习参数向量（ <cite>C</cite> 是输入的大小）。
默认情况下，<span class="math notranslate nohighlight">\(\gamma\)</span> 的元素均为 1 而 <span class="math notranslate nohighlight">\(\beta\)</span> 的元素均为 0 。标准差的计算等价于 <cite>torch.var(input, unbiased=False)</cite> 。</p>
<p>此外，默认情况下，在训练期间，该层不断估计计算的均值和方差，然后评估时将其归一化。运行估计默认 <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 为 0.1 。</p>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> 被设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> ，则该层不会继续进行估计，并且在评估时也使用批处理统计信息。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 参数不同于优化器 (optimizer) 类中使用的参数或传统的动量概念。数学上，这里的更新规则是：
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span> ，其中 <span class="math notranslate nohighlight">\(\hat{x}\)</span> 是估计的统计量， <span class="math notranslate nohighlight">\(x_t\)</span> 是新的观察值。</p>
</div>
<p>因为批归一化 (Batch Normalization) 是在 <cite>C</cite> 维度上完成的，计算 <cite>(N, H, W)</cite> 切片的统计数据，所以常称其为 Spatial Batch Normalization 。</p>
<dl>
<dt>参数：</dt><dd><ul>
<li><p><strong>num_features</strong> : <span class="math notranslate nohighlight">\(C\)</span> 来自于大小为 <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> 的预期输入</p></li>
<li><p><strong>eps</strong> : 为数值稳定性而为分母加的值。默认为：1e-5</p></li>
<li><p><strong>momentum</strong> : 用于 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 计算的值。设定为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 则计算移动平均 (Moving average) ，默认：0.1</p></li>
<li><p><strong>affine</strong> : 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，该模块具有可学习的仿射参数。默认为 <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><dl>
<dt><strong>track_running_stats</strong><span class="classifier">当设置为 <code class="docutils literal notranslate"><span class="pre">True</span></code> 时，该模块跟踪运行均值和方差，当设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> 时，此模块不会跟踪此类统计信息，</span></dt><dd><p>并将统计缓冲区 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 初始化为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 。当这些缓冲区为“无”时，此模块在训练和评估模式中始终使用批处理统计信息。默认值： <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C, H, W)\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> （与输入形状相同）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.BatchNorm3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">BatchNorm3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.BatchNorm3d" title="Permalink to this definition">¶</a></dt>
<dd><p>在 5D 输入（具有可选附加通道维度的小批量 3D 输入）上应用批归一化 (Batch Normalization) 。行为与论文 <a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift</a> 一致。</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>按小批量逐维度求平均值和标准差， <span class="math notranslate nohighlight">\(\gamma\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span> 是大小为 <cite>C</cite> 的可学习参数向量（ <cite>C</cite> 是输入的大小）。
默认情况下，<span class="math notranslate nohighlight">\(\gamma\)</span> 的元素均为 1 而 <span class="math notranslate nohighlight">\(\beta\)</span> 的元素均为 0 。标准差的计算等价于 <cite>torch.var(input, unbiased=False)</cite> 。</p>
<p>此外，默认情况下，在训练期间，该层不断估计计算的均值和方差，然后评估时将其归一化。运行估计默认 <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 为 0.1 。</p>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> 被设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> ，则该层不会继续进行估计，并且在评估时也使用批处理统计信息。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 参数不同于优化器 (optimizer) 类中使用的参数或传统的动量概念。数学上，这里的更新规则是：
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span> ，其中 <span class="math notranslate nohighlight">\(\hat{x}\)</span> 是估计的统计量， <span class="math notranslate nohighlight">\(x_t\)</span> 是新的观察值。</p>
</div>
<p>因为批归一化 (Batch Normalization) 是在 <cite>C</cite> 维度上完成的，计算 <cite>(N, H, W)</cite> 切片的统计数据，所以常称其为 Spatial Batch Normalization 。</p>
<dl>
<dt>参数：</dt><dd><ul>
<li><p><strong>num_features</strong> : <span class="math notranslate nohighlight">\(C\)</span> 来自于大小为 <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span> 的预期输入</p></li>
<li><p><strong>eps</strong> : 为数值稳定性而为分母加的值。默认为：1e-5</p></li>
<li><p><strong>momentum</strong> : 用于 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 计算的值。设定为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 则计算移动平均 (Moving average) ，默认：0.1</p></li>
<li><p><strong>affine</strong> : 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，该模块具有可学习的仿射参数。默认为 <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><dl>
<dt><strong>track_running_stats</strong><span class="classifier">当设置为 <code class="docutils literal notranslate"><span class="pre">True</span></code> 时，该模块跟踪运行均值和方差，当设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> 时，此模块不会跟踪此类统计信息，</span></dt><dd><p>并将统计缓冲区 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 初始化为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 。当这些缓冲区为“无”时，此模块在训练和评估模式中始终使用批处理统计信息。默认值： <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span> （与输入形状相同）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([3, 2, 5, 8, 4])</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.CELU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">CELU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.CELU" title="Permalink to this definition">¶</a></dt>
<dd><p>应用逐元素方程：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\text{CELU}(x, \alpha) = \begin{cases}
                        x &amp; \text{ if } x \ge 0  \\
        \alpha*(exp(\frac{x}{\alpha})-1) &amp; \text{ otherwise } \\
            \end{cases}\end{split}\]</div></div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>alpha</strong> (float): CELU 公式中的 <span class="math notranslate nohighlight">\(\alpha\)</span> 。默认值：1.0</p></li>
<li><p><strong>inplace</strong> (bool): 是否执行 place 操作。默认： <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N,*)\)</span> 其中 <cite>*</cite> 的意思是，可以增加任意维度</p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, *)\)</span>, 与输入相同</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">celu</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CELU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">celu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([-0.3161,  0.0000,  0.5000], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.CELU.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.CELU.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.COCOReader">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">COCOReader</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">annotation_file</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_dir</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_seed</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_by_aspect_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_images_without_annotations</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride_partition</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.device</span><span class="p"><span class="pre">,</span> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">placement</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.placement</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sbp</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.sbp.sbp</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.sbp.sbp</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.COCOReader" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.CTCLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">CTCLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">blank</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zero_infinity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.CTCLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>计算 CTC(Connectionist Temporal Classification) 损失。
此接口与 PyTorch 一致。可在 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html#torch.nn.CTCLoss">https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html#torch.nn.CTCLoss</a>
找到相关文档。</p>
<p>计算连续且未分段的时间序列和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 序列之间的损失。CTCLoss 对 <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> 与 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 可能对齐的概率求和，产生一个损失值，该值相对于每个 <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> 节点是可导的。
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> 与 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 的对齐被假定为“多对一”，这限制了 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 序列的长度，即 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> <span class="math notranslate nohighlight">\(\leq\)</span> <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> 。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>blank</strong> (int, 可选的): 空白标签。 默认 <span class="math notranslate nohighlight">\(0\)</span></p></li>
<li><dl class="simple">
<dt><strong>reduction</strong> (string, 可选的): 指定应用于输出的简化，可以是 <code class="docutils literal notranslate"><span class="pre">'none'</span></code> 、 <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> 或   <code class="docutils literal notranslate"><span class="pre">'sum'</span></code> ：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">'none'</span></code> ：不进行简化； <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> ：输出损失将除以目标长度，然后取批次的平均值。默认： <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>zero_infinity</strong> (bool, 可选的): 是否设定无限损失和相关梯度归零。默认： <code class="docutils literal notranslate"><span class="pre">False</span></code> 。</dt><dd><p>无限损失主要发生在 <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> 太短而无法与 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 对齐时</p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Log_probs</strong> : 形状为 <span class="math notranslate nohighlight">\((T, N, C)\)</span> 的张量且 <span class="math notranslate nohighlight">\(T = \text{input length}\)</span>  、 <span class="math notranslate nohighlight">\(N = \text{batch size}\)</span> 、 <span class="math notranslate nohighlight">\(C = \text{number of classes (including blank)}\)</span></p></li>
<li><p><strong>Targets</strong> : 形状为 <span class="math notranslate nohighlight">\((N, S)\)</span> 或 <span class="math notranslate nohighlight">\((\operatorname{sum}(\text{target_lengths}))\)</span> 的张量，其中 <span class="math notranslate nohighlight">\(N = \text{batch size}\)</span> 、 <span class="math notranslate nohighlight">\(S = \text{max target length, if shape is } (N, S)\)</span> 。
它代表 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 序列。 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 序列中的每个元素都是一个 class 索引。并且 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 索引不能为空（默认为 0）。在 <span class="math notranslate nohighlight">\((N, S)\)</span> 形式中，<code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 被填充到最长序列的长度并堆叠。
在 <span class="math notranslate nohighlight">\((\operatorname{sum}(\text{target_lengths}))\)</span> 形式中，我们假设目标在 1 维内未填充和连接。</p></li>
<li><p><strong>Input_lengths</strong> : 大小为 <span class="math notranslate nohighlight">\((N)\)</span> 的元组或张量，其中 <span class="math notranslate nohighlight">\(N = \text{batch size}\)</span> 。它表示 <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> 的长度（每个都必须 <span class="math notranslate nohighlight">\(\leq T\)</span> ）。假设序列被填充为相等长度的情况下，为每个序列指定长度以实现掩码。</p></li>
<li><p><strong>Target_lengths</strong> : 大小为 <span class="math notranslate nohighlight">\((N)\)</span> 的元组或张量，其中 <span class="math notranslate nohighlight">\(N = \text{batch size}\)</span> 。它代表 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 的长度。在假设序列被填充为相等长度的情况下，为每个序列指定长度以实现掩码。如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 形状是 <span class="math notranslate nohighlight">\((N,S)\)</span>，
则 target_lengths 是每个目标序列的有效停止索引 <span class="math notranslate nohighlight">\(s_n\)</span> ，这样每个目标序列都满足 <code class="docutils literal notranslate"><span class="pre">target_n</span> <span class="pre">=</span> <span class="pre">targets[n,0:s_n]</span></code> ，长度都必须 <span class="math notranslate nohighlight">\(\leq S\)</span> 。
如果目标是作为单个目标的串联的 1d 张量给出的，则 target_lengths 必须加起来为张量的总长度。</p></li>
</ul>
</dd>
<dt>参考文献：</dt><dd><p>A. Graves et al.: Connectionist Temporal Classification:
Labelling Unsegmented Sequence Data with Recurrent Neural Networks:
<a class="reference external" href="https://www.cs.toronto.edu/~graves/icml_2006.pdf">https://www.cs.toronto.edu/~graves/icml_2006.pdf</a></p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">log_probs</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>   <span class="p">[</span>
<span class="gp">... </span>       <span class="p">[[</span><span class="o">-</span><span class="mf">1.1031</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7998</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5200</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.9808</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1363</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1908</span><span class="p">]],</span>
<span class="gp">... </span>       <span class="p">[[</span><span class="o">-</span><span class="mf">1.2258</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0665</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0153</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.1135</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2331</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9671</span><span class="p">]],</span>
<span class="gp">... </span>       <span class="p">[[</span><span class="o">-</span><span class="mf">1.3348</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6611</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5118</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.9823</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2355</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0941</span><span class="p">]],</span>
<span class="gp">... </span>       <span class="p">[[</span><span class="o">-</span><span class="mf">1.3850</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3273</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7247</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.8235</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4783</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0994</span><span class="p">]],</span>
<span class="gp">... </span>       <span class="p">[[</span><span class="o">-</span><span class="mf">0.9049</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8867</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6962</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.4938</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3630</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6547</span><span class="p">]],</span>
<span class="gp">... </span>   <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">targets</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_lengths</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_lengths</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_mean</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CTCLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">loss_mean</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(1.1376, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_sum</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CTCLoss</span><span class="p">(</span><span class="n">blank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">loss_sum</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(6.8257, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.CoinFlip">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">CoinFlip</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_seed</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">probability</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.device</span><span class="p"><span class="pre">,</span> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">placement</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.placement</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sbp</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.sbp.sbp</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.sbp.sbp</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.CoinFlip" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.CombinedMarginLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">CombinedMarginLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">m1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">m2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">m3</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.CombinedMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>以下操作在 InsightFace 中实现了 “margin_softmax” ：
<a class="reference external" href="https://github.com/deepinsight/insightface/blob/master/recognition/arcface_mxnet/train.py">https://github.com/deepinsight/insightface/blob/master/recognition/arcface_mxnet/train.py</a>
InsightFace 中 margin_softmax 的实现是由多个算子组成的。
我们将它们组合在一起以加快速度。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>input</strong> (oneflow.Tensor): 输入张量</p></li>
<li><p><strong>label</strong> (oneflow.Tensor): 数据类型为整数的标签</p></li>
<li><p><strong>m1</strong> (float): 损失参数 m1</p></li>
<li><p><strong>m2</strong> (float): 损失参数 m2</p></li>
<li><p><strong>m3</strong> (float): 损失参数 m3</p></li>
</ul>
</dd>
<dt>返回类型：</dt><dd><p>oneflow.tensor</p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.7027179</span><span class="p">,</span> <span class="mf">0.0230609</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.02721931</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.16056311</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.4565852</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.64471215</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_func</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CombinedMarginLoss</span><span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[-0.0423,  0.0231],</span>
<span class="go">        [-0.0272,  0.1237],</span>
<span class="go">        [-0.4566, -0.0204]], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ConstantPad1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ConstantPad1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">padding</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ConstantPad1d" title="Permalink to this definition">¶</a></dt>
<dd><p>用常数值填充输入 tensor 边界。此接口与 PyTorch 一致，参考：<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad1d.html?highlight=constantpad1d#torch.nn.ConstantPad1d">https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad1d.html?highlight=constantpad1d#torch.nn.ConstantPad1d</a></p>
<p>用 <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code> 来进行 <cite>N</cite> 维填充。</p>
<dl>
<dt>参数:</dt><dd><ul class="simple">
<li><p><strong>padding</strong> (int, list, tuple): 填充的大小。如果数据类型为 <cite>int</cite> 则在两个边界中使用相同的填充。如果是 2-<cite>tuple</cite> ，则 (<span class="math notranslate nohighlight">\(\text{padding_left}\)</span>, <span class="math notranslate nohighlight">\(\text{padding_right}\)</span>)</p></li>
<li><p><strong>value</strong> (int, float): 用于填充的常量值。默认为 0</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul>
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C, W_{in})\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C, W_{out})\)</span> 其中</p>
<p><span class="math notranslate nohighlight">\(W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}\)</span></p>
</li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad1d</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">value</span><span class="o">=</span><span class="mf">9.9999</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([[[9.9999, 0.0000, 1.0000, 9.9999, 9.9999],</span>
<span class="go">         [9.9999, 2.0000, 3.0000, 9.9999, 9.9999]],</span>

<span class="go">        [[9.9999, 4.0000, 5.0000, 9.9999, 9.9999],</span>
<span class="go">         [9.9999, 6.0000, 7.0000, 9.9999, 9.9999]]], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ConstantPad2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ConstantPad2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">padding</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ConstantPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>此接口与 PyTorch 一致。文档可以参考：
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad2d.html">https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad2d.html</a></p>
<p>用 0 填充输入张量边界。用户可以通过设置参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">paddings</span></code> 来设置填充量。</p>
<dl>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>padding</strong> (int 或 tuple): 填充的大小。如果是 <cite>int</cite>，则在所有边界中使用相同的填充。如果是 4-<cite>tuple</cite> ，则(<span class="math notranslate nohighlight">\(\mathrm{padding_{left}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{right}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{top}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{bottom}}\)</span>)</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul>
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> 其中</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(H_{out} = H_{in} + \mathrm{padding_{top}} + \mathrm{padding_{bottom}}\)</span></p>
<p><span class="math notranslate nohighlight">\(W_{out} = W_{in} + \mathrm{padding_{left}} + \mathrm{padding_{right}}\)</span></p>
</div></blockquote>
</li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m1</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ZeroPad2d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m2</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ZeroPad2d</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m1</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="go">oneflow.Size([1, 2, 7, 7])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([[[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  1.,  2.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  3.,  4.,  5.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  6.,  7.,  8.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]],</span>

<span class="go">         [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  9., 10., 11.,  0.,  0.],</span>
<span class="go">          [ 0.,  0., 12., 13., 14.,  0.,  0.],</span>
<span class="go">          [ 0.,  0., 15., 16., 17.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]]]], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m2</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([[[[ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  1.,  2.,  0.,  0.],</span>
<span class="go">          [ 0.,  3.,  4.,  5.,  0.,  0.],</span>
<span class="go">          [ 0.,  6.,  7.,  8.,  0.,  0.]],</span>

<span class="go">         [[ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  9., 10., 11.,  0.,  0.],</span>
<span class="go">          [ 0., 12., 13., 14.,  0.,  0.],</span>
<span class="go">          [ 0., 15., 16., 17.,  0.,  0.]]]], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ConstantPad3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ConstantPad3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">padding</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ConstantPad3d" title="Permalink to this definition">¶</a></dt>
<dd><p>用常数值填充输入 tensor 边界。此接口与 PyTorch 一致，参考：<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad1d.html?highlight=constantpad1d#torch.nn.ConstantPad1d">https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad1d.html?highlight=constantpad1d#torch.nn.ConstantPad1d</a></p>
<p>用 <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code> 来进行 <cite>N</cite> 维填充。</p>
<dl>
<dt>参数:</dt><dd><ul class="simple">
<li><p><strong>padding</strong> (int, list, tuple): 填充的大小。如果数据类型为 <cite>int</cite> 则在所有边界中使用相同的填充。如果是 6-<cite>tuple</cite> ，则 ( <span class="math notranslate nohighlight">\(\text{padding_left}\)</span> , <span class="math notranslate nohighlight">\(\text{padding_right}\)</span> , <span class="math notranslate nohighlight">\(\text{padding_top}\)</span> , <span class="math notranslate nohighlight">\(\text{padding_bottom}\)</span> , <span class="math notranslate nohighlight">\(\text{padding_front}\)</span> , <span class="math notranslate nohighlight">\(\text{padding_back}\)</span> )</p></li>
<li><p><strong>value</strong> (int, float): 用于填充的常量值。默认为 0</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span> 其中</p>
<p><span class="math notranslate nohighlight">\(D_{out} = D_{in} + \text{padding_front} + \text{padding_back}\)</span></p>
<p><span class="math notranslate nohighlight">\(H_{out} = H_{in} + \text{padding_top} + \text{padding_bottom}\)</span></p>
<p><span class="math notranslate nohighlight">\(W_{out} = W_{in} + \text{padding_left} + \text{padding_right}\)</span></p>
</li>
</ul>
</dd>
<dt>示例：</dt><dd><div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="k">as</span> <span class="nn">flow</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad3d</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([[[[[9, 9, 9, 9],</span>
<span class="go">           [9, 9, 9, 9],</span>
<span class="go">           [9, 9, 9, 9],</span>
<span class="go">           [9, 9, 9, 9]],</span>

<span class="go">          [[9, 9, 9, 9],</span>
<span class="go">           [9, 0, 1, 9],</span>
<span class="go">           [9, 2, 3, 9],</span>
<span class="go">           [9, 9, 9, 9]],</span>

<span class="go">          [[9, 9, 9, 9],</span>
<span class="go">           [9, 4, 5, 9],</span>
<span class="go">           [9, 6, 7, 9],</span>
<span class="go">           [9, 9, 9, 9]],</span>

<span class="go">          [[9, 9, 9, 9],</span>
<span class="go">           [9, 9, 9, 9],</span>
<span class="go">           [9, 9, 9, 9],</span>
<span class="go">           [9, 9, 9, 9]]]]], dtype=oneflow.int32)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Conv1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Conv1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Conv1d" title="Permalink to this definition">¶</a></dt>
<dd><p>此接口与 PyTorch 一致。
文档参考自：<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Conv1d.html#conv1d">https://pytorch.org/docs/master/generated/torch.nn.Conv1d.html#conv1d</a></p>
<p>对由多个平面组成的输入信号应用 1D 卷积。</p>
<p>在最简单的情况下，大小为 <span class="math notranslate nohighlight">\((N, C_{\text{in}}, L)\)</span> 的输入层的输出值和输出 <span class="math notranslate nohighlight">\((N, C_{\text{out}}, L_{\text{out}})\)</span> 可以被准确的表述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +
\sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{\text{out}_j}, k)
\star \text{input}(N_i, k)\]</div></div>
<p>其中 <span class="math notranslate nohighlight">\(\star\)</span> 为有效的 <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> 运算符， <span class="math notranslate nohighlight">\(N\)</span> 是批量大小， <span class="math notranslate nohighlight">\(C\)</span> 表示通道数，
<span class="math notranslate nohighlight">\(L\)</span> 是信号序列的长度。</p>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> 是控制互相关 (cross-correlation) 的步幅 (stride) 的单个数字或单元素元组。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 控制应用于输入的填充量。可以是 <cite>string</cite> {{‘valid’, ‘same’}}
或一个给出在两侧的隐式填充量的整数元组。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 控制核心点 (kernel points) 之间的间距，也称为 <cite>à trous algorithm</cite> 。此操作很难描述，
但是 <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> 很好的将 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 的作用可视化。</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">padding='valid'</span></code> 等同于无填充。 <code class="docutils literal notranslate"><span class="pre">padding='same'</span></code> 填充输入，使输出具有与输入相同的形状。
但是此种情况下不支持除了 1 以外的任何步幅 (stride) 值。</p>
</div>
<dl>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>in_channels</strong> (int): 输入图像的通道数</p></li>
<li><p><strong>out_channels</strong> (int): 卷积产生的通道数</p></li>
<li><p><strong>kernel_size</strong> (int 或者 tuple): 卷积核的大小</p></li>
<li><p><strong>stride</strong> (int 或者 tuple, 可选的): 卷积的步幅 (stride) 。默认： 1</p></li>
<li><p><strong>padding</strong> (int, tuple 或者 str, 可选的): 添加到输入两侧的填充值。默认： 0</p></li>
<li><p><strong>padding_mode</strong> (string, 可选的): 默认： <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code></p></li>
<li><p><strong>dilation</strong> (int 或者 tuple, 可选的): 核心的元素之间的间距。默认： 1</p></li>
<li><p><strong>groups</strong> (int, 可选的): 从输入通道到输出通道的 <cite>blocked connections</cite> 数。默认：1</p></li>
<li><p><strong>bias</strong> (bool, 可选的): 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则向输出添加可学习的偏差。默认：<code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul>
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C_{in}, L_{in})\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C_{out}, L_{out})\)</span> 其中</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[L_{out} = \left\lfloor\frac{L_{in} + 2 \times \text{padding} - \text{dilation}
          \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor\]</div></div>
</li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="oneflow.nn.Conv1d.weight">
<code class="sig-name descname"><span class="pre">weight</span></code><a class="headerlink" href="#oneflow.nn.Conv1d.weight" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 <span class="math notranslate nohighlight">\((\text{out\_channels},
\frac{\text{in\_channels}}{\text{groups}}, \text{kernel\_size})\)</span> 的模块可学习权重。
这些权重的值是由公式 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 计算而来，其中
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{in} * \text{kernel\_size}}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.Conv1d.bias">
<code class="sig-name descname"><span class="pre">bias</span></code><a class="headerlink" href="#oneflow.nn.Conv1d.bias" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 (out_channels) 的模块可学习权重。如果 <a class="reference internal" href="#oneflow.nn.Conv1d.bias" title="oneflow.nn.Conv1d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> 为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，
则那么这些权重的值是由公式 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 计算而来，其中
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{in} * \text{kernel\_size}}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Conv1d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Conv1d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
<dl class="py method">
<dt id="oneflow.nn.Conv1d.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.Conv1d.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Conv2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Conv2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Conv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>此接口与 PyTorch 一致。
文档参考自：<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html#conv2d">https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html#conv2d</a></p>
<p>对由多个平面组成的输入信号应用 2D 卷积。</p>
<p>在最简单的情况下，大小为 <span class="math notranslate nohighlight">\((N, C_{\text{in}}, H, W)\)</span> 的输入层的输出值和输出
<span class="math notranslate nohighlight">\((N, C_{\text{out}}, H_{\text{out}}, W_{\text{out}})\)</span> 可以被准确的表述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +
\sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)\]</div></div>
<p>其中 <span class="math notranslate nohighlight">\(\star\)</span> 为有效的 2D <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> 运算符， <span class="math notranslate nohighlight">\(N\)</span> 是批量大小， <span class="math notranslate nohighlight">\(C\)</span> 表示通道数，
<span class="math notranslate nohighlight">\(H\)</span> 是以像素为单位的输入平面的高度，和 <span class="math notranslate nohighlight">\(W\)</span> 是以像素为单位的宽度。</p>
<ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> 是控制互相关 (cross-correlation) 的步幅 (stride) 的单个数字或单元素元组。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 控制在输入每个维度两侧隐式填充 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 个点。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 控制核心点 (kernel points) 之间的间距，也称为 <cite>à trous algorithm</cite> 。此操作很难描述，
但是 <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> 很好的将 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 的作用可视化。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> 控制输入和输出之间的连接。 <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> 都必须能被 <code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> 整除。
例如，</p>
<blockquote>
<div><ul class="simple">
<li><p>当 groups=1 时，所有输入都卷积到输出。</p></li>
<li><p>当 groups=2 时，该操作等效于并排放置两个 conv 层，其中每个层检查一半的输入通道并产生一半的输出通道，然后将两者连接起来。</p></li>
<li><p>当 groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> 时， 每个输入通道都与它自己的一组过滤器(大小为
<span class="math notranslate nohighlight">\(\frac{\text{out_channels}}{\text{in_channels}}\)</span>)进行卷积。</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 可以是：</p>
<blockquote>
<div><ul class="simple">
<li><p>单个 <code class="docutils literal notranslate"><span class="pre">int</span></code> – 在这种情况下，高度和宽度使用相同的值</p></li>
<li><p>两个整数的``tuple`` – 在这种情况下，第一个 <cite>int</cite> 用于高度，第二个 <cite>int</cite> 用于宽度</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>当 <cite>groups == in_channels</cite> 并且 <cite>out_channels == K * in_channels</cite> 时，其中 <cite>K</cite> 是一个正整数，这个操作被称为“深度卷积”。</p>
<p>换句话说，对于大小为 <span class="math notranslate nohighlight">\((N, C_{in}, L_{in})\)</span> 的输入，可以使用参数 <span class="math notranslate nohighlight">\((C_\text{in}=C_\text{in}, C_\text{out}=C_\text{in} \times \text{K}, ..., \text{groups}=C_\text{in})\)</span>
执行具有深度乘数 <cite>K</cite> 的深度卷积。</p>
</div>
<dl>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>in_channels</strong> (int): 输入图像的通道数</p></li>
<li><p><strong>out_channels</strong> (int): 卷积产生的通道数</p></li>
<li><p><strong>kernel_size</strong> (int 或者 tuple): 卷积核的大小</p></li>
<li><p><strong>stride</strong> (int 或者 tuple, 可选的): 卷积的步幅 (stride) 。默认： 1</p></li>
<li><p><strong>padding</strong> (int, tuple 或者 str, 可选的): 添加到输入两侧的填充值。默认： 0</p></li>
<li><p><strong>padding_mode</strong> (string, 可选的): 默认： <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code></p></li>
<li><p><strong>dilation</strong> (int 或者 tuple, 可选的): 核心的元素之间的间距。默认： 1</p></li>
<li><p><strong>groups</strong> (int, 可选的): 从输入通道到输出通道的 <cite>blocked connections</cite> 数。默认：1</p></li>
<li><p><strong>bias</strong> (bool, 可选的): 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则向输出添加可学习的偏差。默认：<code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul>
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span> 其中</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] - \text{dilation}[0]
          \times (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] - \text{dilation}[1]
          \times (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor\]</div></div>
</li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="oneflow.nn.Conv2d.weight">
<code class="sig-name descname"><span class="pre">weight</span></code><a class="headerlink" href="#oneflow.nn.Conv2d.weight" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 <span class="math notranslate nohighlight">\((\text{out_channels}, \frac{\text{in_channels}}{\text{groups}},\)</span>
<span class="math notranslate nohighlight">\(\text{kernel_size[0]}, \text{kernel_size[1]})\)</span> 的模块可学习权重。
这些权重的值是由公式 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 计算而来，其中
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{in} * \prod_{i=0}^{1}\text{kernel_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.Conv2d.bias">
<code class="sig-name descname"><span class="pre">bias</span></code><a class="headerlink" href="#oneflow.nn.Conv2d.bias" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 (out_channels) 的模块可学习权重。如果 <a class="reference internal" href="#oneflow.nn.Conv2d.bias" title="oneflow.nn.Conv2d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> 为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，
则那么这些权重的值是由公式 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 计算而来，其中
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{in} * \prod_{i=0}^{1}\text{kernel_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Conv2d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Conv2d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
<dl class="py method">
<dt id="oneflow.nn.Conv2d.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.Conv2d.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Conv3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Conv3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Conv3d" title="Permalink to this definition">¶</a></dt>
<dd><p>此接口与 PyTorch 一致。
文档参考自：<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Conv3d.html#conv3d">https://pytorch.org/docs/master/generated/torch.nn.Conv3d.html#conv3d</a></p>
<p>对由多个平面组成的输入信号应用 3D 卷积。</p>
<p>在最简单的情况下，大小为 <span class="math notranslate nohighlight">\((N, C_{in}, D, H, W)\)</span> 的输入层的输出值和输出
<span class="math notranslate nohighlight">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> 可以被准确的表述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out(N_i, C_{out_j}) = bias(C_{out_j}) +
                        \sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \star input(N_i, k)\]</div></div>
<p>其中 <span class="math notranslate nohighlight">\(\star\)</span> 为有效的 3D <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> 运算符。</p>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> 是控制互相关 (cross-correlation) 的步幅 (stride) 的单个数字或单元素元组。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 控制应用于输入的填充量。可以是 <cite>string</cite> {{‘valid’, ‘same’}}
或一个给出在两侧的隐式填充量的整数元组。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 控制核心点 (kernel points) 之间的间距，也称为 <cite>à trous algorithm</cite> 。此操作很难描述，
但是 <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> 很好的将 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 的作用可视化。</p></li>
</ul>
<p>参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 可以是：</p>
<blockquote>
<div><ul class="simple">
<li><p>单个 <code class="docutils literal notranslate"><span class="pre">int</span></code> – 在这种情况下，长度、宽度和高度使用相同的值</p></li>
<li><p>两个整数的``tuple`` – 在这种情况下，第一个 <cite>int</cite> 用于长度，第二个 <cite>int</cite> 用于高度，第三个 <cite>int</cite> 用于宽度。</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">padding='valid'</span></code> 等同于无填充。 <code class="docutils literal notranslate"><span class="pre">padding='same'</span></code> 填充输入，使输出具有与输入相同的形状。
但是此种情况下不支持除了 1 以外的任何步幅 (stride) 值。</p>
</div>
<dl>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>in_channels</strong> (int): 输入图像的通道数</p></li>
<li><p><strong>out_channels</strong> (int): 卷积产生的通道数</p></li>
<li><p><strong>kernel_size</strong> (int 或者 tuple): 卷积核的大小</p></li>
<li><p><strong>stride</strong> (int 或者 tuple, 可选的): 卷积的步幅 (stride) 。默认： 1</p></li>
<li><p><strong>padding</strong> (int, tuple 或者 str, 可选的): 添加到输入两侧的填充值。默认： 0</p></li>
<li><p><strong>padding_mode</strong> (string, 可选的): 默认： <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code></p></li>
<li><p><strong>dilation</strong> (int 或者 tuple, 可选的): 核心的元素之间的间距。默认： 1</p></li>
<li><p><strong>groups</strong> (int, 可选的): 从输入通道到输出通道的 <cite>blocked connections</cite> 数。默认：1</p></li>
<li><p><strong>bias</strong> (bool, 可选的): 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则向输出添加可学习的偏差。默认：<code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul>
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> 其中</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0]
      \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1]
      \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2]
      \times (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor\]</div></div>
</li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="oneflow.nn.Conv3d.weight">
<code class="sig-name descname"><span class="pre">weight</span></code><a class="headerlink" href="#oneflow.nn.Conv3d.weight" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 <span class="math notranslate nohighlight">\((\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}},\)</span>
<span class="math notranslate nohighlight">\(\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})\)</span> 的模块可学习权重。
这些权重的值是由公式 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 计算而来，其中
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.Conv3d.bias">
<code class="sig-name descname"><span class="pre">bias</span></code><a class="headerlink" href="#oneflow.nn.Conv3d.bias" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 (out_channels) 的模块可学习权重。如果 <a class="reference internal" href="#oneflow.nn.Conv3d.bias" title="oneflow.nn.Conv3d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> 为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，
则那么这些权重的值是由公式 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 计算而来，其中
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Conv3d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Conv3d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
<dl class="py method">
<dt id="oneflow.nn.Conv3d.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.Conv3d.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ConvTranspose1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ConvTranspose1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ConvTranspose1d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个输入平面组成的输入图像上应用 1D 转置卷积算子。</p>
<p>该 module 可以看作是 Conv1d 相对于其输入的梯度。它也称为分数步幅卷积或反卷积（尽管它实际上不是反卷积操作）。</p>
<p>此 module 支持 TensorFloat32 。</p>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> 控制互相关 (cross-correlation) 的步幅 (stride) 。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 控制应用于输入两侧，点的数量为 <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> 的隐式 0 填充。
更多细节请参考 <code class="docutils literal notranslate"><span class="pre">note</span></code> 。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code>  控制添加到输出形状一侧的大小。更多信息请参考 <code class="docutils literal notranslate"><span class="pre">note</span></code> 。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 控制核心点 (kernel points) 之间的间距，也称为 <cite>à trous algorithm</cite> 。此操作很难描述，
但是 <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> 很好的将 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 的作用可视化。</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 参数有效地将 <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> 个 0 填充到输入的两侧。
设定此项的目的是当 <code class="xref py py-class docutils literal notranslate"><span class="pre">Conv1d</span></code> 与 <code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose1d</span></code> 用相同的参数初始化时，
它们的输入和输出的形状是互逆的。然而，当 <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">&gt;</span> <span class="pre">1</span></code> 时， <code class="xref py py-class docutils literal notranslate"><span class="pre">Conv1d</span></code>
将多个输入形状映射到相同的输出形状。则使用 <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> 有效地增加一侧的输出形状来解决这种歧义。
请注意，<code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> 仅用于查找输出形状，但实际上并未填充输出。</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>在某些情况下，将 CUDA 后端与 CuDNN 一起使用时，此运算符可能会选择非确定性算法来提高性能。
如果此操作有不确定性，您可以尝试通过设置 <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code> 来使操作具有确定性（可能以性能为代价）。
背景请参阅有关随机性 (randomness)  的 note 。</p>
</div>
<dl>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>in_channels</strong> (int): 输入图像的通道数</p></li>
<li><p><strong>out_channels</strong> (int): 卷积产生的通道数</p></li>
<li><p><strong>kernel_size</strong> (int 或 tuple): 卷积核的大小</p></li>
<li><p><strong>stride</strong> (int 或 tuple, 可选的): 卷积的步幅 (stride) 。默认： 1</p></li>
<li><p><strong>padding</strong> (int 或 tuple, 可选的): 添加到输入每侧的 <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> 大小的 0 填充值。默认： 0</p></li>
<li><p><strong>output_padding</strong> (int 或 tuple, 可选的): 添加到输出形状一侧的大小。默认：0</p></li>
<li><p><strong>groups</strong> (int, 可选的): 从输入通道到输出通道的 <cite>blocked connections</cite> 数。默认：1</p></li>
<li><p><strong>bias</strong> (bool, 可选的): 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则向输出添加可学习的偏差。默认：<code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>dilation</strong> (int 或 tuple, 可选的): 核心的元素之间的间距。默认： 1</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul>
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C_{in}, L_{in})\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C_{out}, L_{out})\)</span> 其中</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[L_{out} = (L_{in} - 1) \times \text{stride} - 2 \times \text{padding} + \text{dilation}
          \times (\text{kernel_size} - 1) + \text{output_padding} + 1\]</div></div>
</li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="oneflow.nn.ConvTranspose1d.weight">
<code class="sig-name descname"><span class="pre">weight</span></code><a class="headerlink" href="#oneflow.nn.ConvTranspose1d.weight" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 <span class="math notranslate nohighlight">\((\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},\)</span>
<span class="math notranslate nohighlight">\(\text{kernel\\_size})\)</span> 的模块可学习权重。
这些权重的值是由公式 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 计算而来，其中
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{out} * \text{kernel\_size}}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.ConvTranspose1d.bias">
<code class="sig-name descname"><span class="pre">bias</span></code><a class="headerlink" href="#oneflow.nn.ConvTranspose1d.bias" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 (out_channels) 的模块可学习权重。如果 <a class="reference internal" href="#oneflow.nn.ConvTranspose1d.bias" title="oneflow.nn.ConvTranspose1d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> 为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，
则那么这些权重的值是由公式 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 计算而来，其中
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{out} * \text{kernel\_size}}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="oneflow.nn.ConvTranspose1d.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.ConvTranspose1d.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ConvTranspose2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ConvTranspose2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ConvTranspose2d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个输入平面组成的输入图像上应用 2D 转置卷积算子。</p>
<p>该 module 可以看作是 Conv2d 相对于其输入的梯度。它也称为分数步幅卷积或反卷积（尽管它实际上不是反卷积操作）。</p>
<dl>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>in_channels</strong> (int): 输入图像的通道数</p></li>
<li><p><strong>out_channels</strong> (int): 卷积产生的通道数</p></li>
<li><p><strong>kernel_size</strong> (int 或 tuple): 卷积核的大小</p></li>
<li><p><strong>stride</strong> (int 或 tuple, 可选的): 卷积的步幅 (stride) 。默认： 1</p></li>
<li><p><strong>padding</strong> (int 或 tuple, 可选的): 添加到输入每侧的 <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> 大小的 0 填充值。默认： 0</p></li>
<li><p><strong>output_padding</strong> (int 或 tuple, 可选的): 添加到输出形状一侧的大小。默认：0</p></li>
<li><p><strong>groups</strong> (int, 可选的): 从输入通道到输出通道的 <cite>blocked connections</cite> 数。默认：1</p></li>
<li><p><strong>bias</strong> (bool, 可选的): 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则向输出添加可学习的偏差。默认：<code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>dilation</strong> (int 或 tuple, 可选的): 核心的元素之间的间距。默认： 1</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span> 其中</p></li>
</ul>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]\\          \times (\text{kernel_size}[0] - 1) + \text{output_padding}[0] + 1\end{aligned}\end{align} \]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}W_{out} = (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]\\          \times (\text{kernel_size}[1] - 1) + \text{output_padding}[1] + 1\end{aligned}\end{align} \]</div></div>
</dd>
</dl>
<dl class="py attribute">
<dt id="oneflow.nn.ConvTranspose2d.weight">
<code class="sig-name descname"><span class="pre">weight</span></code><a class="headerlink" href="#oneflow.nn.ConvTranspose2d.weight" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 <span class="math notranslate nohighlight">\((\text{in_channels}, \frac{\text{out_channels}}{\text{groups}},\)</span>
<span class="math notranslate nohighlight">\(\text{kernel_size[0]}, \text{kernel_size[1]})\)</span> 的模块可学习权重。
这些权重的值是由公式 <cite>mathcal{U}(-sqrt{k}, sqrt{k})</cite> 计算而来，其中
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{out} * \prod_{i=0}^{1}\text{kernel_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.ConvTranspose2d.bias">
<code class="sig-name descname"><span class="pre">bias</span></code><a class="headerlink" href="#oneflow.nn.ConvTranspose2d.bias" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 (out_channels) 的模块可学习权重。如果 <a class="reference internal" href="#oneflow.nn.ConvTranspose2d.bias" title="oneflow.nn.ConvTranspose2d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> 为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，
则那么这些权重的值是由公式 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 计算而来，其中
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{out} * \text{kernel\_size}}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([20, 33, 93, 100])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.ConvTranspose2d.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.ConvTranspose2d.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ConvTranspose3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ConvTranspose3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ConvTranspose3d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个输入平面组成的输入图像上应用 3D 转置卷积算子。</p>
<p>转置卷积算子将每个输入值逐元素乘以一个可学习的内核 (kernel) ，并对所有输入特征平面的输出求和。</p>
<p>该 module 可以看作是 Conv3d 相对于其输入的梯度。它也称为分数步幅卷积或反卷积（尽管它实际上不是反卷积操作）。</p>
<p>此 module 支持 TensorFloat32 。</p>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> 控制互相关 (cross-correlation) 的步幅 (stride) 。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 控制应用于输入两侧，点的数量为 <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> 的隐式 0 填充。
更多细节请参考 <code class="docutils literal notranslate"><span class="pre">note</span></code> 。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code>  控制添加到输出形状一侧的大小。更多信息请参考 <code class="docutils literal notranslate"><span class="pre">note</span></code> 。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 控制核心点 (kernel points) 之间的间距，也称为 <cite>à trous algorithm</cite> 。此操作很难描述，
但是 <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> 很好的将 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 的作用可视化。</p></li>
</ul>
<p>参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> 可以是以下形式：</p>
<blockquote>
<div><ul class="simple">
<li><p>单个 <code class="docutils literal notranslate"><span class="pre">int</span></code> – 在这种情况下，长度、高度和宽度尺寸使用相同的值</p></li>
<li><p>三个整数的 <code class="docutils literal notranslate"><span class="pre">tuple</span></code> – 在这种情况下，第一个 <cite>int</cite> 用于长度，第二个 <cite>int</cite> 表示高度，第三个 <cite>int</cite> 表示宽度</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 参数有效地将 <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> 个 0 填充到输入的两侧。
设定此项的目的是当 <code class="xref py py-class docutils literal notranslate"><span class="pre">Conv3d</span></code> 与 <code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose3d</span></code> 用相同的参数初始化时，
它们的输入和输出的形状是互逆的。然而，当 <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">&gt;</span> <span class="pre">1</span></code> 时， <code class="xref py py-class docutils literal notranslate"><span class="pre">Conv3d</span></code>
将多个输入形状映射到相同的输出形状。则使用 <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> 有效地增加一侧的输出形状来解决这种歧义。
请注意，<code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> 仅用于查找输出形状，但实际上并未填充输出。</p>
</div>
<dl>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>in_channels</strong> (int): 输入图像的通道数</p></li>
<li><p><strong>out_channels</strong> (int): 卷积产生的通道数</p></li>
<li><p><strong>kernel_size</strong> (int 或 tuple): 卷积核的大小</p></li>
<li><p><strong>stride</strong> (int 或 tuple, 可选的): 卷积的步幅 (stride) 。默认： 1</p></li>
<li><p><strong>padding</strong> (int 或 tuple, 可选的): 添加到输入每侧的 <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> 大小的 0 填充值。默认： 0</p></li>
<li><p><strong>output_padding</strong> (int 或 tuple, 可选的): 添加到输出形状一侧的大小。默认：0</p></li>
<li><p><strong>groups</strong> (int, 可选的): 从输入通道到输出通道的 <cite>blocked connections</cite> 数。默认：1</p></li>
<li><p><strong>bias</strong> (bool, 可选的): 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则向输出添加可学习的偏差。默认：<code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>dilation</strong> (int 或 tuple, 可选的): 核心的元素之间的间距。默认： 1</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> 其中</p></li>
</ul>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[D_{out} = (D_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]
          \times (\text{kernel_size}[0] - 1) + \text{output_padding}[0] + 1\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = (H_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]
          \times (\text{kernel_size}[1] - 1) + \text{output_padding}[1] + 1\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = (W_{in} - 1) \times \text{stride}[2] - 2 \times \text{padding}[2] + \text{dilation}[2]
          \times (\text{kernel_size}[2] - 1) + \text{output_padding}[2] + 1\]</div></div>
</dd>
</dl>
<dl class="py attribute">
<dt id="oneflow.nn.ConvTranspose3d.weight">
<code class="sig-name descname"><span class="pre">weight</span></code><a class="headerlink" href="#oneflow.nn.ConvTranspose3d.weight" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 <span class="math notranslate nohighlight">\((\text{in_channels}, \frac{\text{out_channels}}{\text{groups}},\)</span>
<span class="math notranslate nohighlight">\(\text{kernel_size[0]}, \text{kernel_size[1]}, \text{kernel_size[2]})\)</span> 的模块可学习权重。
这些权重的值是由公式 <cite>mathcal{U}(-sqrt{k}, sqrt{k})</cite> 计算而来，其中
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{out} * \prod_{i=0}^{2}\text{kernel_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.ConvTranspose3d.bias">
<code class="sig-name descname"><span class="pre">bias</span></code><a class="headerlink" href="#oneflow.nn.ConvTranspose3d.bias" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 (out_channels) 的模块可学习权重。如果 <a class="reference internal" href="#oneflow.nn.ConvTranspose3d.bias" title="oneflow.nn.ConvTranspose3d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> 为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，
则那么这些权重的值是由公式 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 计算而来，其中
<span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{out} * \text{kernel\_size}}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.ConvTranspose3d.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.ConvTranspose3d.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.CropMirrorNormalize">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">CropMirrorNormalize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">color_space</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'BGR'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_layout</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'NCHW'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crop_h</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crop_w</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crop_pos_y</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crop_pos_x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">[0.0]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">[1.0]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dtype</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">oneflow._oneflow_internal.dtype</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">oneflow.float32</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.CropMirrorNormalize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.CrossEntropyLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">CrossEntropyLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.CrossEntropyLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>将类 <code class="xref py py-class docutils literal notranslate"><span class="pre">LogSoftmax</span></code> 和 <code class="xref py py-class docutils literal notranslate"><span class="pre">NLLLoss</span></code> 组合在一起。</p>
<p>该类在使用 <cite>C</cite> 类训练分类问题时很有用。</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> 应包含每个类的原始的，非标准化分数。</p>
<p>在 <cite>K</cite> 维下， <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> 的大小必须为 <span class="math notranslate nohighlight">\((minibatch, C)\)</span> 或 <span class="math notranslate nohighlight">\((minibatch, C, d_1, d_2, ..., d_K)\)</span> ，
其中 <span class="math notranslate nohighlight">\(K \geq 1\)</span> （见下文）。</p>
<p>在此标准中，类的索引应在 <span class="math notranslate nohighlight">\([0, C-1]\)</span> 范围内并引作为大小为 <cite>minibatch</cite> 的 1D tensor 的 <cite>target</cite> ；</p>
<p>该损失可以被描述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right)
               = -x[class] + \log\left(\sum_j \exp(x[j])\right)\]</div></div>
<p>通过提供大小为 <span class="math notranslate nohighlight">\((minibatch, C, d_1, d_2, ..., d_K)\)</span> 的输入和适当形状的目标（其中 <span class="math notranslate nohighlight">\(K \geq 1\)</span> ， <span class="math notranslate nohighlight">\(K\)</span> 是维度数），
此类也可用于更高维度的输入，例如 2D 图像（见下文）。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><dl class="simple">
<dt><strong>reduction</strong> (string, 可选的): 指定应用于输出的简化（可以是 <code class="docutils literal notranslate"><span class="pre">'none'</span></code> 、 <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> 、 <code class="docutils literal notranslate"><span class="pre">'sum'</span></code> ，默认为 <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> ）：</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">'none'</span></code> :不进行简化；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'mean'</span></code> :取输出的加权平均值；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'sum'</span></code> :取输出的和。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>   <span class="p">[[</span><span class="o">-</span><span class="mf">0.1664078</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7256707</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.14690138</span><span class="p">],</span>
<span class="gp">... </span>       <span class="p">[</span><span class="o">-</span><span class="mf">0.21474946</span><span class="p">,</span> <span class="mf">0.53737473</span><span class="p">,</span> <span class="mf">0.99684894</span><span class="p">],</span>
<span class="gp">... </span>       <span class="p">[</span><span class="o">-</span><span class="mf">1.135804</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.50371903</span><span class="p">,</span> <span class="mf">0.7645404</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.8020, 1.1167, 0.3583], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_sum</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_sum</span>
<span class="go">tensor(2.2769, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_mean</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_mean</span>
<span class="go">tensor(0.7590, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Dropout">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Dropout</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>During training, randomly zeroes some of the elements of the input
tensor with probability <a class="reference internal" href="#oneflow.nn.Dropout.p" title="oneflow.nn.Dropout.p"><code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code></a> using samples from a Bernoulli
distribution. Each channel will be zeroed out independently on every forward
call.</p>
<p>This has proven to be an effective technique for regularization and
preventing the co-adaptation of neurons as described in the paper
“Improving neural networks by preventing co-adaptation of feature
detectors”.</p>
<p>Furthermore, the outputs are scaled by a factor of <span class="math notranslate nohighlight">\(\frac{1}{1-p}\)</span> during
training. This means that during evaluation the module simply computes an
identity function.</p>
<p>Additionally, we can pass an extra Tensor <cite>addend</cite> which shape is consistent with input Tensor.
The <cite>addend</cite> Tensor will be add in result after dropout, it is very useful in model’s residual connection structure.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> – probability of an element to be zeroed. Default: 0.5</p></li>
<li><p><strong>inplace</strong> – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>generator</strong> – A pseudorandom number generator for sampling</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((*)\)</span>. Input can be of any shape</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((*)\)</span>. Output is of the same shape as input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<p>example 1:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
<span class="gp">... </span>   <span class="p">[</span>
<span class="gp">... </span>       <span class="p">[</span><span class="o">-</span><span class="mf">0.7797</span><span class="p">,</span> <span class="mf">0.2264</span><span class="p">,</span> <span class="mf">0.2458</span><span class="p">,</span> <span class="mf">0.4163</span><span class="p">],</span>
<span class="gp">... </span>       <span class="p">[</span><span class="mf">0.4299</span><span class="p">,</span> <span class="mf">0.3626</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4892</span><span class="p">,</span> <span class="mf">0.4141</span><span class="p">],</span>
<span class="gp">... </span>       <span class="p">[</span><span class="o">-</span><span class="mf">1.4115</span><span class="p">,</span> <span class="mf">1.2183</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5503</span><span class="p">,</span> <span class="mf">0.6520</span><span class="p">],</span>
<span class="gp">... </span>   <span class="p">]</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> 
<span class="go">tensor([[-0.7797,  0.2264,  0.2458,  0.4163],</span>
<span class="go">        [ 0.4299,  0.3626, -0.4892,  0.4141],</span>
<span class="go">        [-1.4115,  1.2183, -0.5503,  0.6520]], dtype=oneflow.float32)</span>
</pre></div>
</div>
<p>example 2:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
<span class="gp">... </span>   <span class="p">[</span>
<span class="gp">... </span>       <span class="p">[</span><span class="o">-</span><span class="mf">0.7797</span><span class="p">,</span> <span class="mf">0.2264</span><span class="p">,</span> <span class="mf">0.2458</span><span class="p">,</span> <span class="mf">0.4163</span><span class="p">],</span>
<span class="gp">... </span>       <span class="p">[</span><span class="mf">0.4299</span><span class="p">,</span> <span class="mf">0.3626</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4892</span><span class="p">,</span> <span class="mf">0.4141</span><span class="p">],</span>
<span class="gp">... </span>       <span class="p">[</span><span class="o">-</span><span class="mf">1.4115</span><span class="p">,</span> <span class="mf">1.2183</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5503</span><span class="p">,</span> <span class="mf">0.6520</span><span class="p">],</span>
<span class="gp">... </span>   <span class="p">]</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">addend</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">addend</span><span class="o">=</span><span class="n">addend</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> 
<span class="go">tensor([[ 0.2203,  1.2264,  1.2458,  1.4163],</span>
<span class="go">        [ 1.4299,  1.3626,  0.5108,  1.4141],</span>
<span class="go">        [-0.4115,  2.2183,  0.4497,  1.6520]], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py attribute">
<dt id="oneflow.nn.Dropout.inplace">
<code class="sig-name descname"><span class="pre">inplace</span></code><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#oneflow.nn.Dropout.inplace" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.Dropout.p">
<code class="sig-name descname"><span class="pre">p</span></code><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><a class="headerlink" href="#oneflow.nn.Dropout.p" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ELU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ELU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\text{ELU}(x) = \begin{cases}
                        x &amp; \text{ if } x \gt 0  \\
        \alpha*(exp(x)-1) &amp; \text{ if } x \le 0 \\
            \end{cases}\end{split}\]</div></div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> – the <span class="math notranslate nohighlight">\(\alpha\)</span> value for the ELU formulation. Default: 1.0</p></li>
<li><p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">elu</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">elu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([-0.3935,  0.0000,  0.5000], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.ELU.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ELU.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Embedding">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Embedding</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_embeddings</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_idx</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_norm</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_grad_by_freq</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sparse</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_weight</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Embedding" title="Permalink to this definition">¶</a></dt>
<dd><p>A simple lookup table that stores embeddings of a fixed dictionary and size.</p>
<p>This module is often used to store word embeddings and retrieve them using indices.
The input to the module is a list of indices, and the output is the corresponding
word embeddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_embeddings</strong> (<em>int</em>) – size of the dictionary of embeddings</p></li>
<li><p><strong>embedding_dim</strong> (<em>int</em>) – the size of each embedding vector</p></li>
<li><p><strong>padding_idx</strong> (<em>int</em><em>, </em><em>optional</em>) – If specified, the entries at <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_idx</span></code> do not contribute to the gradient;
therefore, the embedding vector at <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_idx</span></code> is not updated during training,
i.e. it remains as a fixed “pad”. For a newly constructed Embedding,
the embedding vector at <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_idx</span></code> will default to all zeros,
but can be updated to another value to be used as the padding vector.</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Embedding.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.Embedding.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.FakeQuantization">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">FakeQuantization</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">quantization_formula</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'google'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_bit</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_scheme</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'symmetric'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.FakeQuantization" title="Permalink to this definition">¶</a></dt>
<dd><p>Simulate the quantize and dequantize operations in training time.</p>
<p>The output will be computed as:</p>
<blockquote>
<div><p>if quantization_scheme == “symmetric”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; quant\_max = 2^{quantization\_to\_bit - 1} - 1\\&amp; quant\_min = -quant\_max\\&amp; clamp(round(x / scale), quant\_min, quant\_max) * scale\end{aligned}\end{align} \]</div></div>
<p>elif quantization_scheme == “affine”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; quant\_max = 2^{quantization\_to\_bit} - 1\\&amp; quant\_min = 0\\&amp; (clamp(round(x / scale + zero\_point), quant\_min, quant\_max) - zero\_point) * scale\end{aligned}\end{align} \]</div></div>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>quantization_bit</strong> (<em>int</em>) – Quantize input to uintX / intX, X can be in range [2, 8]. Defaults to 8.</p></li>
<li><p><strong>quantization_scheme</strong> (<em>str</em>) – “symmetric” or “affine”, quantize to signed / unsigned integer. Defaults to “symmetric”.</p></li>
<li><p><strong>quantization_formula</strong> (<em>str</em>) – Support “google” or “cambricon”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Input tensor after quantize and dequantize operations.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">oneflow.Tensor</a></p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">weight</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_bit</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_scheme</span> <span class="o">=</span> <span class="s2">"symmetric"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_formula</span> <span class="o">=</span> <span class="s2">"google"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">per_layer_quantization</span> <span class="o">=</span> <span class="bp">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">min_max_observer</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MinMaxObserver</span><span class="p">(</span><span class="n">quantization_formula</span><span class="o">=</span><span class="n">quantization_formula</span><span class="p">,</span> <span class="n">quantization_bit</span><span class="o">=</span><span class="n">quantization_bit</span><span class="p">,</span>
<span class="gp">... </span><span class="n">quantization_scheme</span><span class="o">=</span><span class="n">quantization_scheme</span><span class="p">,</span> <span class="n">per_layer_quantization</span><span class="o">=</span><span class="n">per_layer_quantization</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fake_quantization</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">FakeQuantization</span><span class="p">(</span><span class="n">quantization_formula</span><span class="o">=</span><span class="n">quantization_formula</span><span class="p">,</span> <span class="n">quantization_bit</span><span class="o">=</span><span class="n">quantization_bit</span><span class="p">,</span>
<span class="gp">... </span><span class="n">quantization_scheme</span><span class="o">=</span><span class="n">quantization_scheme</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> <span class="n">min_max_observer</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">input_tensor</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">output_tensor</span> <span class="o">=</span> <span class="n">fake_quantization</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">input_tensor</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">scale</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">zero_point</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Flatten">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Flatten</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">start_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">end_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Flatten" title="Permalink to this definition">¶</a></dt>
<dd><p>Flattens a contiguous range of dims into a tensor. For use with: nn.Sequential.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start_dim</strong> – first dim to flatten (default = 1).</p></li>
<li><p><strong>end_dim</strong> – last dim to flatten (default = -1).</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="go">oneflow.Size([32, 25])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Flatten.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.Flatten.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.FusedBatchNorm1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">FusedBatchNorm1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.FusedBatchNorm1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Fused Batch Normalization over a 2D or 3D input, the formula is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = ReLU(BatchNorm(input) + addend)\]</div></div>
<p>The formula of Batch Normalization is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size). By default, the elements of <span class="math notranslate nohighlight">\(\gamma\)</span> are set
to 1 and the elements of <span class="math notranslate nohighlight">\(\beta\)</span> are set to 0. The standard-deviation is calculated
via the biased estimator, equivalent to <cite>torch.var(input, unbiased=False)</cite>.</p>
<p>Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code>
of 0.1.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, this layer then does not
keep running estimates, and batch statistics are instead used during
evaluation time as well.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span>,
where <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the estimated statistic and <span class="math notranslate nohighlight">\(x_t\)</span> is the
new observed value.</p>
</div>
<p>Because the Batch Normalization is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, L)</cite> slices, it’s common terminology to call this Temporal Batch Normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> – <span class="math notranslate nohighlight">\(C\)</span> from an expected input of size
<span class="math notranslate nohighlight">\((N, C, L)\)</span> or <span class="math notranslate nohighlight">\(L\)</span> from input of size <span class="math notranslate nohighlight">\((N, L)\)</span></p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability.
Default: 1e-5</p></li>
<li><p><strong>momentum</strong> – the value used for the running_mean and running_var
computation. Can be set to <code class="docutils literal notranslate"><span class="pre">None</span></code> for cumulative moving average
(i.e. simple average). Default: 0.1</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics, and initializes statistics
buffers <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> as <code class="docutils literal notranslate"><span class="pre">None</span></code>.
When these buffers are <code class="docutils literal notranslate"><span class="pre">None</span></code>, this module always uses batch statistics.
in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C)\)</span> or <span class="math notranslate nohighlight">\((N, C, L)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C)\)</span> or <span class="math notranslate nohighlight">\((N, C, L)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span> <span class="c1"># FusedBatchNorm support in GPU currently.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">FusedBatchNorm1d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">addend</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.FusedBatchNorm2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">FusedBatchNorm2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.FusedBatchNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Fused Batch Normalization over a 4D input, the formula is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = ReLU(BatchNorm(input) + addend)\]</div></div>
<p>The formula of Batch Normalization is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size). By default, the elements of <span class="math notranslate nohighlight">\(\gamma\)</span> are set
to 1 and the elements of <span class="math notranslate nohighlight">\(\beta\)</span> are set to 0. The standard-deviation is calculated
via the biased estimator, equivalent to <cite>torch.var(input, unbiased=False)</cite>.</p>
<p>Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code>
of 0.1.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, this layer then does not
keep running estimates, and batch statistics are instead used during
evaluation time as well.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span>,
where <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the estimated statistic and <span class="math notranslate nohighlight">\(x_t\)</span> is the
new observed value.</p>
</div>
<p>Because the Batch Normalization is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, H, W)</cite> slices, it’s common terminology to call this Spatial Batch Normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> – <span class="math notranslate nohighlight">\(C\)</span> from an expected input of size
<span class="math notranslate nohighlight">\((N, C, H, W)\)</span></p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability.
Default: 1e-5</p></li>
<li><p><strong>momentum</strong> – the value used for the running_mean and running_var
computation. Can be set to <code class="docutils literal notranslate"><span class="pre">None</span></code> for cumulative moving average
(i.e. simple average). Default: 0.1</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics, and initializes statistics
buffers <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> as <code class="docutils literal notranslate"><span class="pre">None</span></code>.
When these buffers are <code class="docutils literal notranslate"><span class="pre">None</span></code>, this module always uses batch statistics.
in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span> <span class="c1"># FusedBatchNorm support in GPU currently.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">FusedBatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">addend</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.FusedBatchNorm3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">FusedBatchNorm3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.FusedBatchNorm3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Fused Batch Normalization over a 5D input, the formula is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = ReLU(BatchNorm(input) + addend)\]</div></div>
<p>The formula of Batch Normalization is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}y = \\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\end{split}\]</div></div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size). By default, the elements of <span class="math notranslate nohighlight">\(\gamma\)</span> are set
to 1 and the elements of <span class="math notranslate nohighlight">\(\beta\)</span> are set to 0. The standard-deviation is calculated
via the biased estimator, equivalent to <cite>torch.var(input, unbiased=False)</cite>.</p>
<p>Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code>
of 0.1.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, this layer then does not
keep running estimates, and batch statistics are instead used during
evaluation time as well.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times     x_t\)</span>,
where <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the estimated statistic and <span class="math notranslate nohighlight">\(x_t\)</span> is the
new observed value.</p>
</div>
<p>Because the Batch Normalization is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, D, H, W)</cite> slices, it’s common terminology to call this Spatial Batch Normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> – <span class="math notranslate nohighlight">\(C\)</span> from an expected input of size
<span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span></p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability.
Default: 1e-5</p></li>
<li><p><strong>momentum</strong> – the value used for the running_mean and running_var
computation. Can be set to <code class="docutils literal notranslate"><span class="pre">None</span></code> for cumulative moving average
(i.e. simple average). Default: 0.1</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics, and initializes statistics
buffers <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> as <code class="docutils literal notranslate"><span class="pre">None</span></code>.
When these buffers are <code class="docutils literal notranslate"><span class="pre">None</span></code>, this module always uses batch statistics.
in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span> <span class="c1"># FusedBatchNorm support in GPU currently.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">FusedBatchNorm3d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">addend</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.FusedMLP">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">FusedMLP</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_features</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_features</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_features</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_final_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.FusedMLP" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a linear transformation with relu activation to the incoming data: <span class="math notranslate nohighlight">\(y = ReLU(xA^T + b)\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_features</strong> – size of each input sample</p></li>
<li><p><strong>hidden_features</strong> – A tuple of each Linear layer hidden size</p></li>
<li><p><strong>out_features</strong> – The final Linear layer hidden size</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *, H_{in})\)</span> where <span class="math notranslate nohighlight">\(*\)</span> means any number of
additional dimensions and <span class="math notranslate nohighlight">\(H_{in} = {in\_features}\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *, H_{out})\)</span> where all but the last dimension
are the same shape as the input and <span class="math notranslate nohighlight">\(H_{out} = {out\_features}\)</span>.</p></li>
</ul>
</dd>
<dt>Attr:</dt><dd><ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">skip_final_activation</span></code>: Whether to skip final hidden layer’s activation. Default: False.</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">FusedMLP</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">],</span> <span class="mi">1024</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 1024])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.FusedMLP.add_parameters">
<code class="sig-name descname"><span class="pre">add_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.FusedMLP.add_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="oneflow.nn.FusedMLP.bias">
<code class="sig-name descname"><span class="pre">bias</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">i</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.FusedMLP.bias" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="oneflow.nn.FusedMLP.biases">
<code class="sig-name descname"><span class="pre">biases</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.FusedMLP.biases" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="oneflow.nn.FusedMLP.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.FusedMLP.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
<dl class="py method">
<dt id="oneflow.nn.FusedMLP.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.FusedMLP.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="oneflow.nn.FusedMLP.weight">
<code class="sig-name descname"><span class="pre">weight</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">i</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.FusedMLP.weight" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="oneflow.nn.FusedMLP.weights">
<code class="sig-name descname"><span class="pre">weights</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.FusedMLP.weights" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.GELU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">GELU</span></code><a class="headerlink" href="#oneflow.nn.GELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Gelu activation operator.</p>
<p>The equation is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = 0.5 * x * (1 + tanh(\sqrt{\frac{2}{\pi}} * (x + 0.044715x^{3})))\]</div></div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor"><em>oneflow.Tensor</em></a>) – Input Tensor</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">oneflow.Tensor</a></p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gelu</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">gelu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([-0.1543,  0.0000,  0.3457], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.GLU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">GLU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.GLU" title="Permalink to this definition">¶</a></dt>
<dd><p>The GLU activation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor"><em>Tensor</em></a><em>, </em><em>float</em>) – input tensor.</p></li>
<li><p><strong>dim</strong> (<em>int</em><em>, </em><em>optional</em>) – dimension on which to split the input. Default: -1</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((\ast_1, N, \ast_2)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((\ast_1, M, \ast_2)\)</span> where <span class="math notranslate nohighlight">\(M=N/2\)</span></p></li>
</ul>
</dd>
</dl>
<p>The formula is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[GLU(input) = GLU(a, b) = a \otimes sigmoid(b)\]</div></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>where input is split in half along dim to form a and b, ⊗ is the element-wise product between matrices.</p>
</div>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span>
<span class="go">tensor([[0.9526, 1.9640],</span>
<span class="go">        [4.9954, 5.9980]], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.GroupNorm">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">GroupNorm</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_groups</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_channels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.GroupNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from:
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html">https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html</a></p>
<p>Applies Group Normalization over a mini-batch of inputs as described in
the paper <a class="reference external" href="https://arxiv.org/abs/1803.08494">Group Normalization</a></p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>The input channels are separated into <code class="xref py py-attr docutils literal notranslate"><span class="pre">num_groups</span></code> groups, each containing
<code class="docutils literal notranslate"><span class="pre">num_channels</span> <span class="pre">/</span> <span class="pre">num_groups</span></code> channels. The mean and standard-deviation are calculated
separately over the each group. <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable
per-channel affine transform parameter vectors of size <code class="xref py py-attr docutils literal notranslate"><span class="pre">num_channels</span></code> if
<code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.
The standard-deviation is calculated via the biased estimator, equivalent to
<cite>torch.var(input, unbiased=False)</cite>.</p>
<p>This layer uses statistics computed from input data in both training and
evaluation modes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_groups</strong> (<em>int</em>) – number of groups to separate the channels into</p></li>
<li><p><strong>num_channels</strong> (<em>int</em>) – number of channels expected in input</p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module
has learnable per-channel affine parameters initialized to ones (for weights)
and zeros (for biases). Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, *)\)</span> where <span class="math notranslate nohighlight">\(C=\text{num_channels}\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, *)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Separate 6 channels into 3 groups</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Separate 6 channels into 6 groups (equivalent with InstanceNorm)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Put all 6 channels into a single group (equivalent with LayerNorm)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Activating the module</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.GroupNorm.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.GroupNorm.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
<dl class="py method">
<dt id="oneflow.nn.GroupNorm.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.GroupNorm.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Hardsigmoid">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Hardsigmoid</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Hardsigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\text{Hardsigmoid}(x) = \begin{cases}
    0 &amp; \text{ if } x \le -3  \\
    1 &amp; \text{ if } x \ge +3 \\
    \frac{x}{6} + \frac{1}{2} &amp; \text{ otherwise } \\
\end{cases}\end{split}\]</div></div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hardsigmoid</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Hardsigmoid</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">hardsigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.4167, 0.5000, 0.5833], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Hardsigmoid.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Hardsigmoid.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Hardswish">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Hardswish</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Hardswish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the hardswish function, element-wise, as described in the paper:
<a class="reference external" href="https://arxiv.org/abs/1905.02244">Searching for MobileNetV3</a>.</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\text{Hardswish}(x) = \begin{cases}
    0 &amp; \text{ if } x \le -3  \\
    x &amp; \text{ if } x \ge +3 \\
    x*(x+3)/6 &amp; \text{ otherwise } \\
\end{cases}\end{split}\]</div></div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hardswish</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Hardswish</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">hardswish</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([-0.2083,  0.0000,  0.2917], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Hardswish.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Hardswish.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Hardtanh">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Hardtanh</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">min_val</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_val</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_value</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_value</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Hardtanh" title="Permalink to this definition">¶</a></dt>
<dd><p>按照以下公式（HardTanh），进行 element-wise 操作：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\text{HardTanh}(x) = \begin{cases}
    1 &amp; \text{ if } x &gt; 1 \\
    -1 &amp; \text{ if } x &lt; -1 \\
    x &amp; \text{ otherwise } \\
\end{cases}\end{split}\]</div></div>
<p>默认的线性范围为 <span class="math notranslate nohighlight">\([-1, 1]\)</span>，可以通过设置参数
<code class="xref py py-attr docutils literal notranslate"><span class="pre">min_val</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_val</span></code> 改变。</p>
<dl class="simple">
<dt>参数:</dt><dd><ul class="simple">
<li><p>min_val: 线性范围的下界。 默认值: -1</p></li>
<li><p>max_val: 线性范围的上界。 默认值: 1</p></li>
<li><p>inplace: 是否做 in-place 操作。默认为 <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<p>因为有了参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">min_val</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_val</span></code>，原有的
参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">min_value</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_value</span></code> 已经被不再推荐使用。</p>
<dl class="simple">
<dt>形状:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> 其中 <cite>*</cite> 的意思是，可以指定任意维度</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span> 输入形状与输出形状一致</p></li>
</ul>
</dd>
</dl>
<p>示例:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Hardtanh</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.2000, 0.3000, 1.0000, 1.0000], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Hardtanh.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Hardtanh.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Identity">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Identity</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Identity" title="Permalink to this definition">¶</a></dt>
<dd><p>A placeholder identity operator that is argument-insensitive.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> – any argument (unused)</p></li>
<li><p><strong>kwargs</strong> – any keyword argument (unused)</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="c1"># output = input</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.InstanceNorm1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">InstanceNorm1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.InstanceNorm1d" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from: <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html">https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html</a></p>
<p>Applies Instance Normalization over a 3D input (a mini-batch of 1D
inputs with optional additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1607.08022">Instance Normalization: The Missing Ingredient for Fast Stylization</a>.</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>The mean and standard-deviation are calculated per-dimension separately
for each object in a mini-batch. <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size) if <code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.
The standard-deviation is calculated via the biased estimator, equivalent to
<cite>torch.var(input, unbiased=False)</cite>.</p>
<p>By default, this layer uses instance statistics computed from input data in
both training and evaluation modes.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, during training this
layer keeps running estimates of its computed mean and variance, which are
then used for normalization during evaluation. The running estimates are
kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> of 0.1.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span>,
where <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the estimated statistic and <span class="math notranslate nohighlight">\(x_t\)</span> is the
new observed value.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#oneflow.nn.InstanceNorm1d" title="oneflow.nn.InstanceNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a> and <a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> are very similar, but
have some subtle differences. <a class="reference internal" href="#oneflow.nn.InstanceNorm1d" title="oneflow.nn.InstanceNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a> is applied
on each channel of channeled data like multidimensional time series, but
<a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> is usually applied on entire sample and often in NLP
tasks. Additionally, <a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> applies elementwise affine
transform, while <a class="reference internal" href="#oneflow.nn.InstanceNorm1d" title="oneflow.nn.InstanceNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a> usually don’t apply affine
transform.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> – <span class="math notranslate nohighlight">\(C\)</span> from an expected input of size
<span class="math notranslate nohighlight">\((N, C, L)\)</span> or <span class="math notranslate nohighlight">\(L\)</span> from input of size <span class="math notranslate nohighlight">\((N, L)\)</span></p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</p></li>
<li><p><strong>momentum</strong> – the value used for the running_mean and running_var computation. Default: 0.1</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters, initialized the same way as done for batch normalization.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, L)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, L)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.InstanceNorm2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">InstanceNorm2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.InstanceNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from: <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html">https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html</a></p>
<p>Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs
with additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1607.08022">Instance Normalization: The Missing Ingredient for Fast Stylization</a>.</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>The mean and standard-deviation are calculated per-dimension separately
for each object in a mini-batch. <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size) if <code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.
The standard-deviation is calculated via the biased estimator, equivalent to
<cite>torch.var(input, unbiased=False)</cite>.</p>
<p>By default, this layer uses instance statistics computed from input data in
both training and evaluation modes.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, during training this
layer keeps running estimates of its computed mean and variance, which are
then used for normalization during evaluation. The running estimates are
kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> of 0.1.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span>,
where <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the estimated statistic and <span class="math notranslate nohighlight">\(x_t\)</span> is the
new observed value.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#oneflow.nn.InstanceNorm2d" title="oneflow.nn.InstanceNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a> and <a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> are very similar, but
have some subtle differences. <a class="reference internal" href="#oneflow.nn.InstanceNorm2d" title="oneflow.nn.InstanceNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a> is applied
on each channel of channeled data like RGB images, but
<a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> is usually applied on entire sample and often in NLP
tasks. Additionally, <a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> applies elementwise affine
transform, while <a class="reference internal" href="#oneflow.nn.InstanceNorm2d" title="oneflow.nn.InstanceNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a> usually don’t apply affine
transform.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> – <span class="math notranslate nohighlight">\(C\)</span> from an expected input of size
<span class="math notranslate nohighlight">\((N, C, H, W)\)</span></p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</p></li>
<li><p><strong>momentum</strong> – the value used for the running_mean and running_var computation. Default: 0.1</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters, initialized the same way as done for batch normalization.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.InstanceNorm3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">InstanceNorm3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.InstanceNorm3d" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from: <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm3d.html">https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm3d.html</a></p>
<p>Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs
with additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1607.08022">Instance Normalization: The Missing Ingredient for Fast Stylization</a>.</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>The mean and standard-deviation are calculated per-dimension separately
for each object in a mini-batch. <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameter vectors
of size C (where C is the input size) if <code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.
The standard-deviation is calculated via the biased estimator, equivalent to
<cite>torch.var(input, unbiased=False)</cite>.</p>
<p>By default, this layer uses instance statistics computed from input data in
both training and evaluation modes.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, during training this
layer keeps running estimates of its computed mean and variance, which are
then used for normalization during evaluation. The running estimates are
kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> of 0.1.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span>,
where <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the estimated statistic and <span class="math notranslate nohighlight">\(x_t\)</span> is the
new observed value.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#oneflow.nn.InstanceNorm3d" title="oneflow.nn.InstanceNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a> and <a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> are very similar, but
have some subtle differences. <a class="reference internal" href="#oneflow.nn.InstanceNorm3d" title="oneflow.nn.InstanceNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a> is applied
on each channel of channeled data like 3D models with RGB color, but
<a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> is usually applied on entire sample and often in NLP
tasks. Additionally, <a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> applies elementwise affine
transform, while <a class="reference internal" href="#oneflow.nn.InstanceNorm3d" title="oneflow.nn.InstanceNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a> usually don’t apply affine
transform.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> – <span class="math notranslate nohighlight">\(C\)</span> from an expected input of size
<span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span></p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</p></li>
<li><p><strong>momentum</strong> – the value used for the running_mean and running_var computation. Default: 0.1</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters, initialized the same way as done for batch normalization.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.KLDivLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">KLDivLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_target</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.KLDivLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>此接口与 PyTorch 一致。
文档参考自： <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html?highlight=kldivloss#torch.nn.KLDivLoss">https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html?highlight=kldivloss#torch.nn.KLDivLoss</a></p>
<p>测量 KL 散度。</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler divergence</a> 可用于连续分布中的距离测量，并且在对（离散采样）
连续输出分布的空间执行直接回归时通常很有用。</p>
<p>与 <code class="xref py py-class docutils literal notranslate"><span class="pre">NLLLoss</span></code> 一样， <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> 应包含 <em>log-probabilities</em> 并且不限于 2D tensor。</p>
<p>默认情况下，目标被解释为 <em>probabilities</em> ，
但可以将其视为将 <code class="xref py py-attr docutils literal notranslate"><span class="pre">log_target</span></code> 设置为 <code class="docutils literal notranslate"><span class="pre">True</span></code> 的 <em>log-probabilities</em> 。</p>
<p>此 criterion 要求 <cite>target</cite> 、 <cite>Tensor</cite> 的形状与 <cite>input</cite> 、 <cite>Tensor</cite> 一致。</p>
<p>未简化 （即 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> 设置为 <code class="docutils literal notranslate"><span class="pre">'none'</span></code> ） 的损失可以描述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[l(x,y) = L = \{ l_1,\dots,l_N \}, \quad
l_n = y_n \cdot \left( \log y_n - x_n \right)\]</div></div>
<p>其中索引 <span class="math notranslate nohighlight">\(N\)</span> span <code class="docutils literal notranslate"><span class="pre">input</span></code> 的所有维度，并且 <span class="math notranslate nohighlight">\(L\)</span> 具有与 <code class="docutils literal notranslate"><span class="pre">input</span></code> 相同的形状。
如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> 不为 <code class="docutils literal notranslate"><span class="pre">'none'</span></code> （默认 <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> ），则：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), &amp; \text{if reduction} = \text{`mean';} \\
    \operatorname{sum}(L),  &amp; \text{if reduction} = \text{`sum'.}
\end{cases}\end{split}\]</div></div>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> 为默认值 <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> ，每个 minibatch 的损失在 observations 和维度上取平均值。
如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> 为 <code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> ，可以得到正确的 KL 散度，其中损失仅在批次维度上进行平均。
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code> 模式的行为将在下一个主要版本中更改为与 <code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> 相同。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><dl class="simple">
<dt><strong>reduction</strong> (string, 可选的):  指定应用于输出的简化（可以为 <code class="docutils literal notranslate"><span class="pre">'none'</span></code> 、 <code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> 、 <code class="docutils literal notranslate"><span class="pre">'sum'</span></code> 、 <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> ，默认： <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> ）：</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">'none'</span></code> ：不会进行简化</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> ：输出的总和将除以 batchsize 。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'sum'</span></code> ：将输出求和。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'mean'</span></code> ：输出和将除以输出中的元素数。</p></li>
</ul>
</dd>
</dl>
</li>
<li><p><strong>log_target</strong> (bool, 可选的): 指定是否在 log space 中传递 <cite>target</cite>。默认： <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> 时不会返回真正的 KL 散度值，请使用符合 KL 数学定义的 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = <code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> 。
在下一个主要版本中，<code class="docutils literal notranslate"><span class="pre">'mean'</span></code> 将更改为与 <code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> 相同。</p>
</div>
<dl class="simple">
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, *)\)</span> 其中 <span class="math notranslate nohighlight">\(*\)</span> 表示任意数量的额外维度</p></li>
<li><p><strong>Target</strong> : <span class="math notranslate nohighlight">\((N, *)\)</span>，与输入的形状相同</p></li>
<li><p><strong>Output</strong> : 默认为标量。如果 :attr:<code class="docutils literal notranslate"><span class="pre">reduction</span></code> 为 <code class="docutils literal notranslate"><span class="pre">'none'</span></code> ，则为 <span class="math notranslate nohighlight">\((N, *)\)</span>，形状与输入相同</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.9021705</span><span class="p">,</span> <span class="mf">0.08798598</span><span class="p">,</span> <span class="mf">1.04686249</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.22386942</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.89729659</span><span class="p">,</span> <span class="mf">0.01615712</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">,</span> <span class="n">log_target</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([ 1.3514,  0.0000, -0.0836], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">,</span> <span class="n">log_target</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(0.4226, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">,</span> <span class="n">log_target</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(5.7801, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.L1Loss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">L1Loss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.L1Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>此运算符计算 <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 中每个元素之间的 L1 Loss 。</p>
<p>公式为：</p>
<p>如果 reduction = “none”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[output = |Target - Input|\]</div></div>
<p>如果 reduction = “mean”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[output = \frac{1}{n}\sum_{i=1}^n|Target_i - Input_i|\]</div></div>
<p>如果 reduction = “sum”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[output = \sum_{i=1}^n|Target_i - Input_i|\]</div></div>
<dl class="simple">
<dt>参数:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor): 输入张量。</p></li>
<li><p><strong>target</strong> (Tensor): 目标张量。</p></li>
<li><p><strong>reduction</strong> (str): 简化类型，可以为 <code class="docutils literal notranslate"><span class="pre">"none"</span></code> 、 <code class="docutils literal notranslate"><span class="pre">"mean"</span></code> 、 <code class="docutils literal notranslate"><span class="pre">"sum"</span></code> 。默认为 “mean” 。</p></li>
</ul>
</dd>
<dt>返回类型：</dt><dd><p>oneflow.tensor</p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">]],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[3., 3., 3.],</span>
<span class="go">        [2., 2., 2.],</span>
<span class="go">        [3., 3., 3.]], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m_mean</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m_mean</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(2.6667, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m_mean</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m_mean</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(24., dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.LayerNorm">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">LayerNorm</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">normalized_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">elementwise_affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.LayerNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>对小批量输入应用层归一化 (Layer Normalization) ，行为如论文 <a class="reference external" href="https://arxiv.org/abs/1607.06450">Layer Normalization</a> 所述。</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>在特定数字维度上分别计算均值和标准差，这些维度的形状必须由:attr:<cite>normalized_shape</cite> 指定。
如果 <a class="reference internal" href="#oneflow.nn.LayerNorm.elementwise_affine" title="oneflow.nn.LayerNorm.elementwise_affine"><code class="xref py py-attr docutils literal notranslate"><span class="pre">elementwise_affine</span></code></a> 为 <cite>True</cite> ，则 <span class="math notranslate nohighlight">\(\gamma\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span> 是
参数 <a class="reference internal" href="#oneflow.nn.LayerNorm.normalized_shape" title="oneflow.nn.LayerNorm.normalized_shape"><code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized_shape</span></code></a> 的可学习仿射变换参数。标准差是通过有偏估计器 (biased estimator) 计算的。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>与批量归一化 (Batch Normalization) 和实例归一化 (Instance Normalization) 使用 <code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code>
选项为每个完整通道/平面应用标量 scale 和偏差不同，层归一化 (Layer Normalization) 使用 <a class="reference internal" href="#oneflow.nn.LayerNorm.elementwise_affine" title="oneflow.nn.LayerNorm.elementwise_affine"><code class="xref py py-attr docutils literal notranslate"><span class="pre">elementwise_affine</span></code></a>
处理每个元素的 scale 和偏差。</p>
</div>
<p>该层在训练和评估模式下都使用从输入数据计算的统计信息。</p>
<dl>
<dt>参数：</dt><dd><ul>
<li><p><strong>normalized_shape</strong> (int 或 list 或 oneflow.Size): 来自预期大小输入的输入形状</p>
<blockquote>
<div><div class="math-wrapper"><div class="math notranslate nohighlight">
\[[* \times \text{normalized_shape}[0] \times \text{normalized_shape}[1] \times \ldots \times \text{normalized_shape}[-1]]\]</div></div>
<p>如果使用单个整数，则将其视为单例列表，并且此模块将对最后一个维度进行标准化，该维度预计具有该特定大小。</p>
</div></blockquote>
</li>
<li><p><strong>eps</strong> (float, 可选的): 为数值稳定性而添加到分母的值。默认：1e-5</p></li>
<li><dl class="simple">
<dt><strong>elementwise_affine</strong> (bool, 可选的): 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，该模块具有可学习的逐元素仿射参数，</dt><dd><p>并且将他们初始化为 1 （对于权重）和 0（对于偏差）。默认值： <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, *)\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, *)\)</span> （形状与输入相同）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[[</span><span class="o">-</span><span class="mf">0.16046895</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.03667831</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.34974465</span><span class="p">,</span> <span class="mf">0.26505867</span><span class="p">]],[[</span><span class="o">-</span><span class="mf">1.24111986</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.53806001</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.72426331</span><span class="p">,</span> <span class="mf">0.43572459</span><span class="p">]],],[[[</span><span class="o">-</span><span class="mf">0.77390957</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.42610624</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.16398858</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.35760343</span><span class="p">]],[[</span><span class="mf">1.07541728</span><span class="p">,</span> <span class="mf">0.11008703</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.26361224</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.48663723</span><span class="p">]]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> 
<span class="go">tensor([[[[ 1.0000, -1.0000],</span>
<span class="go">          [-0.9999,  0.9999]],</span>

<span class="go">         [[-1.0000,  1.0000],</span>
<span class="go">          [ 1.0000, -1.0000]]],</span>


<span class="go">        [[[-0.9998,  0.9998],</span>
<span class="go">          [ 1.0000, -1.0000]],</span>

<span class="go">         [[ 1.0000, -1.0000],</span>
<span class="go">          [ 1.0000, -1.0000]]]], dtype=oneflow.float32,</span>
<span class="go">       grad_fn=&lt;broadcast_add_backward&gt;)</span>
</pre></div>
</div>
<dl class="py attribute">
<dt id="oneflow.nn.LayerNorm.elementwise_affine">
<code class="sig-name descname"><span class="pre">elementwise_affine</span></code><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#oneflow.nn.LayerNorm.elementwise_affine" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.LayerNorm.eps">
<code class="sig-name descname"><span class="pre">eps</span></code><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><a class="headerlink" href="#oneflow.nn.LayerNorm.eps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="oneflow.nn.LayerNorm.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.LayerNorm.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.LayerNorm.normalized_shape">
<code class="sig-name descname"><span class="pre">normalized_shape</span></code><em class="property"><span class="pre">:</span> <span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="p"><span class="pre">…</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#oneflow.nn.LayerNorm.normalized_shape" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="oneflow.nn.LayerNorm.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.LayerNorm.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.LeakyReLU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">LeakyReLU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">negative_slope</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.LeakyReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\text{LeakyRELU}(x) = \begin{cases}
    x, &amp; \text{ if } x \geq 0 \\
    \text{negative_slope} \times x, &amp; \text{ otherwise }
\end{cases}\end{split}\]</div></div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>negative_slope</strong> – Controls the angle of the negative slope. Default: 1e-2</p></li>
<li><p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.2000, 0.3000, 3.0000, 4.0000], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.LeakyReLU.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.LeakyReLU.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Linear">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Linear</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_features</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_features</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Linear" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a linear transformation to the incoming data: <span class="math notranslate nohighlight">\(y = xA^T + b\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_features</strong> (<em>-</em>) – size of each input sample</p></li>
<li><p><strong>out_features</strong> (<em>-</em>) – size of each output sample</p></li>
<li><p><strong>bias</strong> (<em>-</em>) – If set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the layer will not learn an additive bias. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *, H_{in})\)</span> where <span class="math notranslate nohighlight">\(*\)</span> means any number of
additional dimensions and <span class="math notranslate nohighlight">\(H_{in} = {in\_features}\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *, H_{out})\)</span> where all but the last dimension
are the same shape as the input and <span class="math notranslate nohighlight">\(H_{out} = {out\_features}\)</span>.</p></li>
</ul>
</dd>
<dt>Attr:</dt><dd><ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code>: the learnable weights of the module of shape <span class="math notranslate nohighlight">\(({out\_features}, {in\_features})\)</span>. The values are initialized from <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>, where <span class="math notranslate nohighlight">\((k = 1 / {in\_features})\)</span></p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code>: the learnable bias of the module of shape <span class="math notranslate nohighlight">\(({out\_features})\)</span>. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the values are initialized from <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where <span class="math notranslate nohighlight">\((k = 1 / {in\_features})\)</span></p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([128, 30])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Linear.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.Linear.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
<dl class="py method">
<dt id="oneflow.nn.Linear.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.Linear.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.LogSigmoid">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">LogSigmoid</span></code><a class="headerlink" href="#oneflow.nn.LogSigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{LogSigmoid}(x) = \log\left(\frac{ 1 }{ 1 + \exp(-x)}\right)\]</div></div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logsigmoid</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LogSigmoid</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">logsigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([-0.9741, -0.6931, -0.4741], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.LogSoftmax">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">LogSoftmax</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.LogSoftmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the LogSoftmax function to an n-dimensional
input Tensor.
The LogSoftmax formulation can be simplified as:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right) = x_i - \log({ \sum_j \exp(x_j)})\]</div></div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dim</strong> (<em>int</em>) – A dimension along which LogSoftmax will be computed.</p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
<span class="gp">... </span>       <span class="p">[[</span> <span class="mf">0.4296</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1957</span><span class="p">,</span>  <span class="mf">2.5463</span><span class="p">],</span>
<span class="gp">... </span>       <span class="p">[</span> <span class="mf">1.2552</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5747</span><span class="p">,</span>  <span class="mf">0.6923</span><span class="p">]]</span>
<span class="gp">... </span>   <span class="p">)</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[-2.2513, -3.8766, -0.1346],</span>
<span class="go">        [-0.4877, -3.3176, -1.0506]], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.LogSoftmax.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.LogSoftmax.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.MSELoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">MSELoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.MSELoss" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from:
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html?highlight=mseloss#torch.nn.MSELoss">https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html?highlight=mseloss#torch.nn.MSELoss</a></p>
<p>Creates a criterion that measures the mean squared error (squared L2 norm) between
each element in the input <span class="math notranslate nohighlight">\(x\)</span> and target <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>The unreduced (i.e. with <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> set to <code class="docutils literal notranslate"><span class="pre">'none'</span></code>) loss can be described as:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = \left( x_n - y_n \right)^2,\]</div></div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the batch size. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is not <code class="docutils literal notranslate"><span class="pre">'none'</span></code>
(default <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>), then:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\ell(x, y) =
\begin{cases}
    \operatorname{mean}(L), &amp;  \text{if reduction} = \text{`mean';}\\
    \operatorname{sum}(L),  &amp;  \text{if reduction} = \text{`sum'.}
\end{cases}\end{split}\]</div></div>
<p><span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are tensors of arbitrary shapes with a total
of <span class="math notranslate nohighlight">\(n\)</span> elements each.</p>
<p>The mean operation still operates over all the elements, and divides by <span class="math notranslate nohighlight">\(n\)</span>.</p>
<p>The division by <span class="math notranslate nohighlight">\(n\)</span> can be avoided if one sets <code class="docutils literal notranslate"><span class="pre">reduction</span> <span class="pre">=</span> <span class="pre">'sum'</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <span class="math notranslate nohighlight">\(*\)</span> means, any number of additional
dimensions</p></li>
<li><p>Target: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span><span class="p">[[</span><span class="o">-</span><span class="mf">0.02557137</span><span class="p">,</span> <span class="mf">0.03101675</span><span class="p">,</span> <span class="mf">1.37493674</span><span class="p">],</span>
<span class="gp">... </span><span class="p">[</span><span class="mf">0.25599439</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.08372561</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.21006816</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span><span class="p">[[</span><span class="o">-</span><span class="mf">1.53105064</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.68137555</span><span class="p">,</span> <span class="mf">0.5931354</span><span class="p">],</span>
<span class="gp">... </span><span class="p">[</span><span class="o">-</span><span class="mf">0.49158347</span><span class="p">,</span> <span class="mf">0.93673637</span><span class="p">,</span> <span class="mf">0.1324141</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[2.2665, 0.5075, 0.6112],</span>
<span class="go">        [0.5589, 4.0823, 0.1173]], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(1.3573, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(8.1436, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.MarginRankingLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">MarginRankingLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">margin</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.MarginRankingLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the loss given
inputs <span class="math notranslate nohighlight">\(x1\)</span>, <span class="math notranslate nohighlight">\(x2\)</span>, two 1D mini-batch <cite>Tensors</cite>,
and a label 1D mini-batch tensor <span class="math notranslate nohighlight">\(y\)</span> (containing 1 or -1).</p>
<p>If <span class="math notranslate nohighlight">\(y = 1\)</span> then it assumed the first input should be ranked higher
(have a larger value) than the second input, and vice-versa for <span class="math notranslate nohighlight">\(y = -1\)</span>.</p>
<p>The loss function for each sample in the mini-batch is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{loss}(x1, x2, y) = \max(0, -y * (x1 - x2) + \text{margin})\]</div></div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>margin</strong> (<em>float</em><em>, </em><em>optional</em>) – Has a default value of <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p><cite>x1</cite> : <span class="math notranslate nohighlight">\((N, D)\)</span> where <cite>N</cite> is the batch size and <cite>D</cite> is the size of a sample.</p></li>
<li><p><cite>x2</cite> : <span class="math notranslate nohighlight">\((N, D)\)</span> where <cite>N</cite> is the batch size and <cite>D</cite> is the size of a sample.</p></li>
<li><p>Target: <span class="math notranslate nohighlight">\((N)\)</span></p></li>
<li><p>Output: scalar. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, then <span class="math notranslate nohighlight">\((N)\)</span>.</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MarginRankingLoss</span><span class="p">(</span><span class="n">margin</span> <span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[2., 1., 0.],</span>
<span class="go">        [3., 0., 5.],</span>
<span class="go">        [0., 0., 0.]], dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MarginRankingLoss</span><span class="p">(</span><span class="n">margin</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(8.2000, dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MarginRankingLoss</span><span class="p">(</span><span class="n">margin</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(8.3333, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.MaxPool1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">MaxPool1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_indices</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.MaxPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from: <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d">https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d</a></p>
<p>Applies a 1D max pooling over an input signal composed of several input planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math notranslate nohighlight">\((N, C, L)\)</span>
and output <span class="math notranslate nohighlight">\((N, C, L_{out})\)</span> can be precisely described as:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out(N_i, C_j, k) = \max_{m=0, \ldots, \text{kernel\_size} - 1}
        input(N_i, C_j, stride \times k + m)\]</div></div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly padded with minimum value on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points. <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> is the stride between the elements within the
sliding window. This <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of the pooling parameters.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding
or the input. Sliding windows that would start in the right padded region are ignored.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> – The size of the sliding window, must be &gt; 0.</p></li>
<li><p><strong>stride</strong> – The stride of the sliding window, must be &gt; 0. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>.</p></li>
<li><p><strong>padding</strong> – Implicit negative infinity padding to be added on both sides, must be &gt;= 0 and &lt;= kernel_size / 2.</p></li>
<li><p><strong>dilation</strong> – The stride between elements within a sliding window, must be &gt; 0.</p></li>
<li><p><strong>return_indices</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the argmax along with the max values.
Useful for <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.MaxUnpool1d</span></code> later</p></li>
<li><p><strong>ceil_mode</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape. This
ensures that every element in the input tensor is covered by a sliding window.</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, L_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, L_{out})\)</span>, where</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[L_{out} = \left\lfloor \frac{L_{in} + 2 \times \text{padding} - \text{dilation}
      \times (\text{kernel_size} - 1) - 1}{\text{stride}} + 1\right\rfloor\]</div></div>
</li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">of_maxpool1d</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">of_maxpool1d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="n">oneflow</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.MaxPool1d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.MaxPool1d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.MaxPool2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">MaxPool2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_indices</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.MaxPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from: <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d">https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d</a></p>
<p>Applies a 2D max pooling over an input signal composed of several input planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math notranslate nohighlight">\((N, C, H, W)\)</span>,
output <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> <span class="math notranslate nohighlight">\((kH, kW)\)</span>
can be precisely described as:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    out(N_i, C_j, h, w) ={} &amp; \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\
                            &amp; \text{input}(N_i, C_j, \text{stride[0]} \times h + m,
                                           \text{stride[1]} \times w + n)
\end{aligned}\end{split}\]</div></div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly minimum value padded on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points. <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding
or the input. Sliding windows that would start in the right padded region are ignored.</p>
</div>
<dl class="simple">
<dt>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> can either be:</dt><dd><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimension</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> – the size of the window to take a max over</p></li>
<li><p><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></p></li>
<li><p><strong>padding</strong> – implicit minimum value padding to be added on both sides</p></li>
<li><p><strong>dilation</strong> – a parameter that controls the stride of elements in the window</p></li>
<li><p><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the max indices along with the outputs.
Useful for <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.MaxUnpool2d</span></code> later</p></li>
<li><p><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span>, where</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor\frac{H_{in} + 2 * \text{padding[0]} - \text{dilation[0]}
      \times (\text{kernel_size[0]} - 1) - 1}{\text{stride[0]}} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor\frac{W_{in} + 2 * \text{padding[1]} - \text{dilation[1]}
      \times (\text{kernel_size[1]} - 1) - 1}{\text{stride[1]}} + 1\right\rfloor\]</div></div>
</li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="n">oneflow</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.MaxPool2d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.MaxPool2d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.MaxPool3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">MaxPool3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_indices</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.MaxPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from: <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html#torch.nn.MaxPool3d">https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html#torch.nn.MaxPool3d</a></p>
<p>Applies a 3D max pooling over an input signal composed of several input planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span>,
output <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> <span class="math notranslate nohighlight">\((kD, kH, kW)\)</span>
can be precisely described as:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \text{out}(N_i, C_j, d, h, w) ={} &amp; \max_{k=0, \ldots, kD-1} \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\
                                      &amp; \text{input}(N_i, C_j, \text{stride[0]} \times d + k,
                                                     \text{stride[1]} \times h + m, \text{stride[2]} \times w + n)
\end{aligned}\end{split}\]</div></div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly minimum value on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points. <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding
or the input. Sliding windows that would start in the right padded region are ignored.</p>
</div>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the depth, height and width dimension</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of three ints – in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the height dimension and the third <cite>int</cite> for the width dimension</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> – the size of the window to take a max over</p></li>
<li><p><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></p></li>
<li><p><strong>padding</strong> – implicit minimum value padding to be added on all three sides</p></li>
<li><p><strong>dilation</strong> – a parameter that controls the stride of elements in the window</p></li>
<li><p><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the max indices along with the outputs.
Useful for <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.MaxUnpool3d</span></code> later</p></li>
<li><p><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span>, where</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times
  (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times
  (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2] \times
  (\text{kernel_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor\]</div></div>
</li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">of_maxpool3d</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">of_maxpool3d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="n">oneflow</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.MaxPool3d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.MaxPool3d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.MinMaxObserver">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">MinMaxObserver</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">quantization_formula</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'google'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_bit</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_scheme</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'symmetric'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_layer_quantization</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.MinMaxObserver" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the quantization parameters of the input tensor.</p>
<p>First compute the max and min values of input tensor:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; max\_value = max(input)\\&amp; min\_value = min(input)\end{aligned}\end{align} \]</div></div>
<p>Then compute the scale and zero_point with the following equations:</p>
<blockquote>
<div><p>if quantization_scheme == “symmetric”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; denom = 2^{quantization\_to\_bit - 1} - 1\\&amp; scale = max(|max\_value|,|min\_value|) / denom\\&amp; zero\_point = 0\end{aligned}\end{align} \]</div></div>
<p>elif quantization_scheme == “affine”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; denom = 2^{quantization\_to\_bit} - 1\\&amp; scale = (max\_value - min\_value) / denom\\&amp; zero\_point = -min\_value / scale\end{aligned}\end{align} \]</div></div>
</div></blockquote>
<p>If per_layer_quantization is False, then the shape of scale and zero_point will be (input.shape[0],).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>quantization_bit</strong> (<em>int</em>) – Quantize input to uintX / intX, X can be in range [2, 8]. Defaults to 8.</p></li>
<li><p><strong>quantization_scheme</strong> (<em>str</em>) – “symmetric” or “affine”, quantize to signed / unsigned integer. Defaults to “symmetric”.</p></li>
<li><p><strong>quantization_formula</strong> (<em>str</em>) – Support “google” or “cambricon”.</p></li>
<li><p><strong>per_layer_quantization</strong> (<em>bool</em>) – True or False, means per-layer / per-channel quantization. Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The scale and zero_point of input tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tuple[<a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">oneflow.Tensor</a>, <a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">oneflow.Tensor</a>]</p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">weight</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_bit</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_scheme</span> <span class="o">=</span> <span class="s2">"symmetric"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_formula</span> <span class="o">=</span> <span class="s2">"google"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">per_layer_quantization</span> <span class="o">=</span> <span class="bp">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">min_max_observer</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MinMaxObserver</span><span class="p">(</span><span class="n">quantization_formula</span><span class="o">=</span><span class="n">quantization_formula</span><span class="p">,</span> <span class="n">quantization_bit</span><span class="o">=</span><span class="n">quantization_bit</span><span class="p">,</span>
<span class="gp">... </span><span class="n">quantization_scheme</span><span class="o">=</span><span class="n">quantization_scheme</span><span class="p">,</span> <span class="n">per_layer_quantization</span><span class="o">=</span><span class="n">per_layer_quantization</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> <span class="n">min_max_observer</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">input_tensor</span><span class="p">,</span> <span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Mish">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Mish</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Mish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{Mish}(x) = x * \text{Tanh}(\text{Softplus}(x))\]</div></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference external" href="https://arxiv.org/abs/1908.08681">Mish: A Self Regularized Non-Monotonic Neural Activation Function</a></p>
</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mish</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Mish</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">mish</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.8651, 1.9440, 2.9865], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ModuleDict">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ModuleDict</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">modules</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">oneflow.nn.module.Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ModuleDict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ModuleList">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ModuleList</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">modules</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow.nn.module.Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ModuleList" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.MovingAverageMinMaxObserver">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">MovingAverageMinMaxObserver</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_formula</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'google'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_update_after_iters</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_bit</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_scheme</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'symmetric'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.MovingAverageMinMaxObserver" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the quantization parameters based on the moving average of the input tensor’s min and max values.</p>
<p>First compute the moving_max and moving_min value of input tensor:</p>
<blockquote>
<div><p>if quantization_scheme == “symmetric”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; moving\_max = moving\_max * momentum + |max(input)| * (1 - momentum)\\&amp; moving\_min = moving\_max\end{aligned}\end{align} \]</div></div>
<p>elif quantization_scheme == “affine”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; moving\_max = moving\_max * momentum + max(input) * (1 - momentum)\\&amp; moving\_min = moving\_min * momentum + min(input) * (1 - momentum)\end{aligned}\end{align} \]</div></div>
</div></blockquote>
<p>The moving average of min and max values are initialized as the first batch of input <cite>Blob</cite>’s min and max.</p>
<p>Then compute the scale and zero_point with the following equations:</p>
<blockquote>
<div><p>if quantization_scheme == “symmetric”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; denom = 2^{quantization\_to\_bit - 1} - 1\\&amp; scale = moving\_max / denom\\&amp; zero\_point = 0\end{aligned}\end{align} \]</div></div>
<p>elif quantization_scheme == “affine”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; denom = 2^{quantization\_to\_bit} - 1\\&amp; scale = (moving\_max - moving\_min) / denom\\&amp; zero\_point = -moving\_min / scale\end{aligned}\end{align} \]</div></div>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">current_train_step</span></code> can be directly assigned to an optimizer(eg.SGD) step.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>training</strong> (<em>bool</em>) – Is the model in training state. Defaults to False.</p></li>
<li><p><strong>quantization_bit</strong> (<em>int</em>) – Quantize input to uintX / intX, X can be in range [2, 8]. Defaults to 8.</p></li>
<li><p><strong>quantization_scheme</strong> (<em>str</em>) – “symmetric” or “affine”, quantize to signed / unsigned integer. Defaults to “symmetric”.</p></li>
<li><p><strong>quantization_formula</strong> (<em>str</em>) – Support “google” or “cambricon”.</p></li>
<li><p><strong>momentum</strong> (<em>float</em>) – Smoothing parameter for exponential moving average operation. Defaults to 0.95.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The scale and zero_point of input tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tuple[<a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">oneflow.Tensor</a>, <a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">oneflow.Tensor</a>]</p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">weight</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">current_train_step_tensor</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>  <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
<span class="gp">... </span>   <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_bit</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_scheme</span> <span class="o">=</span> <span class="s2">"symmetric"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_formula</span> <span class="o">=</span> <span class="s2">"google"</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">moving_average_min_max_observer</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">(</span><span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">quantization_formula</span><span class="o">=</span><span class="n">quantization_formula</span><span class="p">,</span>
<span class="gp">... </span>                                                                      <span class="n">stop_update_after_iters</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">quantization_bit</span><span class="o">=</span><span class="n">quantization_bit</span><span class="p">,</span>
<span class="gp">... </span>                                                                      <span class="n">quantization_scheme</span><span class="o">=</span><span class="n">quantization_scheme</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span>
<span class="gp">... </span>                                                                      <span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">)</span> <span class="o">=</span> <span class="n">moving_average_min_max_observer</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">input_tensor</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">current_train_step_tensor</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.MovingAverageMinMaxObserver.reset_running_stats">
<code class="sig-name descname"><span class="pre">reset_running_stats</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.MovingAverageMinMaxObserver.reset_running_stats" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.NLLLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">NLLLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_index</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">-</span> <span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.NLLLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>The negative log likelihood loss. It is useful to train a classification
problem with <cite>C</cite> classes.</p>
<p>The <cite>input</cite> given through a forward call is expected to contain
log-probabilities of each class. <cite>input</cite> has to be a Tensor of size either
<span class="math notranslate nohighlight">\((minibatch, C)\)</span> or <span class="math notranslate nohighlight">\((minibatch, C, d_1, d_2, ..., d_K)\)</span>
with <span class="math notranslate nohighlight">\(K \geq 1\)</span> for the <cite>K</cite>-dimensional case (described later).</p>
<p>Obtaining log-probabilities in a neural network is easily achieved by
adding a  <cite>LogSoftmax</cite>  layer in the last layer of your network.
You may use <cite>CrossEntropyLoss</cite> instead, if you prefer not to add an extra
layer.</p>
<p>The <cite>target</cite> that this loss expects should be a class index in the range <span class="math notranslate nohighlight">\([0, C-1]\)</span>
where <cite>C = number of classes</cite>;</p>
<p>The unreduced (i.e. with <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> set to <code class="docutils literal notranslate"><span class="pre">'none'</span></code>) loss can be described as:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - w_{y_n} x_{n,y_n}, \quad
w_{c} = \mathbb{1},\]</div></div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the input, <span class="math notranslate nohighlight">\(y\)</span> is the target, <span class="math notranslate nohighlight">\(w\)</span> is the weight, and
<span class="math notranslate nohighlight">\(N\)</span> is the batch size. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is not <code class="docutils literal notranslate"><span class="pre">'none'</span></code>
(default <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>), then</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\ell(x, y) = \begin{cases}
    \sum_{n=1}^N \frac{1}{N} l_n, &amp;
    \text{if reduction} = \text{`mean';}\\
    \sum_{n=1}^N l_n,  &amp;
    \text{if reduction} = \text{`sum'.}
\end{cases}\end{split}\]</div></div>
<p>Can also be used for higher dimension inputs, such as 2D images, by providing
an input of size <span class="math notranslate nohighlight">\((minibatch, C, d_1, d_2, ..., d_K)\)</span> with <span class="math notranslate nohighlight">\(K \geq 1\)</span>,
where <span class="math notranslate nohighlight">\(K\)</span> is the number of dimensions, and a target of appropriate shape
(see below). In the case of images, it computes NLL loss per-pixel.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will
be applied, <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the weighted mean of the output is taken,
<code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span><span class="p">[[</span><span class="o">-</span><span class="mf">0.1664078</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7256707</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.14690138</span><span class="p">],</span>
<span class="gp">... </span><span class="p">[</span><span class="o">-</span><span class="mf">0.21474946</span><span class="p">,</span> <span class="mf">0.53737473</span><span class="p">,</span> <span class="mf">0.99684894</span><span class="p">],</span>
<span class="gp">... </span><span class="p">[</span><span class="o">-</span><span class="mf">1.135804</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.50371903</span><span class="p">,</span> <span class="mf">0.7645404</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([ 0.1664, -0.5374, -0.7645], dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(-1.1355, dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(-0.3785, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.OFRecordBytesDecoder">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">OFRecordBytesDecoder</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">blob_name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.OFRecordBytesDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>This operator reads an tensor as bytes. The output might need</p>
<p>further decoding process like cv2.imdecode() for images and decode(“utf-8”)</p>
<p>for characters,depending on the downstream task.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>blob_name</strong> – The name of the target feature in OFRecord.</p></li>
<li><p><strong>name</strong> – The name for this component in the graph.</p></li>
<li><p><strong>input</strong> – the Tensor which might be provided by an OFRecordReader.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The result Tensor encoded with bytes.</p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">example</span><span class="p">():</span>
<span class="gp">... </span>     <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="gp">... </span>     <span class="n">record_reader</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">OFRecordReader</span><span class="p">(</span>
<span class="gp">... </span>        <span class="s2">"dataset/"</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">part_name_suffix_length</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="gp">... </span>     <span class="p">)</span>
<span class="gp">... </span>     <span class="n">val_record</span> <span class="o">=</span> <span class="n">record_reader</span><span class="p">()</span>

<span class="gp">... </span>     <span class="n">bytesdecoder_img</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">OFRecordBytesDecoder</span><span class="p">(</span><span class="s2">"encoded"</span><span class="p">)</span>

<span class="gp">... </span>     <span class="n">image_bytes_batch</span> <span class="o">=</span> <span class="n">bytesdecoder_img</span><span class="p">(</span><span class="n">val_record</span><span class="p">)</span>

<span class="gp">... </span>     <span class="n">image_bytes</span> <span class="o">=</span> <span class="n">image_bytes_batch</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="gp">... </span>     <span class="k">return</span> <span class="n">image_bytes</span>
<span class="gp">... </span><span class="n">example</span><span class="p">()</span>  
<span class="go">array([255 216 255 ...  79 255 217], dtype=uint8)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.OFRecordImageDecoder">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">OFRecordImageDecoder</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">blob_name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">color_space</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'BGR'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.OFRecordImageDecoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.OFRecordImageDecoderRandomCrop">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">OFRecordImageDecoderRandomCrop</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">blob_name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">color_space</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'BGR'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_attempts</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_seed</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_area</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">[0.08,</span> <span class="pre">1.0]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_aspect_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">[0.75,</span> <span class="pre">1.333333]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.OFRecordImageDecoderRandomCrop" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.OFRecordRawDecoder">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">OFRecordRawDecoder</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">blob_name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">oneflow._oneflow_internal.dtype</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim1_varying_length</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncate</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">auto_zero_padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.OFRecordRawDecoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.OFRecordReader">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">OFRecordReader</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ofrecord_dir</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_part_num</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">part_name_prefix</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'part-'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">part_name_suffix_length</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_shuffle</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle_buffer_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1024</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle_after_epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_seed</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.device</span><span class="p"><span class="pre">,</span> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">placement</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.placement</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sbp</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.sbp.sbp</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.sbp.sbp</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.OFRecordReader" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.PReLU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">PReLU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_parameters</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.PReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[PReLU(x) = \max(0,x) + a * \min(0,x)\]</div></div>
<p>Here <span class="math notranslate nohighlight">\(a\)</span> is a learnable parameter. When called without arguments, <cite>nn.PReLU()</cite> uses a single
parameter <span class="math notranslate nohighlight">\(a\)</span> across all input channels. If called with <cite>nn.PReLU(nChannels)</cite>,
a separate <span class="math notranslate nohighlight">\(a\)</span> is used for each input channel.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>weight decay should not be used when learning <span class="math notranslate nohighlight">\(a\)</span> for good performance.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Channel dim is the 2nd dim of input. When input has dims &lt; 2, then there is
no channel dim and the number of channels = 1.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_parameters</strong> (<em>int</em>) – number of <span class="math notranslate nohighlight">\(a\)</span> to learn.
Although it takes an int as input, there is only two values are legitimate:
1, or the number of channels at input. Default: 1</p></li>
<li><p><strong>init</strong> (<em>float</em>) – the initial value of <span class="math notranslate nohighlight">\(a\)</span>. Default: 0.25</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
<dt>Attr:</dt><dd><ul class="simple">
<li><p>weight (Tensor): the learnable weights of shape (<code class="xref py py-attr docutils literal notranslate"><span class="pre">num_parameters</span></code>).</p></li>
</ul>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[[[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]]]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="go">[[[[ 1.  -0.5]</span>
<span class="go">   [ 3.   4. ]]]]</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.PReLU.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.PReLU.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Parameter">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Parameter</span></code><a class="headerlink" href="#oneflow.nn.Parameter" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ParameterDict">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ParameterDict</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ParameterDict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ParameterList">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ParameterList</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ParameterList" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.PixelShuffle">
<code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">PixelShuffle</span></code><a class="headerlink" href="#oneflow.nn.PixelShuffle" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <a class="reference internal" href="#oneflow.nn.modules.pixelshuffle.PixelShufflev2" title="oneflow.nn.modules.pixelshuffle.PixelShufflev2"><code class="xref py py-class docutils literal notranslate"><span class="pre">oneflow.nn.modules.pixelshuffle.PixelShufflev2</span></code></a></p>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Quantization">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Quantization</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">quantization_formula</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'google'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_bit</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_scheme</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'symmetric'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Quantization" title="Permalink to this definition">¶</a></dt>
<dd><p>Simulate the quantize operation in inference time.</p>
<p>The output will be computed as:</p>
<blockquote>
<div><p>if quantization_scheme == “symmetric”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; quant\_max = 2^{quantization\_to\_bit - 1} - 1\\&amp; quant\_min = -quant\_max\\&amp; clamp(round(x / scale), quant\_min, quant\_max)\end{aligned}\end{align} \]</div></div>
<p>elif quantization_scheme == “affine”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; quant\_max = 2^{quantization\_to\_bit} - 1\\&amp; quant\_min = 0\\&amp; (clamp(round(x / scale + zero\_point), quant\_min, quant\_max) - zero\_point)\end{aligned}\end{align} \]</div></div>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>quantization_bit</strong> (<em>int</em>) – Quantize input to uintX / intX, X can be in range [2, 8]. Defaults to 8.</p></li>
<li><p><strong>quantization_scheme</strong> (<em>str</em>) – “symmetric” or “affine”, quantize to signed / unsigned integer. Defaults to “symmetric”.</p></li>
<li><p><strong>quantization_formula</strong> (<em>str</em>) – Support “google” or “cambricon”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Input tensor after quantize operation.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">oneflow.Tensor</a></p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">weight</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_bit</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_scheme</span> <span class="o">=</span> <span class="s2">"symmetric"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_formula</span> <span class="o">=</span> <span class="s2">"google"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">per_layer_quantization</span> <span class="o">=</span> <span class="bp">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">min_max_observer</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MinMaxObserver</span><span class="p">(</span><span class="n">quantization_formula</span><span class="o">=</span><span class="n">quantization_formula</span><span class="p">,</span> <span class="n">quantization_bit</span><span class="o">=</span><span class="n">quantization_bit</span><span class="p">,</span>
<span class="gp">... </span><span class="n">quantization_scheme</span><span class="o">=</span><span class="n">quantization_scheme</span><span class="p">,</span> <span class="n">per_layer_quantization</span><span class="o">=</span><span class="n">per_layer_quantization</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Quantization</span><span class="p">(</span><span class="n">quantization_formula</span><span class="o">=</span><span class="n">quantization_formula</span><span class="p">,</span> <span class="n">quantization_bit</span><span class="o">=</span><span class="n">quantization_bit</span><span class="p">,</span>
<span class="gp">... </span><span class="n">quantization_scheme</span><span class="o">=</span><span class="n">quantization_scheme</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> <span class="n">min_max_observer</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">input_tensor</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">output_tensor</span> <span class="o">=</span> <span class="n">quantization</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">input_tensor</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">scale</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">zero_point</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ReLU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ReLU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>ReLU 激活函数，对张量中的每一个元素做 element-wise 运算，公式如下:</p>
<p><span class="math notranslate nohighlight">\(\text{ReLU}(x) = (x)^+ = \max(0, x)\)</span></p>
<dl class="simple">
<dt>参数:</dt><dd><p>inplace: 是否做 in-place 操作。 默认为 <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
</dd>
<dt>形状:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> 其中 <cite>*</cite> 的意思是，可以指定任意维度</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span> 输入形状与输出形状一致</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relu</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">tensor([1., 0., 3.], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.ReLU.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ReLU.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ReLU6">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ReLU6</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ReLU6" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\text{Relu6}(x) = \begin{cases}
    6 &amp; \text{ if } x &gt; 6 \\
    0 &amp; \text{ if } x &lt; 0 \\
    x &amp; \text{ otherwise } \\
\end{cases}\end{split}\]</div></div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relu6</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">relu6</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.0000, 0.0000, 0.5000], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.ReLU6.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ReLU6.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ReflectionPad2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ReflectionPad2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ReflectionPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from:
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad2d.html">https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad2d.html</a></p>
<p>This operator pads the input tensor using the reflection of the input boundary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>padding</strong> (<em>Union</em><em>[</em><em>int</em><em>,</em><em>tuple</em><em>]</em>) – The size or bundary of padding, if is <cite>int</cite> uses the same padding in all dimension; if 4-dims <cite>tuple</cite>, uses <span class="math notranslate nohighlight">\((\text{padding}_{\text{left}}, \text{padding}_{\text{right}}, \text{padding}_{\text{top}}, \text{padding}_{\text{bottom}} )\)</span></p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns a new tensor which is result of the reflection padding of the input tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H_{\text{in}}, W_{\text{in}})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H_{\text{out}}, W_{\text{out}})\)</span> where</p>
<p><span class="math notranslate nohighlight">\(H_{\text{out}} = H_{\text{in}} + \text{padding}_{\text{top}} + \text{padding}_{\text{bottom}}\)</span></p>
<p><span class="math notranslate nohighlight">\(W_{\text{out}} = W_{\text{in}} + \text{padding}_{\text{left}} + \text{padding}_{\text{right}}\)</span></p>
</li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">18</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReflectionPad2d</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[[[ 5.,  4.,  3.,  4.,  5.,  4.,  3.],</span>
<span class="go">          [ 2.,  1.,  0.,  1.,  2.,  1.,  0.],</span>
<span class="go">          [ 5.,  4.,  3.,  4.,  5.,  4.,  3.],</span>
<span class="go">          [ 8.,  7.,  6.,  7.,  8.,  7.,  6.],</span>
<span class="go">          [ 5.,  4.,  3.,  4.,  5.,  4.,  3.]],</span>

<span class="go">         [[14., 13., 12., 13., 14., 13., 12.],</span>
<span class="go">          [11., 10.,  9., 10., 11., 10.,  9.],</span>
<span class="go">          [14., 13., 12., 13., 14., 13., 12.],</span>
<span class="go">          [17., 16., 15., 16., 17., 16., 15.],</span>
<span class="go">          [14., 13., 12., 13., 14., 13., 12.]]]], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.ReflectionPad2d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.ReflectionPad2d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ReplicationPad2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ReplicationPad2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ReplicationPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from:
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad2d.html?highlight=replicationpad2d#torch.nn.ReplicationPad2d">https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad2d.html?highlight=replicationpad2d#torch.nn.ReplicationPad2d</a></p>
<p>Pads the input tensor using the replication of the input boundary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>padding</strong> (<em>Union</em><em>[</em><em>int</em><em>, </em><em>tuple</em><em>, </em><em>list</em><em>]</em>) – the size of the padding. If is <cite>int</cite>, uses the same padding in all boundaries. If a 4-<cite>tuple</cite>, uses (<span class="math notranslate nohighlight">\(\mathrm{padding_{left}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{right}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{top}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{bottom}}\)</span>)</p>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> where</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(H_{out} = H_{in} + \mathrm{padding_{top}} + \mathrm{padding_{bottom}}\)</span></p>
<p><span class="math notranslate nohighlight">\(W_{out} = W_{in} + \mathrm{padding_{left}} + \mathrm{padding_{right}}\)</span></p>
</div></blockquote>
</li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad2d</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">18</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_int</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">18</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="go">oneflow.Size([1, 2, 5, 7])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([[[[ 0.,  0.,  0.,  1.,  2.,  2.,  2.],</span>
<span class="go">          [ 0.,  0.,  0.,  1.,  2.,  2.,  2.],</span>
<span class="go">          [ 3.,  3.,  3.,  4.,  5.,  5.,  5.],</span>
<span class="go">          [ 6.,  6.,  6.,  7.,  8.,  8.,  8.],</span>
<span class="go">          [ 6.,  6.,  6.,  7.,  8.,  8.,  8.]],</span>

<span class="go">         [[ 9.,  9.,  9., 10., 11., 11., 11.],</span>
<span class="go">          [ 9.,  9.,  9., 10., 11., 11., 11.],</span>
<span class="go">          [12., 12., 12., 13., 14., 14., 14.],</span>
<span class="go">          [15., 15., 15., 16., 17., 17., 17.],</span>
<span class="go">          [15., 15., 15., 16., 17., 17., 17.]]]], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.ReplicationPad2d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.ReplicationPad2d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.SELU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">SELU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.SELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<p>The formula is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{SELU}(x) = \text{scale} * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))\]</div></div>
<p>with <span class="math notranslate nohighlight">\(\alpha = 1.6732632423543772848170429916717\)</span> and</p>
<p><span class="math notranslate nohighlight">\(\text{scale} = 1.0507009873554804934193349852946\)</span>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When using <code class="docutils literal notranslate"><span class="pre">kaiming_normal</span></code> or <code class="docutils literal notranslate"><span class="pre">kaiming_normal_</span></code> for initialisation,
<code class="docutils literal notranslate"><span class="pre">nonlinearity='linear'</span></code> should be used instead of <code class="docutils literal notranslate"><span class="pre">nonlinearity='selu'</span></code>
in order to get <a class="reference external" href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a>.
See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.init.calculate_gain()</span></code> for more information.</p>
</div>
<p>More details can be found in the paper <a class="reference external" href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a>.</p>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selu</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SELU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">selu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([1.0507, 2.1014, 3.1521], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Sequential">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Sequential</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Sequential" title="Permalink to this definition">¶</a></dt>
<dd><p>A sequential container.
Modules will be added to it in the order they are passed in the constructor.
Alternatively, an ordered dict of modules can also be passed in.</p>
<p>To make it easier to understand, here is a small example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span> 
<span class="go">Sequential(</span>
<span class="go">  (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span>
<span class="go">  (1): ReLU()</span>
<span class="go">  (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span>
<span class="go">  (3): ReLU()</span>
<span class="go">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
<span class="gp">... </span>   <span class="p">(</span><span class="s1">'conv1'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">)),</span>
<span class="gp">... </span>   <span class="p">(</span><span class="s1">'relu1'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
<span class="gp">... </span>   <span class="p">(</span><span class="s1">'conv2'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">5</span><span class="p">)),</span>
<span class="gp">... </span>   <span class="p">(</span><span class="s1">'relu2'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
<span class="gp">... </span><span class="p">]))</span> 
<span class="go">Sequential(</span>
<span class="go">  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span>
<span class="go">  (relu1): ReLU()</span>
<span class="go">  (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span>
<span class="go">  (relu2): ReLU()</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.SiLU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">SiLU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.SiLU" title="Permalink to this definition">¶</a></dt>
<dd><p>SiLU(Swish) activation:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{SiLU}(x) = x * sigmoid(x)\]</div></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference external" href="https://arxiv.org/abs/1606.08415">Gaussian Error Linear Units (GELUs)</a>
where the SiLU (Sigmoid Linear Unit) was originally coined, and see
<a class="reference external" href="https://arxiv.org/abs/1702.03118">Sigmoid-Weighted Linear Units for Neural Network Function Approximation
in Reinforcement Learning</a> and <a class="reference external" href="https://arxiv.org/abs/1710.05941v1">Swish:
a Self-Gated Activation Function</a>
where the SiLU was experimented with later.</p>
</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">silu</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SiLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">silu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.7311, 1.7616, 2.8577], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Sigmoid">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Sigmoid</span></code><a class="headerlink" href="#oneflow.nn.Sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>应用逐元素函数：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)}\]</div></div>
<dl class="simple">
<dt>图型：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> 其中 <cite>*</cite> 表示任意数量的附加维度</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span> 与输入相同的形状</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.81733328</span><span class="p">,</span> <span class="mf">0.43621480</span><span class="p">,</span> <span class="mf">0.10351428</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.6937, 0.6074, 0.5259], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.SmoothL1Loss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">SmoothL1Loss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.SmoothL1Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that uses a squared term if the absolute
element-wise error falls below beta and an L1 term otherwise.
The interface is consistent with PyTorch.
The documentation is referenced from:
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html">https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html</a></p>
<p>It is less sensitive to outliers than <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.MSELoss</span></code> and in some cases
prevents exploding gradients (e.g. see the paper <a class="reference external" href="https://openaccess.thecvf.com/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf">Fast R-CNN</a> by Ross Girshick)..</p>
<p>For a batch of size <span class="math notranslate nohighlight">\(N\)</span>, the unreduced loss can be described as:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\ell(x, y) = L = \{l_1, ..., l_N\}^T\]</div></div>
<p>with</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}l_n = \begin{cases}
0.5 (x_n - y_n)^2 / beta, &amp; \text{if } |x_n - y_n| &lt; beta \\
|x_n - y_n| - 0.5 * beta, &amp; \text{otherwise }
\end{cases}\end{split}\]</div></div>
<p>If <cite>reduction</cite> is not <cite>none</cite>, then:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\ell(x, y) =
\begin{cases}
    \operatorname{mean}(L), &amp;  \text{if reduction} = \text{`mean';}\\
    \operatorname{sum}(L),  &amp;  \text{if reduction} = \text{`sum'.}
\end{cases}\end{split}\]</div></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Smooth L1 loss can be seen as exactly <a class="reference internal" href="#oneflow.nn.L1Loss" title="oneflow.nn.L1Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">L1Loss</span></code></a>, but with the <span class="math notranslate nohighlight">\(|x - y| &lt; beta\)</span>
portion replaced with a quadratic function such that its slope is 1 at <span class="math notranslate nohighlight">\(|x - y| = beta\)</span>.
The quadratic segment smooths the L1 loss near <span class="math notranslate nohighlight">\(|x - y| = 0\)</span>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Smooth L1 loss is closely related to <code class="xref py py-class docutils literal notranslate"><span class="pre">HuberLoss</span></code>, being
equivalent to <span class="math notranslate nohighlight">\(huber(x, y) / beta\)</span> (note that Smooth L1’s beta hyper-parameter is
also known as delta for Huber). This leads to the following differences:</p>
<ul class="simple">
<li><p>As beta -&gt; 0, Smooth L1 loss converges to <a class="reference internal" href="#oneflow.nn.L1Loss" title="oneflow.nn.L1Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">L1Loss</span></code></a>, while <code class="xref py py-class docutils literal notranslate"><span class="pre">HuberLoss</span></code>
converges to a constant 0 loss.</p></li>
<li><p>As beta -&gt; <span class="math notranslate nohighlight">\(+\infty\)</span>, Smooth L1 loss converges to a constant 0 loss, while
<code class="xref py py-class docutils literal notranslate"><span class="pre">HuberLoss</span></code> converges to <a class="reference internal" href="#oneflow.nn.MSELoss" title="oneflow.nn.MSELoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MSELoss</span></code></a>.</p></li>
<li><p>For Smooth L1 loss, as beta varies, the L1 segment of the loss has a constant slope of 1.
For <code class="xref py py-class docutils literal notranslate"><span class="pre">HuberLoss</span></code>, the slope of the L1 segment is beta.</p></li>
</ul>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size_average</strong> (<em>bool</em><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there are multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduce</strong> (<em>bool</em><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
<li><p><strong>beta</strong> (<em>float</em><em>, </em><em>optional</em>) – Specifies the threshold at which to change between L1 and L2 loss.
The value must be non-negative. Default: 1.0</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <span class="math notranslate nohighlight">\(*\)</span> means any number of additional dimensions</p></li>
<li><p>Target: <span class="math notranslate nohighlight">\((N, *)\)</span>; same shape as the input</p></li>
<li><p>Output: scalar. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, then <span class="math notranslate nohighlight">\((N, *)\)</span>; same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.0200, 0.1250, 1.7000, 0.0050, 0.1800], dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(0.4060, dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(2.0300, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Softmax">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Softmax</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> → <span class="pre">Tensor</span><a class="headerlink" href="#oneflow.nn.Softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>将 Softmax 函数应用于 n 维输入 tensor 并重新缩放 tensor，以便n 维输出 tensor
的元素位于 [0,1] 范围内并且和为 1。</p>
<p>Softmax 的公式为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}\]</div></div>
<p>当输入张量是稀疏张量时，则未被指定的值将被视为 <code class="docutils literal notranslate"><span class="pre">-inf</span></code> 。</p>
<dl class="simple">
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((*)\)</span> 其中 <cite>*</cite> 表示任意数量的附加维度</p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((*)\)</span> ，与输入的形状相同</p></li>
</ul>
</dd>
<dt>返回类型：</dt><dd><p>oneflow.tensor: 与输入具有相同维度和形状的张量，其值在 [0, 1] 范围内</p>
</dd>
<dt>参数：</dt><dd><p><strong>dim</strong> (int): 要进行 Softmax 计算的维度（因此沿 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> 的每个切片总和为 1）。</p>
</dd>
</dl>
<p>示例：</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="k">as</span> <span class="nn">flow</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.46716809</span><span class="p">,</span>  <span class="mf">0.40112534</span><span class="p">,</span>  <span class="mf">0.61984003</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.31244969</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.42528763</span><span class="p">,</span>  <span class="mf">1.47953856</span><span class="p">]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[[0.1575, 0.3754, 0.4671],</span>
<span class="go">         [0.0507, 0.1230, 0.8263]]], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Softmax.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Softmax.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Softplus">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Softplus</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">20</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Softplus" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))\]</div></div>
<p>SoftPlus is a smooth approximation to the ReLU function and can be used
to constrain the output of a machine to always be positive.</p>
<p>For numerical stability the implementation reverts to the linear function
when <span class="math notranslate nohighlight">\(input \times \beta &gt; threshold\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>beta</strong> – the <span class="math notranslate nohighlight">\(\beta\)</span> value for the Softplus formulation. Default: 1</p></li>
<li><p><strong>threshold</strong> – values above this revert to a linear function. Default: 20</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softplus</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Softplus</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.4741, 0.6931, 0.9741], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Softplus.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Softplus.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Softsign">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Softsign</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Softsign" title="Permalink to this definition">¶</a></dt>
<dd><p>The SoftSign activation.</p>
<p>The formula is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[SoftSign(x) = \frac{x}{1 + |x|}\]</div></div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softsign</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Softsign</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">softsign</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.5000, 0.6667, 0.7500], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Tanh">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Tanh</span></code><a class="headerlink" href="#oneflow.nn.Tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>This operator computes the hyperbolic tangent value of Tensor.</p>
<p>The equation is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = \frac{e^x-e^{-x}}{e^x+e^{-x}}\]</div></div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input</strong> (<a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor"><em>oneflow.Tensor</em></a>) – A Tensor</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The result Tensor</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">oneflow.Tensor</a></p>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tanh</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([-0.7616,  0.0000,  0.7616], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.TripletMarginLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">TripletMarginLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">margin</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">swap</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size_average</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.TripletMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the triplet loss given an input
tensors <span class="math notranslate nohighlight">\(x1\)</span>, <span class="math notranslate nohighlight">\(x2\)</span>, <span class="math notranslate nohighlight">\(x3\)</span> and a margin with a value greater than <span class="math notranslate nohighlight">\(0\)</span>.
This is used for measuring a relative similarity between samples. A triplet
is composed by <cite>a</cite>, <cite>p</cite> and <cite>n</cite> (i.e., <cite>anchor</cite>, <cite>positive examples</cite> and <cite>negative
examples</cite> respectively). The shapes of all input tensors should be
<span class="math notranslate nohighlight">\((N, D)\)</span>.</p>
<p>The distance swap is described in detail in the paper <a class="reference external" href="http://www.bmva.org/bmvc/2016/papers/paper119/index.html">Learning shallow
convolutional feature descriptors with triplet losses</a> by
V. Balntas, E. Riba et al.</p>
<p>The loss function for each sample in the mini-batch is:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}\]</div></div>
<p>where</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p\]</div></div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>margin</strong> (<em>float</em><em>, </em><em>optional</em>) – Default: <span class="math notranslate nohighlight">\(1\)</span>.</p></li>
<li><p><strong>p</strong> (<em>float</em><em>, </em><em>optional</em>) – The norm degree for pairwise distance. Default: <span class="math notranslate nohighlight">\(2.0\)</span>.</p></li>
<li><p><strong>swap</strong> (<em>bool</em><em>, </em><em>optional</em>) – The distance swap is described in detail in the paper
<cite>Learning shallow convolutional feature descriptors with triplet losses</cite> by
V. Balntas, E. Riba et al. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, D)\)</span> where <span class="math notranslate nohighlight">\(D\)</span> is the vector dimension.</p></li>
<li><p>Output: A Tensor of shape <span class="math notranslate nohighlight">\((N)\)</span> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, or a scalar
otherwise.</p></li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">triplet_loss</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">TripletMarginLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">anchor</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">positive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">negative</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">triplet_loss</span><span class="p">(</span><span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">anchor</span><span class="p">),</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">positive</span><span class="p">),</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">negative</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor(6.2971, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Upsample">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Upsample</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="p"><span class="pre">…</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_factor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="p"><span class="pre">…</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'nearest'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">align_corners</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Upsample" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.</p>
<p>The documentation is referenced from: <a class="reference external" href="https://pytorch.org/docs/1.9.0/_modules/torch/nn/modules/upsampling.html#Upsample">https://pytorch.org/docs/1.9.0/_modules/torch/nn/modules/upsampling.html#Upsample</a></p>
<p>Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.</p>
<p>The input data is assumed to be of the form
<cite>minibatch x channels x [optional depth] x [optional height] x width</cite>.
Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor.</p>
<p>The algorithms available for upsampling are nearest neighbor and linear,
bilinear, bicubic and trilinear for 3D, 4D and 5D input Tensor,
respectively.</p>
<p>One can either give a <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code> or the target output <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> to
calculate the output size. (You cannot give both, as it is ambiguous)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>int</em><em> or </em><em>Tuple</em><em>[</em><em>int</em><em>] or </em><em>Tuple</em><em>[</em><em>int</em><em>, </em><em>int</em><em>] or </em><em>Tuple</em><em>[</em><em>int</em><em>, </em><em>int</em><em>, </em><em>int</em><em>]</em><em>, </em><em>optional</em>) – output spatial sizes</p></li>
<li><p><strong>scale_factor</strong> (<em>float</em><em> or </em><em>Tuple</em><em>[</em><em>float</em><em>] or </em><em>Tuple</em><em>[</em><em>float</em><em>, </em><em>float</em><em>] or </em><em>Tuple</em><em>[</em><em>float</em><em>, </em><em>float</em><em>, </em><em>float</em><em>]</em><em>, </em><em>optional</em>) – multiplier for spatial size. Has to match input size if it is a tuple.</p></li>
<li><p><strong>mode</strong> (<em>str</em><em>, </em><em>optional</em>) – the upsampling algorithm: one of <code class="docutils literal notranslate"><span class="pre">'nearest'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'linear'</span></code>, <code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code>, <code class="docutils literal notranslate"><span class="pre">'bicubic'</span></code> and <code class="docutils literal notranslate"><span class="pre">'trilinear'</span></code>.
Default: <code class="docutils literal notranslate"><span class="pre">'nearest'</span></code></p></li>
<li><p><strong>align_corners</strong> (<em>bool</em><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, the corner pixels of the input
and output tensors are aligned, and thus preserving the values at
those pixels. This only has effect when <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code> is
<code class="docutils literal notranslate"><span class="pre">'linear'</span></code>, <code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'trilinear'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, W_{in})\)</span>, <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span> or <span class="math notranslate nohighlight">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, W_{out})\)</span>, <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span>
or <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span>, where</p></li>
</ul>
</dd>
</dl>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[D_{out} = \left\lfloor D_{in} \times \text{scale_factor} \right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor H_{in} \times \text{scale_factor} \right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor W_{in} \times \text{scale_factor} \right\rfloor\]</div></div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>With <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">True</span></code>, the linearly interpolating modes
(<cite>linear</cite>, <cite>bilinear</cite>, <cite>bicubic</cite>, and <cite>trilinear</cite>) don’t proportionally
align the output and input pixels, and thus the output values can depend
on the input size. This was the default behavior for these modes up to
version 0.3.1. Since then, the default behavior is
<code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">False</span></code>. See below for concrete examples on how this
affects the outputs.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want downsampling/general resizing, you should use <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code>.</p>
</div>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"nearest"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> 
<span class="go">tensor([[[[1., 1., 2., 2.],</span>
<span class="go">          ...</span>
<span class="go">          [3., 3., 4., 4.]]]], device='cuda:0', dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Upsample.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.Upsample.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.UpsamplingBilinear2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">UpsamplingBilinear2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_factor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.UpsamplingBilinear2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D bilinear upsampling to an input signal composed of several input
channels.</p>
<p>To specify the scale, it takes either the <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code>
as it’s constructor argument.</p>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> is given, it is the output size of the image <cite>(h, w)</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>int</em><em> or </em><em>Tuple</em><em>[</em><em>int</em><em>, </em><em>int</em><em>]</em><em>, </em><em>optional</em>) – output spatial sizes</p></li>
<li><p><strong>scale_factor</strong> (<em>float</em><em> or </em><em>Tuple</em><em>[</em><em>float</em><em>, </em><em>float</em><em>]</em><em>, </em><em>optional</em>) – multiplier for
spatial size.</p></li>
</ul>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This class is deprecated in favor of <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code>. It is
equivalent to <code class="docutils literal notranslate"><span class="pre">nn.functional.interpolate(...,</span> <span class="pre">mode='bilinear',</span> <span class="pre">align_corners=True)</span></code>.</p>
</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> where</p></li>
</ul>
</dd>
</dl>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor H_{in} \times \text{scale_factor} \right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor W_{in} \times \text{scale_factor} \right\rfloor\]</div></div>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">UpsamplingBilinear2d</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> 
<span class="go">tensor([[[[1.0000, 1.3333, 1.6667, 2.0000],</span>
<span class="go">          ...</span>
<span class="go">          [3.0000, 3.3333, 3.6667, 4.0000]]]], device='cuda:0',</span>
<span class="go">       dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.UpsamplingNearest2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">UpsamplingNearest2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_factor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.UpsamplingNearest2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D nearest neighbor upsampling to an input signal composed of several input
channels.</p>
<p>To specify the scale, it takes either the <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code>
as it’s constructor argument.</p>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> is given, it is the output size of the image <cite>(h, w)</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>int</em><em> or </em><em>Tuple</em><em>[</em><em>int</em><em>, </em><em>int</em><em>]</em><em>, </em><em>optional</em>) – output spatial sizes</p></li>
<li><p><strong>scale_factor</strong> (<em>float</em><em> or </em><em>Tuple</em><em>[</em><em>float</em><em>, </em><em>float</em><em>]</em><em>, </em><em>optional</em>) – multiplier for
spatial size.</p></li>
</ul>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This class is deprecated in favor of <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code>.</p>
</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> where</p></li>
</ul>
</dd>
</dl>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor H_{in} \times \text{scale_factor} \right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor W_{in} \times \text{scale_factor} \right\rfloor\]</div></div>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">UpsamplingNearest2d</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> 
<span class="go">tensor([[[[1., 1., 2., 2.],</span>
<span class="go">          ...</span>
<span class="go">          [3., 3., 4., 4.]]]], device='cuda:0', dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ZeroPad2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ZeroPad2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">tuple</span><span class="p"><span class="pre">,</span> </span><span class="pre">list</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ZeroPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>The interface is consistent with PyTorch.
The documentation is referenced from:
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad2d.html">https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad2d.html</a></p>
<p>Pads the input tensor boundaries with zero. User can set the amount of padding by setting the parameter <cite>paddings</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>padding</strong> (<em>Union</em><em>[</em><em>int</em><em>, </em><em>tuple</em><em>]</em>) – the size of the padding. If is <cite>int</cite>, uses the same padding in all boundaries. If a 4-<cite>tuple</cite>, uses (<span class="math notranslate nohighlight">\(\mathrm{padding_{left}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{right}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{top}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{bottom}}\)</span>)</p>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> where</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(H_{out} = H_{in} + \mathrm{padding_{top}} + \mathrm{padding_{bottom}}\)</span></p>
<p><span class="math notranslate nohighlight">\(W_{out} = W_{in} + \mathrm{padding_{left}} + \mathrm{padding_{right}}\)</span></p>
</div></blockquote>
</li>
</ul>
</dd>
</dl>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m1</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ZeroPad2d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m2</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ZeroPad2d</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">18</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m1</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="go">oneflow.Size([1, 2, 7, 7])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([[[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  1.,  2.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  3.,  4.,  5.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  6.,  7.,  8.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]],</span>

<span class="go">         [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  9., 10., 11.,  0.,  0.],</span>
<span class="go">          [ 0.,  0., 12., 13., 14.,  0.,  0.],</span>
<span class="go">          [ 0.,  0., 15., 16., 17.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]]]], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m2</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([[[[ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  1.,  2.,  0.,  0.],</span>
<span class="go">          [ 0.,  3.,  4.,  5.,  0.,  0.],</span>
<span class="go">          [ 0.,  6.,  7.,  8.,  0.,  0.]],</span>

<span class="go">         [[ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  9., 10., 11.,  0.,  0.],</span>
<span class="go">          [ 0., 12., 13., 14.,  0.,  0.],</span>
<span class="go">          [ 0., 15., 16., 17.,  0.,  0.]]]], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py function">
<dt id="oneflow.nn.modules.pixelshuffle.PixelShufflev2">
<code class="sig-prename descclassname"><span class="pre">oneflow.nn.modules.pixelshuffle.</span></code><code class="sig-name descname"><span class="pre">PixelShufflev2</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">upscale_factor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h_upscale_factor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">w_upscale_factor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.modules.pixelshuffle.PixelShufflev2" title="Permalink to this definition">¶</a></dt>
<dd><p>Part of the documentation is referenced from:
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle">https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle</a></p>
<p>Rearranges elements in a tensor of shape <span class="math notranslate nohighlight">\((*, C \times r_h \times r_w, H, W)\)</span>
to a tensor of shape <span class="math notranslate nohighlight">\((*, C, H \times r_h, W \times r_w)\)</span>, where r_h and r_w are upscale factors.</p>
<p>This is useful for implementing efficient sub-pixel convolution
with a stride of <span class="math notranslate nohighlight">\(1/r\)</span>.</p>
<p>See the paper:
<a class="reference external" href="https://arxiv.org/abs/1609.05158">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</a>
by Shi et. al (2016) for more details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>upscale_factor</strong> (<em>int</em><em>, </em><em>optional</em>) – factor to increase spatial resolution by, only use when factors of height and width spatial are the same.</p></li>
<li><p><strong>h_upscale_factor</strong> (<em>int</em><em>, </em><em>optional</em>) – factor to increase height spatial resolution by, only one of h_upscale_factor and upscale_factor can be used.</p></li>
<li><p><strong>w_upscale_factor</strong> (<em>int</em><em>, </em><em>optional</em>) – factor to increase width spatial resolution by, only one of w_upscale_factor and upscale_factor can be used.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((*, C_{in}, H_{in}, W_{in})\)</span>, where * is zero or more batch dimensions</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((*, C_{out}, H_{out}, W_{out})\)</span>, where</p></li>
</ul>
</dd>
</dl>
<p>if use upscale_factor:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}C_{out} = C_{in} \div \text{h_upscale_factor}^2\\H_{out} = H_{in} \times \text{upscale_factor}\\W_{out} = W_{in} \times \text{upscale_factor}\end{aligned}\end{align} \]</div></div>
<p>if use h_upscale_factor and w_upscale_factor:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}C_{out} = C_{in} \div \text{h_upscale_factor} \div \text{w_upscale_factor}\\H_{out} = H_{in} \times \text{h_upscale_factor}\\W_{out} = W_{in} \times \text{w_upscale_factor}\end{aligned}\end{align} \]</div></div>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">PixelShuffle</span><span class="p">(</span><span class="n">upscale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="go">oneflow.Size([3, 1, 10, 10])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">PixelShuffle</span><span class="p">(</span><span class="n">h_upscale_factor</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">w_upscale_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="go">oneflow.Size([1, 2, 6, 8])</span>
</pre></div>
</div>
</dd></dl>
<dl class="py function">
<dt id="oneflow.nn.parallel.DistributedDataParallel">
<code class="sig-prename descclassname"><span class="pre">oneflow.nn.parallel.</span></code><code class="sig-name descname"><span class="pre">DistributedDataParallel</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">oneflow.nn.module.Module</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">broadcast_buffers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bucket_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.parallel.DistributedDataParallel" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py function">
<dt id="oneflow.nn.utils.clip_grad_norm_">
<code class="sig-prename descclassname"><span class="pre">oneflow.nn.utils.</span></code><code class="sig-name descname"><span class="pre">clip_grad_norm_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_norm</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">error_if_nonfinite</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> → <span class="pre">oneflow._oneflow_internal.Tensor</span><a class="headerlink" href="#oneflow.nn.utils.clip_grad_norm_" title="Permalink to this definition">¶</a></dt>
<dd><p>clip_grad_norm_(parameters, max_norm, norm_type=2.0, error_if_nonfinite=True) → oneflow._oneflow_internal.Tensor</p>
<p>裁剪可迭代参数的梯度范数。
范数是在所有梯度上一起计算的，就好像它们被连接成一个向量一样。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>parameters</strong> (Iterable[Tensor] 或 Tensor) - 一个可迭代的张量或一个将梯度归一化的单个张量</p></li>
<li><p><strong>max_norm</strong> (float 或 int) - 梯度的最大范数</p></li>
<li><p><strong>norm_type</strong> (float 或 int) - 使用的 p-norm 的类型。对于无穷范数，可以是  <code class="docutils literal notranslate"><span class="pre">'inf'</span></code>。</p></li>
<li><p><strong>error_if_nonfinite</strong> (bool) - 当为 True 时，如果来自 :attr:<code class="docutils literal notranslate"><span class="pre">parameters</span></code> 的梯度的总范数为 <code class="docutils literal notranslate"><span class="pre">nan</span></code> 、 <code class="docutils literal notranslate"><span class="pre">inf</span></code> 或 <code class="docutils literal notranslate"><span class="pre">-inf</span></code> 会出现 error 。默认：True</p></li>
</ul>
</dd>
<dt>返回类型：</dt><dd><p>裁剪梯度范数后的参数。
参数的总范数（视为单个向量）。</p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">3.7</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m1</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out1</span> <span class="o">=</span> <span class="n">m1</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out1</span> <span class="o">=</span> <span class="n">out1</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out1</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">norm1</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">norm1</span>
<span class="go">tensor(6., dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([[0.1000, 0.1000, 0.1000],</span>
<span class="go">        [0.1000, 0.1000, 0.1000]], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out2</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">atan</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out2</span> <span class="o">=</span> <span class="n">out2</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out2</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">norm2</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">norm2</span>
<span class="go">tensor(1.0394, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([[0.0962, 0.0481, 0.0283],</span>
<span class="go">        [0.0663, 0.4810, 0.0428]], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py function">
<dt id="oneflow.nn.utils.weight_norm">
<code class="sig-prename descclassname"><span class="pre">oneflow.nn.utils.</span></code><code class="sig-name descname"><span class="pre">weight_norm</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">T_module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'weight'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> → <span class="pre">T_module</span><a class="headerlink" href="#oneflow.nn.utils.weight_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>weight_norm(module, name=’weight’, dim=0) → T_module</p>
<p>对给定模块中的参数应用权重归一化 (Weight Normalization)。</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\mathbf{w}=g \frac{\mathbf{v}}{\|\mathbf{v}\|}\]</div></div>
<p>权重归一化 (Weight Normalization) 是一种重新参数化 (Reparameterization)，
它将权重张量的大小从其方向解耦。此操作将用两个参数替换有 <code class="xref py py-attr docutils literal notranslate"><span class="pre">name`（例如`</span></code>’weight’<code class="docutils literal notranslate"><span class="pre">）指定的参数：</span>
<span class="pre">一个指定大小（例如</span> <span class="pre">``'weight'</span></code>），另一个指定方向（例如 <code class="docutils literal notranslate"><span class="pre">'weight_v'</span></code>)。
权重归一化 (Weight Normalization) 是通过一个 hook 实现的，该 hook 在每个
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> 调用之前从大小和方向重新计算权重张量。</p>
<p>默认情况下，当 <code class="docutils literal notranslate"><span class="pre">dim=0</span></code> 时，每个输出通道/平面独立计算范数。
要计算整个权重张量的范数，请使用 <code class="docutils literal notranslate"><span class="pre">dim=None</span></code>。</p>
<p>参见 <a class="reference external" href="https://arxiv.org/abs/1602.07868">https://arxiv.org/abs/1602.07868</a></p>
<p>本文档说明参考 Pytorch 文档：
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html">https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html</a></p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>module</strong> (Module): 包含模块</p></li>
<li><p><strong>name</strong> (str, 可选的): 权重参数名称</p></li>
<li><p><strong>dim</strong> (int, 可选的): 计算范数的维度</p></li>
</ul>
</dd>
<dt>返回类型：</dt><dd><p>带有权重范数 hook 的原始模块</p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">weight_norm</span><span class="p">(</span><span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">'weight'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span>
<span class="go">Linear(in_features=20, out_features=40, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">weight_g</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([40, 1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">weight_v</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([40, 20])</span>
</pre></div>
</div>
</dd></dl>
<dl class="py function">
<dt id="oneflow.nn.utils.remove_weight_norm">
<code class="sig-prename descclassname"><span class="pre">oneflow.nn.utils.</span></code><code class="sig-name descname"><span class="pre">remove_weight_norm</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'weight'</span></span></em><span class="sig-paren">)</span> → <span class="pre">T_module</span><a class="headerlink" href="#oneflow.nn.utils.remove_weight_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>从模块中删除 Weight Normalization Reparameterization。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>module</strong> (Module): 包含模块</p></li>
<li><p><strong>name</strong> (str, 可选的): 权重参数名称</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">weight_norm</span><span class="p">(</span><span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">remove_weight_norm</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="go">Linear(in_features=20, out_features=40, bias=True)</span>
</pre></div>
</div>
</dd></dl>
</div>
</div>

      </article>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="functional.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">oneflow.nn.functional</div>
              </div>
              <svg><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="tensor_attributes.html">
              <svg><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Tensor Attributes</div>
                
              </div>
            </a>
        </div>

        <div class="related-information">
              Copyright &#169; 2020, OneFlow
            |
            Built with <a href="https://www.sphinx-doc.org/">Sphinx</a>
              and
              <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
              <a href="https://github.com/pradyunsg/furo">Furo theme</a>.
            |
            <a class="muted-link" href="_sources/nn.rst.txt"
               rel="nofollow">
              Show Source
            </a>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            Contents
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">oneflow.nn</a><ul>
<li><a class="reference internal" href="#module-oneflow.nn">Operators for neural networks</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </main>
</div>
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="_static/scripts/main.js?digest=e931d09b2a40c1bb82b542effe772014573baf67"></script></body>
</html>