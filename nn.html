<!doctype html>
<html class="no-js">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><link rel="index" title="Index" href="genindex.html" /><link rel="search" title="Search" href="search.html" /><link rel="next" title="oneflow.nn.functional" href="functional.html" /><link rel="prev" title="Tensor Attributes" href="tensor_attributes.html" />

    <meta name="generator" content="sphinx-3.5.4, furo 2021.04.11.beta34"/>
        <title>oneflow.nn - OneFlow documentation</title>
      <link rel="stylesheet" href="_static/styles/furo.css?digest=59ab60ac09ea94ccfe6deddff6d715cce948a6fc">
    <link rel="stylesheet" href="_static/pygments.css">
    <link media="(prefers-color-scheme: dark)" rel="stylesheet" href="_static/pygments_dark.css">
    


<style>
  :root {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media (prefers-color-scheme: dark) {
    :root {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
  }

  /* For allowing end-user-specific overrides */
  .override-light {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  .override-dark {
    --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
  }
</style><link rel="stylesheet" href="_static/styles/furo-extensions.css?digest=d391b54134226e4196576da3bdb6dddb7e05ba2b"></head>
  <body dir="">
    
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
      stroke-width="1.5" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z"/>
      <line x1="4" y1="6" x2="20" y2="6" />
      <line x1="10" y1="12" x2="20" y2="12" />
      <line x1="6" y1="18" x2="20" y2="18" />
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
      stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
      class="feather feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
      stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
      class="feather feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="index.html"><div class="brand">OneFlow  documentation</div></a>
    </div>
    <div class="header-right">
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="index.html">
  
  
  <span class="sidebar-brand-text">OneFlow  documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="search.html">
  <input class="sidebar-search" placeholder=Search name="q">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul>
<li class="toctree-l1"><a class="reference internal" href="troubleshooting.html">Troubleshooting</a></li>
</ul>
<p class="caption"><span class="caption-text">OneFlow Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="oneflow.html">oneflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">oneflow.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">oneflow.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="functional.html">oneflow.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">oneflow.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">oneflow.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">oneflow.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">oneflow.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">oneflow.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">oneflow.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="module.html">oneflow.nn.Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="graph.html">oneflow.nn.Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="image.html">oneflow.nn.image</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">oneflow.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="env.html">oneflow.env</a></li>
<li class="toctree-l1"><a class="reference internal" href="comm.html">oneflow.comm</a></li>
</ul>

</div>
</div>
      </div>
      
    </div>
  </aside>
  <main class="main">
    <div class="content">
      <article role="main">
        <label class="toc-overlay-icon toc-content-icon" for="__toc">
          <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
        </label>
        <div class="section" id="oneflow-nn">
<h1>oneflow.nn<a class="headerlink" href="#oneflow-nn" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-oneflow.nn">
<span id="operators-for-neural-networks"></span><h2>Operators for neural networks<a class="headerlink" href="#module-oneflow.nn" title="Permalink to this headline">¶</a></h2>
<p>Copyright 2020 The OneFlow Authors. All rights reserved.</p>
<p>Licensed under the Apache License, Version 2.0 (the “License”);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at</p>
<blockquote>
<div><p><a class="reference external" href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></p>
</div></blockquote>
<p>Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an “AS IS” BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.</p>
<dl class="py class">
<dt id="oneflow.nn.AdaptiveAvgPool1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">AdaptiveAvgPool1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.AdaptiveAvgPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个平面组成的输入信号 <cite>input</cite> 上应用 1D 自适应平均池化。</p>
<p>对于任何大小的输入，输出大小都是 H。</p>
<p>输出的数量等于输入平面的数量。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>output_size</strong> - 目标输出大小 H（单个整数）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 64, 5])</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.AdaptiveAvgPool2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">AdaptiveAvgPool2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.AdaptiveAvgPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个平面组成的的信号 <cite>input</cite> 上应用 2D 自适应平均池化。</p>
<p>对于任何大小的输入，输出大小都是 H x W 。</p>
<p>输出的数量等于输入平面的数量。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>output_size</strong> - 目标输出大小（单个整数 H 或包含两个整数的元组 <code class="docutils literal notranslate"><span class="pre">(H,</span> <span class="pre">W)</span></code> ）。 H 和 W 可以是 <code class="docutils literal notranslate"><span class="pre">int</span></code> 也可以是 <code class="docutils literal notranslate"><span class="pre">None</span></code> ，如果为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 则大小将和输入大小一致。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 64, 5, 7])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 64, 7, 7])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="bp">None</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 64, 10, 7])</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.AdaptiveAvgPool3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">AdaptiveAvgPool3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.AdaptiveAvgPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个平面组成的信号 <cite>input</cite> 上应用 3D 自适应平均池化。</p>
<p>对于任何大小的输入，输出大小都是 D x H x W 。</p>
<p>输出的数量等于输入平面的数量。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>output_size</strong> - 目标输出大小（单个整数 D 则为 D x D x D 或包含三个整数的元组 (D, H, W) ）。 H 、 W 和 D 可以是 <code class="docutils literal notranslate"><span class="pre">int</span></code> 也可以是 <code class="docutils literal notranslate"><span class="pre">None</span></code> ，如果为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 则大小将和输入大小一致。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool3d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">9</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 64, 5, 7, 9])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool3d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 64, 7, 7, 7])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool3d</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 64, 7, 9, 8])</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.AvgPool1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">AvgPool1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">count_include_pad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.AvgPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个平面组成的信号 <cite>input</cite> 上执行 1D 平均池化。
在最简单的情况下，输出值是输入大小为 <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> 的层。
输出 <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> 和 <cite>kernel_size</cite> ， <span class="math notranslate nohighlight">\(k\)</span> 可以被精确地描述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out(N_i, C_j, l)  = \frac{1}{k} \sum_{m=0}^{k-1}
                    input(N_i, C_j, stride[0] \times h + m, stride*l + m)\]</div></div>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 非零，则输入在两侧隐式填充 0 以填充点数。</p>
<p>参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 可以为 int 或者单元素元组。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>当 <code class="xref py py-attr docutils literal notranslate"><span class="pre">ceil_mode</span></code> 为 True 时，如果滑动窗口在 left padding 或输入内开始，则允许滑动窗口出界。忽略在右侧填充区域开始的滑动窗口。</p>
</div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>kernel_size</strong> (Union[int, Tuple[int, int]]): 窗口的大小</p></li>
<li><p><strong>strides</strong> (Union[int, Tuple[int, int]], 可选): 窗口的 stride 。默认值为 None</p></li>
<li><p><strong>padding</strong> (Union[int, Tuple[int, int]]): 如果非 0 ，在两侧添加隐式填充 0 。默认为 0</p></li>
<li><p><strong>ceil_mode</strong> (bool): 如果为 True ，将使用 ceil 而不是 floor 来计算输出形状。默认为 False</p></li>
<li><p><strong>count_include_pad</strong> (bool): 如果为 True ，将在平均计算中填充 0 ，默认为 True</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">AvgPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="n">oneflow</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.AvgPool1d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.AvgPool1d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.AvgPool2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">AvgPool2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">count_include_pad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">divisor_override</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.AvgPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个平面组成的信号 <cite>input</cite> 上执行 2D 平均池化。</p>
<p>在最简单的情况下，输出值是输入大小为 <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> 的层。</p>
<p>输出 <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> 和 <cite>kernel_size</cite> ， <span class="math notranslate nohighlight">\((kH, kW)\)</span> 可以被精确地描述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out(N_i, C_j, h, w)  = \frac{1}{kH * kW} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1}
                       input(N_i, C_j, stride[0] \times h + m, stride[1] \times w + n)\]</div></div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>kernel_size</strong> (Union[int, Tuple[int, int]]): 整数或长度为 1 或 2 的整数列表。输入张量的每个维度的窗口大小</p></li>
<li><p><strong>strides</strong> (Union[int, Tuple[int, int]]): 整数或长度为 1 或 2 的整数列表。输入张量的每个维度的滑动窗口的 stride 。默认为 None</p></li>
<li><p><strong>padding</strong> (Tuple[int, int]): 整数或长度为 1 或 2 的整数列表。在两侧添加隐式填充 0 。默认为 0</p></li>
<li><p><strong>ceil_mode</strong> (bool, default to False): 如果为 True 。将使用 ceil 而不是 floor 来计算输出形状。默认为 False</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="n">oneflow</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.AvgPool2d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.AvgPool2d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.AvgPool3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">AvgPool3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">count_include_pad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">divisor_override</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.AvgPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个平面组成的信号 <cite>input</cite> 上执行 3D 平均池化。在最简单的情况下，输出值是输入大小为 <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span> 的层。</p>
<p>输出 <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span> 和 <cite>kernel_size</cite> ， <span class="math notranslate nohighlight">\((kD, kH, kW)\)</span> 可以被精确地描述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out(N_i, C_j, d, h, w)  = \frac{1}{kD * kH * kW } \sum_{k=0}^{kD-1} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1}
                       input(N_i, C_j, stride[0] \times d + k, stride[1] \times h + m, stride[2] \times w + n)\]</div></div>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 非零，则输入在三侧隐式填充 0 以填充点数。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>当 <code class="xref py py-attr docutils literal notranslate"><span class="pre">ceil_mode</span></code> 为 True 时，如果滑动窗口在 left padding 或输入内开始，则允许滑动窗口出界。忽略在右侧填充区域开始的滑动窗口。</p>
</div>
<dl>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>kernel_size</strong> (Union[int, Tuple[int, int, int]]): 窗口的大小</p></li>
<li><p><strong>strides</strong> (Union[int, Tuple[int, int, int]], 可选): 窗口的 stride 。默认值为 None</p></li>
<li><p><strong>padding</strong> (Union[int, Tuple[int, int, int]]):  如果非 0 ，在三侧添加隐式填充 0 。默认为 0</p></li>
<li><p><strong>ceil_mode</strong> (bool): 如果为 True ，将使用 ceil 而不是 floor 来计算输出形状。默认为 False</p></li>
<li><p><strong>count_include_pad</strong> (bool): 如果为 True ，将在平均计算中填充 0 ，默认为 True</p></li>
<li><p><strong>divisor_override</strong> (int): 如果设定了 attr:<cite>divisor_override</cite> ，它将用作除数，否则 attr:<cite>kernel_size</cite> 将作为除数。默认为 0</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span></p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{kernel_size}[0]}{\text{stride}[0]} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{kernel_size}[1]}{\text{stride}[1]} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{kernel_size}[2]}{\text{stride}[2]} + 1\right\rfloor\]</div></div>
</li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">AvgPool3d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="n">oneflow</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">19</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.AvgPool3d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.AvgPool3d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.BCELoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">BCELoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span> → <span class="pre">Tensor</span><a class="headerlink" href="#oneflow.nn.BCELoss" title="Permalink to this definition">¶</a></dt>
<dd><p>计算二值交叉熵损失 (binary cross-entropy loss)。</p>
<p>公式为：</p>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = “none” ：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = -(Target_i*log(Input_i) + (1-Target_i)*log(1-Input_i))\]</div></div>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = “mean”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = -\frac{1}{n}\sum_{i=1}^n(Target_i*log(Input_i) + (1-Target_i)*log(1-Input_i))\]</div></div>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = “sum”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = -\sum_{i=1}^n(Target_i*log(Input_i) + (1-Target_i)*log(1-Input_i))\]</div></div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>weight</strong> (oneflow.Tensor, 可选的): 手动重新调整损失的权重。默认为 <code class="docutils literal notranslate"><span class="pre">None</span></code> ，对应的权重值为 1</p></li>
<li><p><strong>reduction</strong> (str, 可选的): 规约的方式，可以是 “none” 、 “mean” 、 “sum” 中的一种。默认为 “mean”</p></li>
</ul>
</dd>
</dl>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>输入值必须在区间 (0, 1) 内。否则此损失函数可能返回 <cite>nan</cite> 值。</p>
</div>
<dl class="simple">
<dt>返回类型：</dt><dd><p>oneflow.tensor</p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">activation</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sigmoid_input</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">sigmoid_input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[2.9266, 1.1963, 1.1087],</span>
<span class="go">        [0.8064, 2.0750, 4.2539]], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m_sum</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m_sum</span><span class="p">(</span><span class="n">sigmoid_input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(12.3668, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m_mean</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m_mean</span><span class="p">(</span><span class="n">sigmoid_input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(2.0611, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m_none</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m_none</span><span class="p">(</span><span class="n">sigmoid_input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(1.0306, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.BCEWithLogitsLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">BCEWithLogitsLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> → <span class="pre">Tensor</span><a class="headerlink" href="#oneflow.nn.BCEWithLogitsLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>此运算将 <cite>Sigmoid</cite> 和 <cite>BCELoss</cite> 组合在一起。为了数据的稳定性，我们用了一些数学技巧，而不是将 <cite>Sigmoid</cite> 作用于 <cite>BCELoss</cite> 层。</p>
<p>公式为：</p>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = <code class="docutils literal notranslate"><span class="pre">"none"</span></code>:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = -weight*[Pos\_weight*y*log\sigma({x}) + (1-y)*log(1-\sigma(x))]\]</div></div>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = <code class="docutils literal notranslate"><span class="pre">"mean"</span></code>:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = -\frac{weight}{n}\sum_{i=1}^n[Pos\_weight*y*log\sigma({x}) + (1-y)*log(1-\sigma(x))]\]</div></div>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = <code class="docutils literal notranslate"><span class="pre">"sum"</span></code>:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = -weight*\sum_{i=1}^n[Pos\_weight*y*log\sigma({x}) + (1-y)*log(1-\sigma(x))]\]</div></div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>weight</strong> (Tensor, 可选的): 手动重新调整损失的权重。默认为 <code class="docutils literal notranslate"><span class="pre">None</span></code></p></li>
<li><p><strong>reduction</strong> (str, 可选的): 规约的方式，可以是 <code class="docutils literal notranslate"><span class="pre">"none"</span></code> 、 <code class="docutils literal notranslate"><span class="pre">"mean"</span></code> 、 <code class="docutils literal notranslate"><span class="pre">"sum"</span></code> 中的一种。默认为 “mean” 。如果为 <code class="docutils literal notranslate"><span class="pre">'none'</span></code> 则不进行规约。如果为 <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> ，输出的值的和除以元素数。如果为 <code class="docutils literal notranslate"><span class="pre">'sum'</span></code> ，输出将被求和。默认为 <code class="docutils literal notranslate"><span class="pre">"mean"</span></code></p></li>
<li><p><strong>pos_weight</strong> (Tensor, 可选的): 手动重新调整正例的权重。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N,*)\)</span> 其中 <cite>*</cite> 的意思是，可以增加任意维度</p></li>
<li><p><strong>Target</strong> : <span class="math notranslate nohighlight">\((N,*)\)</span> 与输入形状一样</p></li>
<li><p><strong>Output</strong> : 标量。如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> 为 <code class="docutils literal notranslate"><span class="pre">"none"</span></code> ，则 <span class="math notranslate nohighlight">\((N,*)\)</span> 和输入形状一样</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pos_weight</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">pos_weight</span><span class="o">=</span><span class="n">pos_weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[2.9266, 1.5552, 1.1087],</span>
<span class="go">        [0.9676, 2.0750, 5.9554],</span>
<span class="go">        [0.9676, 2.0750, 5.9554]], dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">pos_weight</span><span class="o">=</span><span class="n">pos_weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(2.6207, dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">pos_weight</span><span class="o">=</span><span class="n">pos_weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(23.5865, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.BatchNorm1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">BatchNorm1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.BatchNorm1d" title="Permalink to this definition">¶</a></dt>
<dd><p>在 2D 或 3D 输入（具有可选附加通道维度的小批量 1D 输入）上应用批归一化 (Batch Normalization) 。行为与论文 <a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift</a> 一致。</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>按小批量逐维度求平均值和标准差， <span class="math notranslate nohighlight">\(\gamma\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span> 是大小为 <cite>C</cite> 的可学习参数向量（ <cite>C</cite> 是输入的大小）。
默认情况下，<span class="math notranslate nohighlight">\(\gamma\)</span> 的元素均为 1 而 <span class="math notranslate nohighlight">\(\beta\)</span> 的元素均为 0 。标准差的计算等价于 <cite>torch.var(input, unbiased=False)</cite> 。</p>
<p>此外，默认情况下，在训练期间，该层不断估计计算的均值和方差，然后评估时将其归一化。运行估计默认 <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 为 0.1 。</p>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> 被设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> ，则该层不会继续进行估计，并且在评估时也使用批处理统计信息。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 参数不同于优化器 (optimizer) 类中使用的参数或传统的动量概念。数学上，这里的更新规则是：
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span> ，其中 <span class="math notranslate nohighlight">\(\hat{x}\)</span> 是估计的统计量， <span class="math notranslate nohighlight">\(x_t\)</span> 是新的观察值。</p>
</div>
<p>因为批归一化 (Batch Normalization) 是在 <cite>C</cite> 维度上完成的，计算 <cite>(N, L)</cite> 切片的统计数据，所以常称其为 Temporal Batch Normalization 。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>num_features</strong> - <span class="math notranslate nohighlight">\(C\)</span> 来自于大小为 <span class="math notranslate nohighlight">\((N, C, L)\)</span> 的预期输入或 <span class="math notranslate nohighlight">\(L\)</span> 来自大小为 <span class="math notranslate nohighlight">\((N, L)\)</span> 的输入</p></li>
<li><p><strong>eps</strong> - 为数值稳定性而为分母加的值。默认为：1e-5</p></li>
<li><p><strong>momentum</strong> - 用于 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 计算的值。设定为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 则计算移动平均 (Moving average) ，默认：0.1</p></li>
<li><p><strong>affine</strong> - 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，该模块具有可学习的仿射参数。默认为 <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>track_running_stats</strong> - 当设置为 <code class="docutils literal notranslate"><span class="pre">True</span></code> 时，该模块跟踪运行均值和方差，当设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> 时，此模块不会跟踪此类统计信息，并将统计缓冲区 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 初始化为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 。当这些缓冲区为“无”时，此模块在训练和评估模式中始终使用批处理统计信息。默认值： <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C)\)</span> 或 <span class="math notranslate nohighlight">\((N, C, L)\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C)\)</span> 或 <span class="math notranslate nohighlight">\((N, C, L)\)</span> （与输入形状相同）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.BatchNorm2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">BatchNorm2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.BatchNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><p>在 4D 输入（具有可选附加通道维度的小批量 2D 输入）上应用批归一化 (Batch Normalization) 。行为与论文 <a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift</a> 一致。</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>按小批量逐维度求平均值和标准差， <span class="math notranslate nohighlight">\(\gamma\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span> 是大小为 <cite>C</cite> 的可学习参数向量（ <cite>C</cite> 是输入的大小）。
默认情况下，<span class="math notranslate nohighlight">\(\gamma\)</span> 的元素均为 1 而 <span class="math notranslate nohighlight">\(\beta\)</span> 的元素均为 0 。标准差的计算等价于 <cite>torch.var(input, unbiased=False)</cite> 。</p>
<p>此外，默认情况下，在训练期间，该层不断估计计算的均值和方差，然后评估时将其归一化。运行估计默认 <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 为 0.1 。</p>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> 被设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> ，则该层不会继续进行估计，并且在评估时也使用批处理统计信息。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 参数不同于优化器 (optimizer) 类中使用的参数或传统的动量概念。数学上，这里的更新规则是：
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span> ，其中 <span class="math notranslate nohighlight">\(\hat{x}\)</span> 是估计的统计量， <span class="math notranslate nohighlight">\(x_t\)</span> 是新的观察值。</p>
</div>
<p>因为批归一化 (Batch Normalization) 是在 <cite>C</cite> 维度上完成的，计算 <cite>(N, H, W)</cite> 切片的统计数据，所以常称其为 Spatial Batch Normalization 。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>num_features</strong> - <span class="math notranslate nohighlight">\(C\)</span> 来自于大小为 <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> 的预期输入</p></li>
<li><p><strong>eps</strong> - 为数值稳定性而为分母加的值。默认为：1e-5</p></li>
<li><p><strong>momentum</strong> - 用于 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 计算的值。设定为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 则计算移动平均 (Moving average) ，默认：0.1</p></li>
<li><p><strong>affine</strong> - 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，该模块具有可学习的仿射参数。默认为 <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>track_running_stats</strong> - 当设置为 <code class="docutils literal notranslate"><span class="pre">True</span></code> 时，该模块跟踪运行均值和方差，当设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> 时，此模块不会跟踪此类统计信息，并将统计缓冲区 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 初始化为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 。当这些缓冲区为“无”时，此模块在训练和评估模式中始终使用批处理统计信息。默认值： <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C, H, W)\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> （与输入形状相同）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.BatchNorm3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">BatchNorm3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.BatchNorm3d" title="Permalink to this definition">¶</a></dt>
<dd><p>在 5D 输入（具有可选附加通道维度的小批量 3D 输入）上应用批归一化 (Batch Normalization) 。行为与论文 <a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift</a> 一致。</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>按小批量逐维度求平均值和标准差， <span class="math notranslate nohighlight">\(\gamma\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span> 是大小为 <cite>C</cite> 的可学习参数向量（ <cite>C</cite> 是输入的大小）。
默认情况下，<span class="math notranslate nohighlight">\(\gamma\)</span> 的元素均为 1 而 <span class="math notranslate nohighlight">\(\beta\)</span> 的元素均为 0 。标准差的计算等价于 <cite>torch.var(input, unbiased=False)</cite> 。</p>
<p>此外，默认情况下，在训练期间，该层不断估计计算的均值和方差，然后评估时将其归一化。运行估计默认 <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 为 0.1 。</p>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> 被设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> ，则该层不会继续进行估计，并且在评估时也使用批处理统计信息。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 参数不同于优化器 (optimizer) 类中使用的参数或传统的动量概念。数学上，这里的更新规则是：
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span> ，其中 <span class="math notranslate nohighlight">\(\hat{x}\)</span> 是估计的统计量， <span class="math notranslate nohighlight">\(x_t\)</span> 是新的观察值。</p>
</div>
<p>因为批归一化 (Batch Normalization) 是在 <cite>C</cite> 维度上完成的，计算 <cite>(N, H, W)</cite> 切片的统计数据，所以常称其为 Spatial Batch Normalization 。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>num_features</strong> - <span class="math notranslate nohighlight">\(C\)</span> 来自于大小为 <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span> 的预期输入</p></li>
<li><p><strong>eps</strong> - 为数值稳定性而为分母加的值。默认为：1e-5</p></li>
<li><p><strong>momentum</strong> - 用于 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 计算的值。设定为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 则计算移动平均 (Moving average) ，默认：0.1</p></li>
<li><p><strong>affine</strong> - 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，该模块具有可学习的仿射参数。默认为 <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>track_running_stats</strong> - 当设置为 <code class="docutils literal notranslate"><span class="pre">True</span></code> 时，该模块跟踪运行均值和方差，当设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> 时，此模块不会跟踪此类统计信息，并将统计缓冲区 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 初始化为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 。当这些缓冲区为“无”时，此模块在训练和评估模式中始终使用批处理统计信息。默认值： <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span> （与输入形状相同）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([3, 2, 5, 8, 4])</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.CELU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">CELU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.CELU" title="Permalink to this definition">¶</a></dt>
<dd><p>应用逐元素方程：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\text{CELU}(x, \alpha) = \begin{cases}
                        x &amp; \text{ if } x \ge 0  \\
        \alpha*(exp(\frac{x}{\alpha})-1) &amp; \text{ otherwise } \\
            \end{cases}\end{split}\]</div></div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>alpha</strong> (float): CELU 公式中的 <span class="math notranslate nohighlight">\(\alpha\)</span> 。默认值：1.0</p></li>
<li><p><strong>inplace</strong> (bool): 是否执行 place 操作。默认： <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N,*)\)</span> 其中 <cite>*</cite> 的意思是，可以增加任意维度</p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, *)\)</span>, 与输入相同</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">celu</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CELU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">celu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([-0.3161,  0.0000,  0.5000], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.CELU.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.CELU.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.COCOReader">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">COCOReader</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">annotation_file</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_dir</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_seed</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_by_aspect_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_images_without_annotations</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride_partition</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.device</span><span class="p"><span class="pre">,</span> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">placement</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.placement</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sbp</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.sbp.sbp</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.sbp.sbp</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.COCOReader" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.CTCLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">CTCLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">blank</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zero_infinity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.CTCLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>计算 CTC(Connectionist Temporal Classification) 损失。</p>
<p>此接口与 PyTorch 一致，文档参考自： <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html#torch.nn.CTCLoss">https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html#torch.nn.CTCLoss</a> 。</p>
<p>计算连续且未分段的时间序列和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 序列之间的损失。CTCLoss 对 <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> 与 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 可能对齐的概率求和，产生一个相对于每个 <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> 节点可微的损失值。
假定 <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> 与 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 的对齐为“多对一”，这限制了 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 序列的长度，即 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> <span class="math notranslate nohighlight">\(\leq\)</span> <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>blank</strong> (int, 可选的) - 空白标签。默认值为 <span class="math notranslate nohighlight">\(0\)</span>。</p></li>
<li><p><strong>reduction</strong> (string, 可选的) - 指定应用于输出的 reduction：<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code> ：不进行 reduction；<code class="docutils literal notranslate"><span class="pre">'mean'</span></code> ：输出损失将除以目标长度，然后取该批次的平均值。默认值为： <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>。</p></li>
<li><p><strong>zero_infinity</strong> (bool, 可选的) - 是否将无限损失和相关梯度归零。默认值为：<code class="docutils literal notranslate"><span class="pre">False</span></code>。无限损失主要发生在 <code class="xref py py-attr docutils literal notranslate"><span class="pre">inputs</span></code> 太短而无法与 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 对齐时。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Log_probs</strong> : 形状为 <span class="math notranslate nohighlight">\((T, N, C)\)</span> 的张量且 <span class="math notranslate nohighlight">\(T = \text{input length}\)</span>  、 <span class="math notranslate nohighlight">\(N = \text{batch size}\)</span> 、 <span class="math notranslate nohighlight">\(C = \text{number of classes (including blank)}\)</span>。</p></li>
<li><p><strong>Targets</strong> : 形状为 <span class="math notranslate nohighlight">\((N, S)\)</span> 或 <span class="math notranslate nohighlight">\((\operatorname{sum}(\text{target_lengths}))\)</span> 的张量，其中 <span class="math notranslate nohighlight">\(N = \text{batch size}\)</span> 、 <span class="math notranslate nohighlight">\(S = \text{max target length, if shape is } (N, S)\)</span>。
它代表 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 序列。 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 序列中的每个元素都是一个 class 索引。并且 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 索引不能为空（默认值为 0）。在 <span class="math notranslate nohighlight">\((N, S)\)</span> 形式中，<code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 被填充到最长序列的长度并堆叠。
在 <span class="math notranslate nohighlight">\((\operatorname{sum}(\text{target_lengths}))\)</span> 形式中，我们假定目标在 1 维内未填充和连接。</p></li>
<li><p><strong>Input_lengths</strong> : 大小为 <span class="math notranslate nohighlight">\((N)\)</span> 的元组或张量，其中 <span class="math notranslate nohighlight">\(N = \text{batch size}\)</span>。它表示 <code class="xref py py-attr docutils literal notranslate"><span class="pre">inputs</span></code> 的长度（每个都必须 <span class="math notranslate nohighlight">\(\leq T\)</span>）。假定序列被填充为相等长度，为每个序列指定长度以实现掩码。</p></li>
<li><p><strong>Target_lengths</strong> : 大小为 <span class="math notranslate nohighlight">\((N)\)</span> 的元组或张量，其中 <span class="math notranslate nohighlight">\(N = \text{batch size}\)</span>。它代表 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 的长度。若假定序列被填充为相等长度，为每个序列指定长度以实现掩码。若 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 形状是 <span class="math notranslate nohighlight">\((N,S)\)</span>，
则 target_lengths 是每个目标序列的有效停止索引 <span class="math notranslate nohighlight">\(s_n\)</span> ，这样每个目标序列都满足 <code class="docutils literal notranslate"><span class="pre">target_n</span> <span class="pre">=</span> <span class="pre">targets[n,0:s_n]</span></code> ，长度都必须 <span class="math notranslate nohighlight">\(\leq S\)</span>。
若目标是作为单个目标的串联的 1d 张量给出的，则 target_lengths 必须加起来为张量的总长度。</p></li>
</ul>
</dd>
<dt>参考文献：</dt><dd><p>A. Graves et al.: Connectionist Temporal Classification:
Labelling Unsegmented Sequence Data with Recurrent Neural Networks:
<a class="reference external" href="https://www.cs.toronto.edu/~graves/icml_2006.pdf">https://www.cs.toronto.edu/~graves/icml_2006.pdf</a></p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">log_probs</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>   <span class="p">[</span>
<span class="gp">... </span>       <span class="p">[[</span><span class="o">-</span><span class="mf">1.1031</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7998</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5200</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.9808</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1363</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1908</span><span class="p">]],</span>
<span class="gp">... </span>       <span class="p">[[</span><span class="o">-</span><span class="mf">1.2258</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0665</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0153</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.1135</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2331</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9671</span><span class="p">]],</span>
<span class="gp">... </span>       <span class="p">[[</span><span class="o">-</span><span class="mf">1.3348</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6611</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5118</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.9823</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2355</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0941</span><span class="p">]],</span>
<span class="gp">... </span>       <span class="p">[[</span><span class="o">-</span><span class="mf">1.3850</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3273</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7247</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.8235</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4783</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0994</span><span class="p">]],</span>
<span class="gp">... </span>       <span class="p">[[</span><span class="o">-</span><span class="mf">0.9049</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8867</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6962</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.4938</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3630</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6547</span><span class="p">]],</span>
<span class="gp">... </span>   <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">targets</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_lengths</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_lengths</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_mean</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CTCLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">loss_mean</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(1.1376, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_sum</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CTCLoss</span><span class="p">(</span><span class="n">blank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">loss_sum</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(6.8257, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.CoinFlip">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">CoinFlip</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_seed</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">probability</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.device</span><span class="p"><span class="pre">,</span> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">placement</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.placement</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sbp</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.sbp.sbp</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.sbp.sbp</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.CoinFlip" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.CombinedMarginLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">CombinedMarginLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">m1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">m2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">m3</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.CombinedMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>以下操作在 InsightFace 中实现了 <code class="docutils literal notranslate"><span class="pre">margin_softmax</span></code>：
<a class="reference external" href="https://github.com/deepinsight/insightface/blob/master/recognition/arcface_mxnet/train.py">https://github.com/deepinsight/insightface/blob/master/recognition/arcface_mxnet/train.py</a>
InsightFace 中 margin_softmax 的实现是由多个算子组成的。
我们将它们组合在一起以加快速度。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>input</strong> (oneflow.Tensor) - 输入张量。</p></li>
<li><p><strong>label</strong> (oneflow.Tensor) - 数据类型为整数的标签。</p></li>
<li><p><strong>m1</strong> (float) - 损失参数 m1。</p></li>
<li><p><strong>m2</strong> (float) - 损失参数 m2。</p></li>
<li><p><strong>m3</strong> (float) - 损失参数 m3。</p></li>
</ul>
</dd>
<dt>返回类型：</dt><dd><p>oneflow.tensor</p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.7027179</span><span class="p">,</span> <span class="mf">0.0230609</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.02721931</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.16056311</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.4565852</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.64471215</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_func</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CombinedMarginLoss</span><span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[-0.0423,  0.0231],</span>
<span class="go">        [-0.0272,  0.1237],</span>
<span class="go">        [-0.4566, -0.0204]], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ConstantPad1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ConstantPad1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">padding</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ConstantPad1d" title="Permalink to this definition">¶</a></dt>
<dd><p>用常数值填充输入张量的边界。此接口与 PyTorch 一致，文档参考自： <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad1d.html">https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad1d.html</a> 。</p>
<p>用 <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code> 来进行 <cite>N</cite> 维填充。</p>
<dl>
<dt>参数:</dt><dd><ul class="simple">
<li><p><strong>padding</strong> (int, list, tuple) - 填充的大小。若数据类型为 <cite>int</cite> 则在两个边界中使用相同的填充。若是 2-<cite>tuple</cite> ，则 (<span class="math notranslate nohighlight">\(\text{padding_left}\)</span>, <span class="math notranslate nohighlight">\(\text{padding_right}\)</span>)。</p></li>
<li><p><strong>value</strong> (int, float) - 用于填充的常量值。默认值为 0。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul>
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C, W_{in})\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C, W_{out})\)</span> ，其中</p>
<p><span class="math notranslate nohighlight">\(W_{out} = W_{in} + \text{padding_left} + \text{padding_right}\)</span></p>
</li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad1d</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">value</span><span class="o">=</span><span class="mf">9.9999</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([[[9.9999, 0.0000, 1.0000, 9.9999, 9.9999],</span>
<span class="go">         [9.9999, 2.0000, 3.0000, 9.9999, 9.9999]],</span>

<span class="go">        [[9.9999, 4.0000, 5.0000, 9.9999, 9.9999],</span>
<span class="go">         [9.9999, 6.0000, 7.0000, 9.9999, 9.9999]]], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ConstantPad2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ConstantPad2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">padding</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ConstantPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>此接口与 PyTorch 一致，文档参考自： <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad2d.html">https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad2d.html</a> 。</p>
<p>用 0 填充输入张量边界。用户可以通过设置参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">paddings</span></code> 来设置填充量。</p>
<dl>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>padding</strong> (int 或 tuple) - 填充的大小。若是 <cite>int</cite>，则在所有边界中使用相同的填充。若是 4-<cite>tuple</cite> ，则(<span class="math notranslate nohighlight">\(\mathrm{padding_{left}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{right}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{top}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{bottom}}\)</span>)。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul>
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> ，其中</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(H_{out} = H_{in} + \text{padding_top} + \text{padding_bottom}\)</span></p>
<p><span class="math notranslate nohighlight">\(W_{out} = W_{in} + \text{padding_left} + \text{padding_right}\)</span></p>
</div></blockquote>
</li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m1</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ZeroPad2d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m2</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ZeroPad2d</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m1</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="go">oneflow.Size([1, 2, 7, 7])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([[[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  1.,  2.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  3.,  4.,  5.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  6.,  7.,  8.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]],</span>

<span class="go">         [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  9., 10., 11.,  0.,  0.],</span>
<span class="go">          [ 0.,  0., 12., 13., 14.,  0.,  0.],</span>
<span class="go">          [ 0.,  0., 15., 16., 17.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]]]], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m2</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([[[[ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  1.,  2.,  0.,  0.],</span>
<span class="go">          [ 0.,  3.,  4.,  5.,  0.,  0.],</span>
<span class="go">          [ 0.,  6.,  7.,  8.,  0.,  0.]],</span>

<span class="go">         [[ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  9., 10., 11.,  0.,  0.],</span>
<span class="go">          [ 0., 12., 13., 14.,  0.,  0.],</span>
<span class="go">          [ 0., 15., 16., 17.,  0.,  0.]]]], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ConstantPad3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ConstantPad3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">padding</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ConstantPad3d" title="Permalink to this definition">¶</a></dt>
<dd><p>用常数填充输入张量的边界。此接口与 PyTorch 一致，文档参考自： <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad3d.html">https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad3d.html</a> 。</p>
<p>用 <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code> 来进行 <cite>N</cite> 维填充。</p>
<dl>
<dt>参数:</dt><dd><ul class="simple">
<li><p><strong>padding</strong> (int, list, tuple) - 填充的大小。若数据类型为 <cite>int</cite> 则在所有边界中使用相同的填充。若是 6-<cite>tuple</cite> ，则 ( <span class="math notranslate nohighlight">\(\text{padding_left}\)</span> , <span class="math notranslate nohighlight">\(\text{padding_right}\)</span> , <span class="math notranslate nohighlight">\(\text{padding_top}\)</span> , <span class="math notranslate nohighlight">\(\text{padding_bottom}\)</span> , <span class="math notranslate nohighlight">\(\text{padding_front}\)</span> , <span class="math notranslate nohighlight">\(\text{padding_back}\)</span> )。</p></li>
<li><p><strong>value</strong> (int, float) - 用于填充的常量值。默认值为 0。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span> ，其中</p>
<p><span class="math notranslate nohighlight">\(D_{out} = D_{in} + \text{padding_front} + \text{padding_back}\)</span></p>
<p><span class="math notranslate nohighlight">\(H_{out} = H_{in} + \text{padding_top} + \text{padding_bottom}\)</span></p>
<p><span class="math notranslate nohighlight">\(W_{out} = W_{in} + \text{padding_left} + \text{padding_right}\)</span></p>
</li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad3d</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([[[[[9, 9, 9, 9],</span>
<span class="go">           [9, 9, 9, 9],</span>
<span class="go">           [9, 9, 9, 9],</span>
<span class="go">           [9, 9, 9, 9]],</span>

<span class="go">          [[9, 9, 9, 9],</span>
<span class="go">           [9, 0, 1, 9],</span>
<span class="go">           [9, 2, 3, 9],</span>
<span class="go">           [9, 9, 9, 9]],</span>

<span class="go">          [[9, 9, 9, 9],</span>
<span class="go">           [9, 4, 5, 9],</span>
<span class="go">           [9, 6, 7, 9],</span>
<span class="go">           [9, 9, 9, 9]],</span>

<span class="go">          [[9, 9, 9, 9],</span>
<span class="go">           [9, 9, 9, 9],</span>
<span class="go">           [9, 9, 9, 9],</span>
<span class="go">           [9, 9, 9, 9]]]]], dtype=oneflow.int32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Conv1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Conv1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Conv1d" title="Permalink to this definition">¶</a></dt>
<dd><p>此接口与 PyTorch 一致，文档参考自： <a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Conv1d.html#conv1d">https://pytorch.org/docs/master/generated/torch.nn.Conv1d.html#conv1d</a> 。</p>
<p>对由多个平面组成的输入信号应用 1D 卷积。</p>
<p>在最简单的情况下，大小为 <span class="math notranslate nohighlight">\((N, C_{\text{in}}, L)\)</span> 的输入层的输出值和输出 <span class="math notranslate nohighlight">\((N, C_{\text{out}}, L_{\text{out}})\)</span> 可以被准确的表述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +
\sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{\text{out}_j}, k)
\star \text{input}(N_i, k)\]</div></div>
<p>其中 <span class="math notranslate nohighlight">\(\star\)</span> 为有效的 <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> 算子， <span class="math notranslate nohighlight">\(N\)</span> 是批量大小， <span class="math notranslate nohighlight">\(C\)</span> 表示通道数，
<span class="math notranslate nohighlight">\(L\)</span> 是信号序列的长度。</p>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> 是控制互相关 (cross-correlation) 的步幅 (stride) 的单个数字或单元素元组。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 控制应用于输入的填充量。可以是 <cite>string</cite> {{‘valid’, ‘same’}}
或一个给出在两侧的隐式填充量的整数元组。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 控制核心点 (kernel points) 之间的间距，也称为 <cite>à trous algorithm</cite>。这个 <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">链接</a> 中有 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 的可视化展示。</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">padding='valid'</span></code> 等同于无填充。 <code class="docutils literal notranslate"><span class="pre">padding='same'</span></code> 填充输入，使输出具有与输入相同的形状。
但是在这种情况下，不支持除了 1 以外的任何步幅 (stride) 值。</p>
</div>
<dl>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>in_channels</strong> (int) - 输入图像的通道数。</p></li>
<li><p><strong>out_channels</strong> (int) - 卷积产生的通道数。</p></li>
<li><p><strong>kernel_size</strong> (int 或者 tuple) - 卷积核的大小。</p></li>
<li><p><strong>stride</strong> (int 或者 tuple, 可选的) - 卷积的步幅 (stride)。默认值为： 1。</p></li>
<li><p><strong>padding</strong> (int, tuple 或者 str, 可选的) - 添加到输入两侧的填充值。默认值为： 0。</p></li>
<li><p><strong>padding_mode</strong> (string, 可选的) - 默认值为： <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>。</p></li>
<li><p><strong>dilation</strong> (int 或者 tuple, 可选的) - 核心的元素之间的间距。默认值为： 1。</p></li>
<li><p><strong>groups</strong> (int, 可选的) - 从输入通道到输出通道的 <cite>blocked connections</cite> 数。默认值为：1。</p></li>
<li><p><strong>bias</strong> (bool, 可选的) - 若为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则向输出添加可学习的偏差。默认值为： <code class="docutils literal notranslate"><span class="pre">True</span></code>。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul>
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C_{in}, L_{in})\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C_{out}, L_{out})\)</span> ，其中</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[L_{out} = \left\lfloor\frac{L_{in} + 2 \times \text{padding} - \text{dilation}
          \times (\text{kernel_size} - 1) - 1}{\text{stride}} + 1\right\rfloor\]</div></div>
</li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="oneflow.nn.Conv1d.weight">
<code class="sig-name descname"><span class="pre">weight</span></code><a class="headerlink" href="#oneflow.nn.Conv1d.weight" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 <span class="math notranslate nohighlight">\((\text{out_channels}, \frac{\text{in_channels}}{\text{groups}}, \text{kernel_size})\)</span> 的模块的可学习权重。这些权重的值是由公式 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 计算而来，其中 <span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{in} * \text{kernel_size}}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.Conv1d.bias">
<code class="sig-name descname"><span class="pre">bias</span></code><a class="headerlink" href="#oneflow.nn.Conv1d.bias" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 (out_channels) 的模块的可学习偏置。若 <a class="reference internal" href="#oneflow.nn.Conv1d.bias" title="oneflow.nn.Conv1d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> 为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则那么这些权重的值是由公式 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 计算而来，其中 <span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{in} * \text{kernel_size}}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Conv1d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Conv1d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
<dl class="py method">
<dt id="oneflow.nn.Conv1d.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.Conv1d.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Conv2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Conv2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Conv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>此接口与 PyTorch 一致，文档参考自： <a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html#conv2d">https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html#conv2d</a> 。</p>
<p>对由多个平面组成的输入信号应用 2D 卷积。</p>
<p>在最简单的情况下，大小为 <span class="math notranslate nohighlight">\((N, C_{\text{in}}, H, W)\)</span> 的输入层的输出值和输出
<span class="math notranslate nohighlight">\((N, C_{\text{out}}, H_{\text{out}}, W_{\text{out}})\)</span> 可以被准确的表述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +
\sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)\]</div></div>
<p>其中 <span class="math notranslate nohighlight">\(\star\)</span> 为有效的 2D <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> 算子， <span class="math notranslate nohighlight">\(N\)</span> 是批量大小， <span class="math notranslate nohighlight">\(C\)</span> 表示通道数，
<span class="math notranslate nohighlight">\(H\)</span> 是以像素为单位的输入平面的高度，和 <span class="math notranslate nohighlight">\(W\)</span> 是以像素为单位的宽度。</p>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> 是控制互相关 (cross-correlation) 的步幅 (stride) 的单个数字或单元素元组。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 控制在输入每个维度两侧隐式填充 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 个点。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 控制核心点 (kernel points) 之间的间距，也称为 <cite>à trous algorithm</cite>。这个 <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">链接</a> 中有 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 的可视化展示。</p></li>
<li><dl class="simple">
<dt><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> 控制输入和输出之间的连接。 <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> 都必须能被 <code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> 整除。例如：</dt><dd><ul>
<li><p>当 groups=1 时，所有输入都卷积到输出。</p></li>
<li><p>当 groups=2 时，该操作等效于并排放置两个 conv 层，其中每个层检查一半的输入通道并产生一半的输出通道，然后将两者连接起来。</p></li>
<li><p>当 groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> 时， 每个输入通道都与它自己的一组过滤器(大小为
<span class="math notranslate nohighlight">\(\frac{\text{out_channels}}{\text{in_channels}}\)</span>)进行卷积。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<dl class="simple">
<dt>参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 可以是：</dt><dd><ul class="simple">
<li><p>单个 <code class="docutils literal notranslate"><span class="pre">int</span></code> – 在这种情况下，高度和宽度使用相同的值。</p></li>
<li><p>一个由两个 int 组成的 <code class="docutils literal notranslate"><span class="pre">tuple</span></code> – 在这种情况下，第一个 <cite>int</cite> 用于高度，第二个 <cite>int</cite> 用于宽度。</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>当 <cite>groups == in_channels</cite> 并且 <cite>out_channels == K * in_channels</cite> 时，其中 <cite>K</cite> 是一个正整数，这个操作被称为“深度卷积”。</p>
<p>换句话说，对于大小为 <span class="math notranslate nohighlight">\((N, C_{in}, L_{in})\)</span> 的输入，可以使用参数 <span class="math notranslate nohighlight">\((C_\text{in}=C_\text{in}, C_\text{out}=C_\text{in} \times \text{K}, ..., \text{groups}=C_\text{in})\)</span>
执行具有深度乘数 <cite>K</cite> 的深度卷积。</p>
</div>
<dl>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>in_channels</strong> (int) - 输入图像的通道数。</p></li>
<li><p><strong>out_channels</strong> (int) - 卷积产生的通道数。</p></li>
<li><p><strong>kernel_size</strong> (int 或者 tuple) - 卷积核的大小。</p></li>
<li><p><strong>stride</strong> (int 或者 tuple, 可选的) - 卷积的步幅 (stride)。默认值为：1。</p></li>
<li><p><strong>padding</strong> (int, tuple 或者 str, 可选的) - 添加到输入两侧的填充值。默认值为：0。</p></li>
<li><p><strong>padding_mode</strong> (string, 可选的) - 默认值为： <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>。</p></li>
<li><p><strong>dilation</strong> (int 或者 tuple, 可选的) - 核心的元素之间的间距。默认值为：1。</p></li>
<li><p><strong>groups</strong> (int, 可选的) - 从输入通道到输出通道的 <cite>blocked connections</cite> 数。默认值为：1。</p></li>
<li><p><strong>bias</strong> (bool, 可选的) - 若为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则向输出添加可学习的偏差。默认值为：<code class="docutils literal notranslate"><span class="pre">True</span></code>。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul>
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span> ，其中</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] - \text{dilation}[0]
          \times (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] - \text{dilation}[1]
          \times (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor\]</div></div>
</li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="oneflow.nn.Conv2d.weight">
<code class="sig-name descname"><span class="pre">weight</span></code><a class="headerlink" href="#oneflow.nn.Conv2d.weight" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 <span class="math notranslate nohighlight">\((\text{out_channels}, \frac{\text{in_channels}}{\text{groups}},\text{kernel_size[0]}, \text{kernel_size[1]})\)</span> 的模块的可学习权重。这些权重的值是由公式 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 计算而来，其中 <span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{in} * \prod_{i=0}^{1}\text{kernel_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.Conv2d.bias">
<code class="sig-name descname"><span class="pre">bias</span></code><a class="headerlink" href="#oneflow.nn.Conv2d.bias" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 (out_channels) 的模块的可学习偏置。若 <a class="reference internal" href="#oneflow.nn.Conv2d.bias" title="oneflow.nn.Conv2d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> 为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则那么这些权重的值是由公式 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 计算而来，其中 <span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{in} * \prod_{i=0}^{1}\text{kernel_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Conv2d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Conv2d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
<dl class="py method">
<dt id="oneflow.nn.Conv2d.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.Conv2d.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Conv3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Conv3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Conv3d" title="Permalink to this definition">¶</a></dt>
<dd><p>此接口与 PyTorch 一致，文档参考自： <a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Conv3d.html#conv3">https://pytorch.org/docs/master/generated/torch.nn.Conv3d.html#conv3</a> 。</p>
<p>对由多个平面组成的输入信号应用 3D 卷积。</p>
<p>在最简单的情况下，大小为 <span class="math notranslate nohighlight">\((N, C_{in}, D, H, W)\)</span> 的输入层的输出值和输出。
<span class="math notranslate nohighlight">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> 可以被准确的表述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out(N_i, C_{out_j}) = bias(C_{out_j}) +
                        \sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \star input(N_i, k)\]</div></div>
<p>其中 <span class="math notranslate nohighlight">\(\star\)</span> 为有效的 3D <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> 算子。</p>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> 是控制互相关 (cross-correlation) 的步幅 (stride) 的单个数字或单元素元组。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 控制应用于输入的填充量。可以是 <cite>string</cite> {{‘valid’, ‘same’}}
或一个给出在两侧的隐式填充量的整数元组。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 控制核心点 (kernel points) 之间的间距，也称为 <cite>à trous algorithm</cite>。这个 <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">链接</a> 中有 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 的可视化展示。</p></li>
</ul>
<p>参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 可以是：</p>
<blockquote>
<div><ul class="simple">
<li><p>单个 <code class="docutils literal notranslate"><span class="pre">int</span></code> – 在这种情况下，长度、宽度和高度使用相同的值</p></li>
<li><p>一个由两个 int 组成的 <code class="docutils literal notranslate"><span class="pre">tuple</span></code> – 在这种情况下，第一个 <cite>int</cite> 用于长度，第二个 <cite>int</cite> 用于高度，第三个 <cite>int</cite> 用于宽度。</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">padding='valid'</span></code> 等同于无填充。 <code class="docutils literal notranslate"><span class="pre">padding='same'</span></code> 填充输入，使输出具有与输入相同的形状。
但是在这种情况下，不支持除了 1 以外的任何步幅 (stride) 值。</p>
</div>
<dl>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>in_channels</strong> (int) - 输入图像的通道数。</p></li>
<li><p><strong>out_channels</strong> (int) - 卷积产生的通道数。</p></li>
<li><p><strong>kernel_size</strong> (int 或者 tuple) - 卷积核的大小。</p></li>
<li><p><strong>stride</strong> (int 或者 tuple, 可选的) - 卷积的步幅 (stride)。默认值为：1。</p></li>
<li><p><strong>padding</strong> (int, tuple 或者 str, 可选的) - 添加到输入两侧的填充值。默认值为：0。</p></li>
<li><p><strong>padding_mode</strong> (string, 可选的) - 默认值为： <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>。</p></li>
<li><p><strong>dilation</strong> (int 或者 tuple, 可选的) - 核心的元素之间的间距。默认值为：1。</p></li>
<li><p><strong>groups</strong> (int, 可选的) - 从输入通道到输出通道的 <cite>blocked connections</cite> 数。默认值为：1。</p></li>
<li><p><strong>bias</strong> (bool, 可选的) - 若为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则向输出添加可学习的偏差。默认值为：<code class="docutils literal notranslate"><span class="pre">True</span></code>。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul>
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> ，其中</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0]
      \times (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1]
      \times (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2]
      \times (\text{kernel_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor\]</div></div>
</li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="oneflow.nn.Conv3d.weight">
<code class="sig-name descname"><span class="pre">weight</span></code><a class="headerlink" href="#oneflow.nn.Conv3d.weight" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 <span class="math notranslate nohighlight">\((\text{out_channels}, \frac{\text{in_channels}}{\text{groups}},\text{kernel_size[0]}, \text{kernel_size[1]}, \text{kernel_size[2]})\)</span> 的模块的可学习权重。这些权重的值是由公式 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 计算而来，其中 <span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.Conv3d.bias">
<code class="sig-name descname"><span class="pre">bias</span></code><a class="headerlink" href="#oneflow.nn.Conv3d.bias" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 (out_channels) 的模块的可学习偏置。若 <a class="reference internal" href="#oneflow.nn.Conv3d.bias" title="oneflow.nn.Conv3d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> 为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则那么这些权重的值是由公式 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 计算而来，其中 <span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel_size}[i]}\)</span> 。</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Conv3d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Conv3d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
<dl class="py method">
<dt id="oneflow.nn.Conv3d.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.Conv3d.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ConvTranspose1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ConvTranspose1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ConvTranspose1d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个输入平面组成的输入图像上应用 1D 转置卷积算子。</p>
<p>该 module 可以看作是 Conv1d 相对于其输入的梯度。它也称为分数步幅卷积或反卷积（尽管它实际上不是反卷积操作）。此 module 支持 TensorFloat32。</p>
<dl class="simple">
<dt>其中：</dt><dd><ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> 控制互相关 (cross-correlation) 的步幅 (stride)。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 控制应用于输入两侧，点的数量为 <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> 的隐式 0 填充。更多细节请参考 <code class="docutils literal notranslate"><span class="pre">note</span></code>。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code>  控制添加到输出形状一侧的大小。更多信息请参考 <code class="docutils literal notranslate"><span class="pre">note</span></code>。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 控制核心点 (kernel points) 之间的间距，也称为 <cite>à trous algorithm</cite>。这个 <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">链接</a> 中有 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 的可视化展示。</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 参数有效地将 <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> 个 0 填充到输入的两侧。
设定此项的目的是当 <code class="xref py py-class docutils literal notranslate"><span class="pre">Conv1d</span></code> 与 <code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose1d</span></code> 用相同的参数初始化时，
它们的输入和输出的形状是互逆的。然而，当 <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">&gt;</span> <span class="pre">1</span></code> 时， <code class="xref py py-class docutils literal notranslate"><span class="pre">Conv1d</span></code>
将多个输入形状映射到相同的输出形状。则使用 <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> 有效地增加一侧的输出形状来解决这种歧义。
请注意，<code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> 仅用于查找输出形状，但实际上并未填充输出。</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>在某些情况下，将 CUDA 后端与 CuDNN 一起使用时，此算子可能会选择非确定性算法来提高性能。
若此操作有不确定性，可以尝试通过设置 <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code> 来使操作具有确定性（可能以性能为代价）。
背景请参阅有关随机性 (randomness)  的 notes。</p>
</div>
<dl>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>in_channels</strong> (int) - 输入图像的通道数。</p></li>
<li><p><strong>out_channels</strong> (int) - 卷积产生的通道数。</p></li>
<li><p><strong>kernel_size</strong> (int 或 tuple) - 卷积核的大小。</p></li>
<li><p><strong>stride</strong> (int 或 tuple, 可选的) - 卷积的步幅 (stride)。默认值为：1。</p></li>
<li><p><strong>padding</strong> (int 或 tuple, 可选的) - 添加到输入每侧的 <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> 大小的 0 填充值。默认值为：0。</p></li>
<li><p><strong>output_padding</strong> (int 或 tuple, 可选的) - 添加到输出形状一侧的大小。默认值为：0。</p></li>
<li><p><strong>groups</strong> (int, 可选的) - 从输入通道到输出通道的 <cite>blocked connections</cite> 数。默认值为：1。</p></li>
<li><p><strong>bias</strong> (bool, 可选的) - 若为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则向输出添加可学习的偏差。默认值为：<code class="docutils literal notranslate"><span class="pre">True</span></code>。</p></li>
<li><p><strong>dilation</strong> (int 或 tuple, 可选的) - 核心的元素之间的间距。默认值为：1。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul>
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C_{in}, L_{in})\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C_{out}, L_{out})\)</span> ，其中</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[L_{out} = (L_{in} - 1) \times \text{stride} - 2 \times \text{padding} + \text{dilation}
          \times (\text{kernel_size} - 1) + \text{output_padding} + 1\]</div></div>
</li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="oneflow.nn.ConvTranspose1d.weight">
<code class="sig-name descname"><span class="pre">weight</span></code><a class="headerlink" href="#oneflow.nn.ConvTranspose1d.weight" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 <span class="math notranslate nohighlight">\((\text{in_channels}, \frac{\text{out_channels}}{\text{groups}}, \text{kernel_size})\)</span> 的模块的可学习参数。这些参数的值从 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 中采样，其中 <span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{out} * \text{kernel_size}}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.ConvTranspose1d.bias">
<code class="sig-name descname"><span class="pre">bias</span></code><a class="headerlink" href="#oneflow.nn.ConvTranspose1d.bias" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 (out_channels) 的模块的可学习偏置。如果 <a class="reference internal" href="#oneflow.nn.ConvTranspose1d.bias" title="oneflow.nn.ConvTranspose1d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> 为 <code class="docutils literal notranslate"><span class="pre">True</span></code>，那么这些值从 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 中取样，其中 <span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{out} * \text{kernel_size}}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="oneflow.nn.ConvTranspose1d.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.ConvTranspose1d.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ConvTranspose2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ConvTranspose2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ConvTranspose2d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个输入平面组成的输入图像上应用 2D 转置卷积算子。</p>
<p>该 module 可以看作是 Conv2d 相对于其输入的梯度。它也称为分数步幅卷积或反卷积（尽管它实际上不是反卷积操作）。</p>
<dl>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>in_channels</strong> (int) - 输入图像的通道数。</p></li>
<li><p><strong>out_channels</strong> (int) - 卷积产生的通道数。</p></li>
<li><p><strong>kernel_size</strong> (int 或 tuple) - 卷积核的大小。</p></li>
<li><p><strong>stride</strong> (int 或 tuple, 可选的) - 卷积的步幅 (stride)。默认值为：1。</p></li>
<li><p><strong>padding</strong> (int 或 tuple, 可选的) - 添加到输入每侧的 <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> 大小的 0 填充值。默认值为：0。</p></li>
<li><p><strong>output_padding</strong> (int 或 tuple, 可选的) - 添加到输出形状一侧的大小。默认值为：0。</p></li>
<li><p><strong>groups</strong> (int, 可选的) - 从输入通道到输出通道的 <cite>blocked connections</cite> 数。默认值为：1。</p></li>
<li><p><strong>bias</strong> (bool, 可选的) - 若为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则向输出添加可学习的偏差。默认值为：<code class="docutils literal notranslate"><span class="pre">True</span></code>。</p></li>
<li><p><strong>dilation</strong> (int 或 tuple, 可选的) - 核心的元素之间的间距。默认值为：1。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span> ，其中</p></li>
</ul>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]\\          \times (\text{kernel_size}[0] - 1) + \text{output_padding}[0] + 1\end{aligned}\end{align} \]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}W_{out} = (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]\\          \times (\text{kernel_size}[1] - 1) + \text{output_padding}[1] + 1\end{aligned}\end{align} \]</div></div>
</dd>
</dl>
<dl class="py attribute">
<dt id="oneflow.nn.ConvTranspose2d.weight">
<code class="sig-name descname"><span class="pre">weight</span></code><a class="headerlink" href="#oneflow.nn.ConvTranspose2d.weight" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 <span class="math notranslate nohighlight">\((\text{in_channels}, \frac{\text{out_channels}}{\text{groups}},\)</span> <span class="math notranslate nohighlight">\(\text{kernel_size[0]}, \text{kernel_size[1]})\)</span> 的模块的可学习权重。这些权重的值是由公式 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 计算而来，其中 <span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{out} * \prod_{i=0}^{1}\text{kernel_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.ConvTranspose2d.bias">
<code class="sig-name descname"><span class="pre">bias</span></code><a class="headerlink" href="#oneflow.nn.ConvTranspose2d.bias" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 (out_channels) 的模块的可学习偏置。若 <a class="reference internal" href="#oneflow.nn.ConvTranspose2d.bias" title="oneflow.nn.ConvTranspose2d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> 为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则那么这些权重的值是由公式 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 计算而来，其中 <span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{out} * \text{kernel_size}}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([20, 33, 93, 100])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.ConvTranspose2d.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.ConvTranspose2d.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ConvTranspose3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ConvTranspose3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ConvTranspose3d" title="Permalink to this definition">¶</a></dt>
<dd><p>在由多个输入平面组成的输入图像上应用 3D 转置卷积算子。</p>
<p>转置卷积算子将每个输入值逐元素乘以一个可学习的内核 (kernel) ，并对所有输入特征平面的输出求和。</p>
<p>该 module 可以看作是 Conv3d 相对于其输入的梯度。它也称为分数步幅卷积或反卷积（尽管它实际上不是反卷积操作）。</p>
<p>此 module 支持 TensorFloat32。</p>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> 控制互相关 (cross-correlation) 的步幅 (stride)。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 控制应用于输入两侧，点的数量为 <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> 的隐式 0 填充。
更多细节请参考 <code class="docutils literal notranslate"><span class="pre">note</span></code>。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code>  控制添加到输出形状一侧的大小。更多信息请参考 <code class="docutils literal notranslate"><span class="pre">note</span></code>。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 控制核心点 (kernel points) 之间的间距，也称为 <cite>à trous algorithm</cite>。这个 <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">链接</a> 中有 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 的可视化展示。</p></li>
</ul>
<p>参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 、 <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> 可以是以下形式：</p>
<blockquote>
<div><ul class="simple">
<li><p>单个 <code class="docutils literal notranslate"><span class="pre">int</span></code> – 在这种情况下，长度、高度和宽度的大小使用相同的值。</p></li>
<li><p>一个由三个 int 组成的 <code class="docutils literal notranslate"><span class="pre">tuple</span></code> – 在这种情况下，第一个 <cite>int</cite> 用于长度，第二个 <cite>int</cite> 表示高度，第三个 <cite>int</cite> 表示宽度。</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 参数有效地将 <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> 个 0 填充到输入的两侧。
设定此项的目的是当 <code class="xref py py-class docutils literal notranslate"><span class="pre">Conv3d</span></code> 与 <code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose3d</span></code> 用相同的参数初始化时，
它们的输入和输出的形状是互逆的。然而，当 <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">&gt;</span> <span class="pre">1</span></code> 时， <code class="xref py py-class docutils literal notranslate"><span class="pre">Conv3d</span></code>
将多个输入形状映射到相同的输出形状。则使用 <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> 有效地增加一侧的输出形状来解决这种歧义。
请注意，<code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> 仅用于查找输出形状，但实际上并未填充输出。</p>
</div>
<dl>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>in_channels</strong> (int) - 输入图像的通道数。</p></li>
<li><p><strong>out_channels</strong> (int) - 卷积产生的通道数。</p></li>
<li><p><strong>kernel_size</strong> (int 或 tuple) - 卷积核的大小。</p></li>
<li><p><strong>stride</strong> (int 或 tuple, 可选的) - 卷积的步幅 (stride)。默认值为：1。</p></li>
<li><p><strong>padding</strong> (int 或 tuple, 可选的) - 添加到输入每侧的 <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> 大小的 0 填充值。默认值为：0。</p></li>
<li><p><strong>output_padding</strong> (int 或 tuple, 可选的) - 添加到输出形状一侧的大小。默认值为：0。</p></li>
<li><p><strong>groups</strong> (int, 可选的) - 从输入通道到输出通道的 <cite>blocked connections</cite> 数。默认值为：1。</p></li>
<li><p><strong>bias</strong> (bool, 可选的) - 若为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则向输出添加可学习的偏差。默认值为：<code class="docutils literal notranslate"><span class="pre">True</span></code>。</p></li>
<li><p><strong>dilation</strong> (int 或 tuple, 可选的) - 核心的元素之间的间距。默认值为：1。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> ，其中</p></li>
</ul>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[D_{out} = (D_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]
          \times (\text{kernel_size}[0] - 1) + \text{output_padding}[0] + 1\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = (H_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]
          \times (\text{kernel_size}[1] - 1) + \text{output_padding}[1] + 1\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = (W_{in} - 1) \times \text{stride}[2] - 2 \times \text{padding}[2] + \text{dilation}[2]
          \times (\text{kernel_size}[2] - 1) + \text{output_padding}[2] + 1\]</div></div>
</dd>
</dl>
<dl class="py attribute">
<dt id="oneflow.nn.ConvTranspose3d.weight">
<code class="sig-name descname"><span class="pre">weight</span></code><a class="headerlink" href="#oneflow.nn.ConvTranspose3d.weight" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 <span class="math notranslate nohighlight">\((\text{in_channels}, \frac{\text{out_channels}}{\text{groups}},\)</span> <span class="math notranslate nohighlight">\(\text{kernel_size[0]}, \text{kernel_size[1]}, \text{kernel_size[2]})\)</span> 的模块的可学习权重。这些权重的值是由公式 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 计算而来，其中 <span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{out} * \prod_{i=0}^{2}\text{kernel_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.ConvTranspose3d.bias">
<code class="sig-name descname"><span class="pre">bias</span></code><a class="headerlink" href="#oneflow.nn.ConvTranspose3d.bias" title="Permalink to this definition">¶</a></dt>
<dd><p>形状为 (out_channels) 的模块的可学习偏置。若 <a class="reference internal" href="#oneflow.nn.ConvTranspose3d.bias" title="oneflow.nn.ConvTranspose3d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> 为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则那么这些权重的值是由公式 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 计算而来，其中 <span class="math notranslate nohighlight">\(k = \frac{groups}{C_\text{out} * \text{kernel_size}}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="tensor.html#oneflow.Tensor" title="oneflow.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.ConvTranspose3d.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.ConvTranspose3d.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.CropMirrorNormalize">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">CropMirrorNormalize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">color_space</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'BGR'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_layout</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'NCHW'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crop_h</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crop_w</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crop_pos_y</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crop_pos_x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">[0.0]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">[1.0]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dtype</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">oneflow._oneflow_internal.dtype</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">oneflow.float32</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.CropMirrorNormalize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.CrossEntropyLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">CrossEntropyLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.CrossEntropyLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>将类 <code class="xref py py-class docutils literal notranslate"><span class="pre">LogSoftmax</span></code> 和 <code class="xref py py-class docutils literal notranslate"><span class="pre">NLLLoss</span></code> 组合在一起。</p>
<p>该类在使用 <cite>C</cite> 类训练分类问题时很有效。</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> 应包含每个类的原始的，非标准化分数。</p>
<p>在 <cite>K</cite> 维下， <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> 的大小必须为 <span class="math notranslate nohighlight">\((minibatch, C)\)</span> 或 <span class="math notranslate nohighlight">\((minibatch, C, d_1, d_2, ..., d_K)\)</span> ，
其中 <span class="math notranslate nohighlight">\(K \geq 1\)</span> （见下文）。</p>
<p>在此标准中，类的索引应在 <span class="math notranslate nohighlight">\([0, C-1]\)</span> 范围内并引作为大小为 <cite>minibatch</cite> 的一维张量的 <cite>target</cite> ；</p>
<p>该损失可以被描述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right)
               = -x[class] + \log\left(\sum_j \exp(x[j])\right)\]</div></div>
<p>通过提供大小为 <span class="math notranslate nohighlight">\((minibatch, C, d_1, d_2, ..., d_K)\)</span> 的输入和适当形状的目标（其中 <span class="math notranslate nohighlight">\(K \geq 1\)</span> ， <span class="math notranslate nohighlight">\(K\)</span> 是维度数），
此类也可用于更高维度的输入，例如 2D 图像（见下文）。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>reduction</strong> (string, 可选的) - 指定应用于输出的 reduction（可以是 <code class="docutils literal notranslate"><span class="pre">'none'</span></code> 、 <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> 、 <code class="docutils literal notranslate"><span class="pre">'sum'</span></code> ，默认值为 <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>）。其中 <code class="docutils literal notranslate"><span class="pre">'none'</span></code> :不进行简化；<code class="docutils literal notranslate"><span class="pre">'mean'</span></code> :取输出的加权平均值；<code class="docutils literal notranslate"><span class="pre">'sum'</span></code> :取输出的和。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>   <span class="p">[[</span><span class="o">-</span><span class="mf">0.1664078</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7256707</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.14690138</span><span class="p">],</span>
<span class="gp">... </span>       <span class="p">[</span><span class="o">-</span><span class="mf">0.21474946</span><span class="p">,</span> <span class="mf">0.53737473</span><span class="p">,</span> <span class="mf">0.99684894</span><span class="p">],</span>
<span class="gp">... </span>       <span class="p">[</span><span class="o">-</span><span class="mf">1.135804</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.50371903</span><span class="p">,</span> <span class="mf">0.7645404</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.8020, 1.1167, 0.3583], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_sum</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_sum</span>
<span class="go">tensor(2.2769, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_mean</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_mean</span>
<span class="go">tensor(0.7590, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Dropout">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Dropout</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="pre">p:=0.5</span></em>, <em class="sig-param"><span class="pre">inplace=False</span></em>, <em class="sig-param"><span class="pre">generator=None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>在训练期间，使用来自伯努利分布 (Bernoulli distribution) 的样本，以概率 <a class="reference internal" href="#oneflow.nn.Dropout.p" title="oneflow.nn.Dropout.p"><code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code></a> 随机将输入张量的一些元素归零。
在每次 forward call 时，所有的通道都将独立归零。</p>
<p>如 “Improving neural networks by preventing co-adaptation of feature detectors” 所述，此方法已被证明可以有效用于正则化 (regularization) 和防止神经元协同适应 (co-adaptation of neurons) 。</p>
<p>此外，在训练期间，输出按因数 <span class="math notranslate nohighlight">\(\frac{1}{1-p}\)</span> 进行缩放。这意味着在评估过程中只计算一个恒等函数。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>p</strong> (float): 元素归零的概率。默认：0.5</p></li>
<li><p><strong>inplace</strong> (bool): 是否执行 in-place 操作。默认为： <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((*)\)</span> 输入可以是任何形状</p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((*)\)</span> 输出与输入形状相同</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.7797</span><span class="p">,</span> <span class="mf">0.2264</span><span class="p">,</span> <span class="mf">0.2458</span><span class="p">,</span> <span class="mf">0.4163</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4299</span><span class="p">,</span> <span class="mf">0.3626</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4892</span><span class="p">,</span> <span class="mf">0.4141</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.4115</span><span class="p">,</span> <span class="mf">1.2183</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5503</span><span class="p">,</span> <span class="mf">0.6520</span><span class="p">]],</span><span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> 
<span class="go">tensor([[-0.7797,  0.2264,  0.2458,  0.4163],</span>
<span class="go">        [ 0.4299,  0.3626, -0.4892,  0.4141],</span>
<span class="go">        [-1.4115,  1.2183, -0.5503,  0.6520]], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py attribute">
<dt id="oneflow.nn.Dropout.inplace">
<code class="sig-name descname"><span class="pre">inplace</span></code><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#oneflow.nn.Dropout.inplace" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.Dropout.p">
<code class="sig-name descname"><span class="pre">p</span></code><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><a class="headerlink" href="#oneflow.nn.Dropout.p" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ELU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ELU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ELU" title="Permalink to this definition">¶</a></dt>
<dd><p>应用以下逐元素公式：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\text{ELU}(x) = \begin{cases}
                        x &amp; \text{ if } x \gt 0  \\
        \alpha*(exp(x)-1) &amp; \text{ if } x \le 0 \\
            \end{cases}\end{split}\]</div></div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>alpha</strong> : ELU 公式的 <span class="math notranslate nohighlight">\(\alpha\)</span> 值。默认：1.0</p></li>
<li><p><strong>inplace</strong> : 是否执行 in-place 操作。默认： <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, *)\)</span> 其中 <cite>*</cite> 表示任意数量的额外维度</p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, *)\)</span> 形状与输入一致</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">elu</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">elu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([-0.3935,  0.0000,  0.5000], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.ELU.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ELU.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Embedding">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Embedding</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_embeddings</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_grad_by_freq</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sparse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Embedding" title="Permalink to this definition">¶</a></dt>
<dd><p>一个简单的查找表，用于存储固定字典和大小的嵌入。</p>
<p>该模块通常用于储存词嵌入并索引它们。该模块的输入是索引列表，输出是相应的词嵌入。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>num_embeddings</strong> (int): 嵌入字典的大小</p></li>
<li><p><strong>embedding_dim</strong> (int): 每个嵌入向量的大小</p></li>
<li><p><strong>padding_idx</strong> (int, 可选的): 如果设定了此参数，则 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_idx</span></code> 处的元素不会影响梯度；因此，在训练期间不会更新 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_idx</span></code> 的嵌入向量，它仍然是一个固定的 <cite>pad</cite> 。对于新构建的嵌入， <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_idx</span></code> 处的嵌入向量将默认为全零，但可以更新为另一个值以用作填充向量。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Embedding.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.Embedding.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.FakeQuantization">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">FakeQuantization</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">quantization_formula</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'google'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_bit</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_scheme</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'symmetric'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.FakeQuantization" title="Permalink to this definition">¶</a></dt>
<dd><p>在训练时间内模拟量化 (quantize) 和反量化 (dequantize) 操作。输出将被计算为：</p>
<blockquote>
<div><p>若 quantization_scheme == “symmetric”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; quant\_max = 2^{quantization\_to\_bit - 1} - 1\\&amp; quant\_min = -quant\_max\\&amp; clamp(round(x / scale), quant\_min, quant\_max) * scale\end{aligned}\end{align} \]</div></div>
<p>若 quantization_scheme == “affine”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; quant\_max = 2^{quantization\_to\_bit} - 1\\&amp; quant\_min = 0\\&amp; (clamp(round(x / scale + zero\_point), quant\_min, quant\_max) - zero\_point) * scale\end{aligned}\end{align} \]</div></div>
</div></blockquote>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>quantization_bit</strong> (int): 量化输入为 uintX / intX ， X 可以在范围 [2, 8] 中。默认为 8</p></li>
<li><p><strong>quantization_scheme</strong> (str): “symmetric” 或 “affine” ， 量化为有符号/无符号整数。 默认为 “symmetric”</p></li>
<li><p><strong>quantization_formula</strong> (str): 支持 “google” 或 “cambricon”</p></li>
</ul>
</dd>
<dt>返回类型：</dt><dd><p>oneflow.Tensor: 量化和反量化操作后的输入张量</p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_bit</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_scheme</span> <span class="o">=</span> <span class="s2">"symmetric"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_formula</span> <span class="o">=</span> <span class="s2">"google"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">per_layer_quantization</span> <span class="o">=</span> <span class="bp">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">min_max_observer</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MinMaxObserver</span><span class="p">(</span><span class="n">quantization_formula</span><span class="o">=</span><span class="n">quantization_formula</span><span class="p">,</span> <span class="n">quantization_bit</span><span class="o">=</span><span class="n">quantization_bit</span><span class="p">,</span>
<span class="gp">... </span><span class="n">quantization_scheme</span><span class="o">=</span><span class="n">quantization_scheme</span><span class="p">,</span> <span class="n">per_layer_quantization</span><span class="o">=</span><span class="n">per_layer_quantization</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fake_quantization</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">FakeQuantization</span><span class="p">(</span><span class="n">quantization_formula</span><span class="o">=</span><span class="n">quantization_formula</span><span class="p">,</span> <span class="n">quantization_bit</span><span class="o">=</span><span class="n">quantization_bit</span><span class="p">,</span>
<span class="gp">... </span><span class="n">quantization_scheme</span><span class="o">=</span><span class="n">quantization_scheme</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> <span class="n">min_max_observer</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">input_tensor</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">output_tensor</span> <span class="o">=</span> <span class="n">fake_quantization</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">input_tensor</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">scale</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">zero_point</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Flatten">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Flatten</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">start_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">end_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Flatten" title="Permalink to this definition">¶</a></dt>
<dd><p>将 tensor 指定连续范围的维度展平。用于：nn.Sequential 。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>start_dim</strong> (int): 展平开始的维度（默认为 1）</p></li>
<li><p><strong>end_dim</strong> (int): 展平结束的维度（默认为 -1）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="go">oneflow.Size([32, 25])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Flatten.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.Flatten.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.FusedBatchNorm1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">FusedBatchNorm1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.FusedBatchNorm1d" title="Permalink to this definition">¶</a></dt>
<dd><p>在 2D 或 3D 输入上应用 Fused Batch Normalization ，公式为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = ReLU(BatchNorm(input) + addend)\]</div></div>
<p>Batch Normalization 的公式为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>逐维度小批量计算均值和标准差，其中 <span class="math notranslate nohighlight">\(\gamma\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span>  是大小为 <cite>C</cite> （ <cite>C</cite> 是输入大小）
的可学习参数向量。默认情况下， <span class="math notranslate nohighlight">\(\gamma\)</span> 的元素被设置为 1 ，并且 <span class="math notranslate nohighlight">\(\beta\)</span> 被设置为 0 。通过有偏估计器
(biased estimator) 计算标准差，相当于 <cite>torch.var(input, unbiased=False)</cite> 。</p>
<p>默认情况下，该层在训练期间不断估计计算的均值和方差，然后在评估期间用于归一化（normalization）。
运行估计期间， <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 为 0.1。</p>
<p>如果将 <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> 设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> ，则该层不会继续进行估计，
而是在评估期间也使用批处理统计信息。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 与优化器 (optimizer) 中使用的参数和传统的动量 (momentum) 概念都不同。
在数学上，这里运行统计的更新规则是 <span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span> ，
其中 <span class="math notranslate nohighlight">\(\hat{x}\)</span> 是估计的统计量，<span class="math notranslate nohighlight">\(x_t\)</span> 是新的观察值。</p>
</div>
<p>因为 Batch Normalization 是在维度 <cite>C</cite> 上完成的，并在 <cite>(N, L)</cite> 切片上计算统计数据，所以学术上普遍称之为 Temporal Batch Normalization 。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>num_features</strong> : 来自大小为 <span class="math notranslate nohighlight">\((N, C, L)\)</span> 的预期输入的 <span class="math notranslate nohighlight">\(C\)</span> 或者来自大小为 <span class="math notranslate nohighlight">\((N, L)\)</span> 的输入的 <span class="math notranslate nohighlight">\(L\)</span></p></li>
<li><p><strong>eps</strong> : 为数值稳定性而添加到分母的值。默认： 1e-5</p></li>
<li><p><strong>momentum</strong> : 用于 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 计算的值。可以设置为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 以计算累积移动平均。默认： 0.1</p></li>
<li><p><strong>affine</strong> (bool, 可选): 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则该模块具有可学习的仿射参数。默认： <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>track_running_stats</strong> (bool, 可选): 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则此模块跟踪运行均值和方差。如果为 <code class="docutils literal notranslate"><span class="pre">False</span></code> ，此模块不跟踪此类统计信息，同时初始化统计缓冲区 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 。当这些缓冲区为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 时， 此模块在训练和评估模式中始终使用 batch statistics 。默认： <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C)\)</span> 或 <span class="math notranslate nohighlight">\((N, C, L)\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C)\)</span> 或 <span class="math notranslate nohighlight">\((N, C, L)\)</span> （与输入形状相同）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span> <span class="c1"># 目前 GPU 支持 FusedBatchNorm 。</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">FusedBatchNorm1d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">addend</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.FusedBatchNorm2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">FusedBatchNorm2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.FusedBatchNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><p>在 4D 输入上应用 Fused Batch Normalization ，公式为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = ReLU(BatchNorm(input) + addend)\]</div></div>
<p>Batch Normalization 的公式为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>逐维度小批量计算均值和标准差，其中 <span class="math notranslate nohighlight">\(\gamma\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span>  是大小为 <cite>C</cite> （ <cite>C</cite> 是输入大小）
的可学习参数向量。默认情况下， <span class="math notranslate nohighlight">\(\gamma\)</span> 的元素被设置为 1 ，并且 <span class="math notranslate nohighlight">\(\beta\)</span> 被设置为 0 。通过有偏估计器
(biased estimator) 计算标准差，相当于 <cite>torch.var(input, unbiased=False)</cite> 。</p>
<p>默认情况下，该层在训练期间不断估计计算的均值和方差，然后在评估期间用于归一化（normalization）。
运行估计期间， <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 为 0.1 。</p>
<p>如果将 <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> 设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> ，则该层不会继续进行估计，
而是在评估期间也使用批处理统计信息。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 与优化器 (optimizer) 中使用的参数和传统的动量 (momentum) 概念都不同。
在数学上，这里运行统计的更新规则是 <span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span> ，
其中 <span class="math notranslate nohighlight">\(\hat{x}\)</span> 是估计的统计量，<span class="math notranslate nohighlight">\(x_t\)</span> 是新的观察值。</p>
</div>
<p>因为 Batch Normalization 是在维度 <cite>C</cite> 上完成的，并在 <cite>(N, H, W)</cite> 切片上计算统计数据，所以学术上普遍称之为 Temporal Batch Normalization 。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>num_features</strong> : 来自大小为 <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> 的预期输入的 <span class="math notranslate nohighlight">\(C\)</span></p></li>
<li><p><strong>eps</strong> : 为数值稳定性而添加到分母的值。默认： 1e-5</p></li>
<li><p><strong>momentum</strong> : 用于 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 计算的值。可以设置为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 以计算累积移动平均。默认： 0.1</p></li>
<li><p><strong>affine</strong> (bool, 可选): 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则该模块具有可学习的仿射参数。默认： <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>track_running_stats</strong> (bool, 可选): 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则此模块跟踪运行均值和方差，如果为 <code class="docutils literal notranslate"><span class="pre">False</span></code> ，此模块不跟踪此类统计信息，同时初始化统计缓冲区 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 。当这些缓冲区为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 时， 此模块在训练和评估模式中始终使用 batch statistics 。默认： <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> （与输入形状相同）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span> <span class="c1"># 目前 GPU 支持 FusedBatchNorm 。</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">FusedBatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">addend</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.FusedBatchNorm3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">FusedBatchNorm3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.FusedBatchNorm3d" title="Permalink to this definition">¶</a></dt>
<dd><p>在 5D 输入上应用 Fused Batch Normalization ，公式为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = ReLU(BatchNorm(input) + addend)\]</div></div>
<p>Batch Normalization 的公式为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>逐维度小批量计算均值和标准差，其中 <span class="math notranslate nohighlight">\(\gamma\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span>  是大小为 <cite>C</cite> （ <cite>C</cite> 是输入大小）
的可学习参数向量。默认情况下， <span class="math notranslate nohighlight">\(\gamma\)</span> 的元素被设置为 1 ，并且 <span class="math notranslate nohighlight">\(\beta\)</span> 被设置为 0 。通过有偏估计器
(biased estimator) 计算标准差，相当于 <cite>torch.var(input, unbiased=False)</cite> 。</p>
<p>默认情况下，该层在训练期间不断估计计算的均值和方差，然后在评估期间用于归一化（normalization）。
运行估计期间， <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 为 0.1 。</p>
<p>如果将 <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> 设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> ，则该层不会继续进行估计，
而是在评估期间也使用批处理统计信息。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 与优化器 (optimizer) 中使用的参数和传统的动量 (momentum) 概念都不同。
在数学上，这里运行统计的更新规则是 <span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span> ，
其中 <span class="math notranslate nohighlight">\(\hat{x}\)</span> 是估计的统计量，<span class="math notranslate nohighlight">\(x_t\)</span> 是新的观察值。</p>
</div>
<p>因为 Batch Normalization 是在维度 <cite>C</cite> 上完成的，并在 <cite>(N, D, H, W)</cite> 切片上计算统计数据，所以学术上普遍称之为 Temporal Batch Normalization 。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>num_features</strong> : 来自大小为 <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span> 的预期输入的 <span class="math notranslate nohighlight">\(C\)</span></p></li>
<li><p><strong>eps</strong> : 为数值稳定性而添加到分母的值。默认： 1e-5</p></li>
<li><p><strong>momentum</strong> : 用于 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 计算的值。可以设置为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 以计算累积移动平均。默认： 0.1</p></li>
<li><p><strong>affine</strong> (bool, 可选): 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则该模块具有可学习的仿射参数。默认： <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>track_running_stats</strong> (bool, 可选): 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则此模块跟踪运行均值和方差。如果为 <code class="docutils literal notranslate"><span class="pre">False</span></code> ，此模块不跟踪此类统计信息，同时初始化统计缓冲区 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_mean</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">running_var</span></code> 为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 。当这些缓冲区为 <code class="docutils literal notranslate"><span class="pre">None</span></code> 时， 此模块在训练和评估模式中始终使用 batch statistics 。默认： <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span> （与输入形状相同）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span> <span class="c1"># 目前 GPU 中支持 FusedBatchNorm 。</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">FusedBatchNorm3d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">addend</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.FusedMLP">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">FusedMLP</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_features</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_features</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_features</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_final_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.FusedMLP" title="Permalink to this definition">¶</a></dt>
<dd><p>对输入数据应用带有 relu 激活的线性变换：<span class="math notranslate nohighlight">\(y = ReLU(xA^T + b)\)</span></p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>in_features</strong> - 每一个输入样本的大小</p></li>
<li><p><strong>hidden_features</strong> - 每一个线形层的藏层大小的元组</p></li>
<li><p><strong>out_features</strong> - 最后一层的隐藏大小</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *, H_{in})\)</span>，其中 <span class="math notranslate nohighlight">\(*\)</span> 表示任意维度且 <span class="math notranslate nohighlight">\(H_{in} = {in\_features}\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *, H_{out})\)</span></p></li>
</ul>
</dd>
<dt>属性：</dt><dd><ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">skip_final_activation</span></code>: 是否跳过最后一个隐藏层的激活，默认为 False</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">FusedMLP</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">],</span> <span class="mi">1024</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([1, 1024])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.FusedMLP.add_parameters">
<code class="sig-name descname"><span class="pre">add_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.FusedMLP.add_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="oneflow.nn.FusedMLP.bias">
<code class="sig-name descname"><span class="pre">bias</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">i</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.FusedMLP.bias" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="oneflow.nn.FusedMLP.biases">
<code class="sig-name descname"><span class="pre">biases</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.FusedMLP.biases" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="oneflow.nn.FusedMLP.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.FusedMLP.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
<dl class="py method">
<dt id="oneflow.nn.FusedMLP.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.FusedMLP.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="oneflow.nn.FusedMLP.weight">
<code class="sig-name descname"><span class="pre">weight</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">i</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.FusedMLP.weight" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="oneflow.nn.FusedMLP.weights">
<code class="sig-name descname"><span class="pre">weights</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.FusedMLP.weights" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.GELU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">GELU</span></code><a class="headerlink" href="#oneflow.nn.GELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Gelu 激活算子。</p>
<p>公式为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = 0.5 * x * (1 + tanh(\sqrt{\frac{2}{\pi}} * (x + 0.044715x^{3})))\]</div></div>
<dl class="simple">
<dt>参数：</dt><dd><p><strong>x</strong> (oneflow.tensor): 输入张量</p>
</dd>
<dt>返回类型：</dt><dd><p>oneflow.tensor</p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gelu</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">gelu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([-0.1543,  0.0000,  0.3457], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.GLU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">GLU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.GLU" title="Permalink to this definition">¶</a></dt>
<dd><p>GLU 激活算子。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor, float): 输入张量</p></li>
<li><p><strong>dim</strong> (int, 可选的): 分割输入的维度。默认：-1</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((\ast_1, N, \ast_2)\)</span> 其中 <cite>*</cite> 表示任意数量的额外维度</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((\ast_1, M, \ast_2)\)</span> 其中 <span class="math notranslate nohighlight">\(M=N/2\)</span></p></li>
</ul>
</dd>
</dl>
<p>公式为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[GLU(input) = GLU(a, b) = a \otimes sigmoid(b)\]</div></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>其中输入沿 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> 分成 a 和 b ，⊗ 是矩阵之间的元素积。</p>
</div>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span>
<span class="go">tensor([[0.9526, 1.9640],</span>
<span class="go">        [4.9954, 5.9980]], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.GroupNorm">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">GroupNorm</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_groups</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_channels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.GroupNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>此接口与 PyTorch 对其，可参考以下文档：
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html">https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html</a></p>
<p>对小批量输入应用组归一化 (Group Normalization) 的行为按 <a class="reference external" href="https://arxiv.org/abs/1803.08494">论文</a> 中所述。</p>
<p>公式为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>输入通道被分成 <code class="xref py py-attr docutils literal notranslate"><span class="pre">num_groups</span></code> 个组，每组包含 <code class="docutils literal notranslate"><span class="pre">num_channels</span> <span class="pre">/</span> <span class="pre">num_groups</span></code> 个通道。
每个组的平均值和标准差分开计算。如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> 为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则
<span class="math notranslate nohighlight">\(\gamma\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span> 是大小为 <code class="xref py py-attr docutils literal notranslate"><span class="pre">num_channels</span></code> 的可学习逐通道仿射变换参数向量
(learnable per-channel affine transform parameter vectors)。</p>
<p>通过有偏估计器 (biased estimator) 计算标准差，相当于 <cite>torch.var(input, unbiased=False)</cite> 。</p>
<p>该层在训练和评估模式下都使用从输入计算的统计数据。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>num_groups</strong> (int): 将通道分成的组数</p></li>
<li><p><strong>num_channels</strong> (int): 输入中预期的通道数</p></li>
<li><p><strong>eps</strong> (float, 可选): 为数值稳定性而添加到分母的值。默认：1e-5</p></li>
<li><p><strong>affine</strong> (bool, 可选): 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，该模块具有可学习逐通道仿射变换参数，并初始化为 1 （对于权重）和 0（对于偏差）。默认： <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, *)\)</span> 其中 <span class="math notranslate nohighlight">\(C=\text{num_channels}\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, *)\)</span> （与输入形状相同）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 将 6 个通道分成 3 组</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 将6个通道分成6组（相当于InstanceNorm）</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 将所有 6 个通道放在一个组中（相当于 LayerNorm）</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 激活模块</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.GroupNorm.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.GroupNorm.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
<dl class="py method">
<dt id="oneflow.nn.GroupNorm.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.GroupNorm.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Hardsigmoid">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Hardsigmoid</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Hardsigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>应用逐元素公式：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\text{Hardsigmoid}(x) = \begin{cases}
    0 &amp; \text{ if } x \le -3  \\
    1 &amp; \text{ if } x \ge +3 \\
    \frac{x}{6} + \frac{1}{2} &amp; \text{ otherwise } \\
\end{cases}\end{split}\]</div></div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>inplace</strong> (bool): 是否进行 in-place 操作。默认： <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, *)\)</span> 其中 <cite>*</cite> 表示任意数量的额外维度</p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, *)\)</span>, 与输入相同的形状</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hardsigmoid</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Hardsigmoid</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">hardsigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.4167, 0.5000, 0.5833], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Hardsigmoid.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Hardsigmoid.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Hardswish">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Hardswish</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Hardswish" title="Permalink to this definition">¶</a></dt>
<dd><p>如论文 <a class="reference external" href="https://arxiv.org/abs/1905.02244">Searching for MobileNetV3</a> 中所述，逐元素应用 Hardswish 函数。</p>
<p>公式为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\text{Hardswish}(x) = \begin{cases}
    0 &amp; \text{ if } x \le -3  \\
    x &amp; \text{ if } x \ge +3 \\
    x*(x+3)/6 &amp; \text{ otherwise } \\
\end{cases}\end{split}\]</div></div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>inplace</strong> (bool, 可选): 是否执行 in-place 操作。默认： <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, *)\)</span> 其中 <cite>*</cite> 表示任意数量的额外维度</p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, *)\)</span> 与输入形状相同</p></li>
</ul>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hardswish</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Hardswish</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">hardswish</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([-0.2083,  0.0000,  0.2917], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Hardswish.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Hardswish.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Hardtanh">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Hardtanh</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">min_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Hardtanh" title="Permalink to this definition">¶</a></dt>
<dd><p>逐元素应用 HardTanh 函数。</p>
<p>公式为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\text{HardTanh}(x) = \begin{cases}
    1 &amp; \text{ if } x &gt; 1 \\
    -1 &amp; \text{ if } x &lt; -1 \\
    x &amp; \text{ otherwise } \\
\end{cases}\end{split}\]</div></div>
<p>可以使用参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">min_val</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_val</span></code> 调整线性区域的范围 <span class="math notranslate nohighlight">\([-1, 1]\)</span>  。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>min_val</strong> (float): 线性区域范围的最小值。默认：-1</p></li>
<li><p><strong>max_val</strong> (float): 线性区域范围的最大值。默认：1</p></li>
<li><p><strong>inplace</strong> (bool): 是否执行 in-place 操作。默认： <cite>False</cite></p></li>
</ul>
</dd>
</dl>
<p>关键词参数： <code class="xref py py-attr docutils literal notranslate"><span class="pre">min_value</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_value</span></code> 已被弃用，由 <code class="xref py py-attr docutils literal notranslate"><span class="pre">min_val</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_val</span></code> 替代。</p>
<dl class="simple">
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, *)\)</span> 其中 <cite>*</cite> 表示任意数量的额外维度</p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, *)\)</span> 与输入形状相同</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Hardtanh</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.2000, 0.3000, 1.0000, 1.0000], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Hardtanh.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Hardtanh.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Identity">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Identity</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Identity" title="Permalink to this definition">¶</a></dt>
<dd><p>对参数不敏感的占位符标识运算符。</p>
<dl class="simple">
<dt>示例：</dt><dd><ul class="simple">
<li><p><strong>args</strong> : 任何参数（未使用）</p></li>
<li><p><strong>kwargs</strong> : 任何关键词参数（未使用）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="c1"># output = input</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.InstanceNorm1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">InstanceNorm1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.InstanceNorm1d" title="Permalink to this definition">¶</a></dt>
<dd><p>此接口与 PyTorch 一致。可以在 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html">https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html</a> 参考相关文档。</p>
<p>将实例归一化 (Instance Normalization) 应用于 3D 输入（具有可选附加通道维度的小批量 1D 输入），行为如论文
<a class="reference external" href="https://arxiv.org/abs/1607.08022">Instance Normalization: The Missing Ingredient for Fast Stylization</a> 所述。</p>
<p>公式为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>逐维度小批量单独计算均值和标准差。
如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> 为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ， <span class="math notranslate nohighlight">\(\gamma\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span> 是大小为 <cite>C</cite> 的可学习参数向量（其中 <cite>C</cite> 是输入大小）。
标准差是通过有偏估计器 (biased estimator) 计算的，相当于 <cite>torch.var(input, unbiased=False)</cite> 。</p>
<p>默认情况下，该层在训练和评估模式下都使用从输入数据计算的实例统计信息。</p>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> 为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，在训练期间，该层会不断计算均值和方差的估计值，
然后在评估期间将其用于归一化 (normalization) 。运行期间 <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 为 0.1 。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 与优化器 (optimizer) 中使用的参数和传统的动量 (momentum) 概念都不同。
在数学上，这里运行统计的更新规则是 <span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span> ，
其中 <span class="math notranslate nohighlight">\(\hat{x}\)</span> 是估计的统计量，<span class="math notranslate nohighlight">\(x_t\)</span> 是新的观察值。</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>尽管 <a class="reference internal" href="#oneflow.nn.InstanceNorm1d" title="oneflow.nn.InstanceNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a> 与 <a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> 非常相似，但有一些细微的区别。
<a class="reference internal" href="#oneflow.nn.InstanceNorm1d" title="oneflow.nn.InstanceNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a> 应用于多维时间序列等通道数据的每个通道，但 <a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a>
通常应用于整个样本，并且经常应用于 NLP 任务。此外， <a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> 应用逐元素仿射变换，
而 <a class="reference internal" href="#oneflow.nn.InstanceNorm1d" title="oneflow.nn.InstanceNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a> 通常不应用仿射变换。</p>
</div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>num_features</strong> (int): 来自大小为 <span class="math notranslate nohighlight">\((N, C, L)\)</span> 的预期输入的 <span class="math notranslate nohighlight">\(C\)</span> 或者来自大小为 <span class="math notranslate nohighlight">\((N, L)\)</span> 的预期输入的 <span class="math notranslate nohighlight">\(L\)</span></p></li>
<li><p><strong>eps</strong> (float): 为数值稳定性而添加到分母的值。默认：1e-5</p></li>
<li><p><strong>momentum</strong> (float): 用于计算 running_mean 和 running_var 。默认：0.1</p></li>
<li><p><strong>affine</strong> (bool): 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，该模块具有可学习的仿射参数 (learnable affine parameters) ，初始化方式与批量标准化 (batch normalization) 相同。默认： <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>track_running_stats</strong> (bool): 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，该模块记录运行均值和方差，如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，该模块不记录运行均值和方差，并且始终在训练和评估模式下都使用批处理该类统计信息。默认： <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C, L)\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C, L)\)</span> （与输入相同）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 没有可学习的参数</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 有可学习的参数</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.InstanceNorm2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">InstanceNorm2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.InstanceNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><p>此接口与 PyTorch 一致。可以在 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html">https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html</a> 参考相关文档。</p>
<p>将实例归一化 (Instance Normalization) 应用于 4D 输入（具有可选附加通道维度的小批量 2D 输入），行为如论文
<a class="reference external" href="https://arxiv.org/abs/1607.08022">Instance Normalization: The Missing Ingredient for Fast Stylization</a> 所述。</p>
<p>公式为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>逐维度小批量单独计算均值和标准差。
如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> 为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ， <span class="math notranslate nohighlight">\(\gamma\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span> 是大小为 <cite>C</cite> 的可学习参数向量（其中 <cite>C</cite> 是输入大小）。
标准差是通过有偏估计器 (biased estimator) 计算的，相当于 <cite>torch.var(input, unbiased=False)</cite> 。</p>
<p>默认情况下，该层在训练和评估模式下都使用从输入数据计算的实例统计信息。</p>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> 为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，在训练期间，该层会不断计算均值和方差的估计值，
然后在评估期间将其用于归一化 (normalization) 。运行期间 <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 为 0.1 。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 与优化器 (optimizer) 中使用的参数和传统的动量 (momentum) 概念都不同。
在数学上，这里运行统计的更新规则是 <span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span> ，
其中 <span class="math notranslate nohighlight">\(\hat{x}\)</span> 是估计的统计量，<span class="math notranslate nohighlight">\(x_t\)</span> 是新的观察值。</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>尽管 <a class="reference internal" href="#oneflow.nn.InstanceNorm2d" title="oneflow.nn.InstanceNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a> 与 <a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> 非常相似，但有一些细微的区别。
<a class="reference internal" href="#oneflow.nn.InstanceNorm2d" title="oneflow.nn.InstanceNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a> 应用RGB图像等通道数据的每个通道，但 <a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a>
通常应用于整个样本，并且经常应用于 NLP 任务。此外， <a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> 应用逐元素仿射变换，
而 <a class="reference internal" href="#oneflow.nn.InstanceNorm2d" title="oneflow.nn.InstanceNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a> 通常不应用仿射变换。</p>
</div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>num_features</strong> (int): 来自大小为 <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> 的预期输入的 <span class="math notranslate nohighlight">\(C\)</span></p></li>
<li><p><strong>eps</strong> (float): 为数值稳定性而添加到分母的值。默认：1e-5</p></li>
<li><p><strong>momentum</strong> (float): 用于计算 running_mean 和 running_var 。默认：0.1</p></li>
<li><p><strong>affine</strong> (bool): 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，该模块具有可学习的仿射参数 (learnable affine parameters) ，初始化方式与批量标准化 (batch normalization) 相同。默认： <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>track_running_stats</strong> (bool): 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，该模块记录运行均值和方差，如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，该模块不记录运行均值和方差，并且始终在训练和评估模式下都使用批处理该类统计信息。默认： <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C, H, W)\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> （与输入相同）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 没有可学习的参数</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 有可学习的参数</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.InstanceNorm3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">InstanceNorm3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_running_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.InstanceNorm3d" title="Permalink to this definition">¶</a></dt>
<dd><p>此接口与 PyTorch 一致。可以在 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html">https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html</a> 参考相关文档。</p>
<p>将实例归一化 (Instance Normalization) 应用于 5D 输入（具有可选附加通道维度的小批量 3D 输入），行为如论文
<a class="reference external" href="https://arxiv.org/abs/1607.08022">Instance Normalization: The Missing Ingredient for Fast Stylization</a> 所述。</p>
<p>公式为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>逐维度小批量单独计算均值和标准差。
如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> 为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ， <span class="math notranslate nohighlight">\(\gamma\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span> 是大小为 <cite>C</cite> 的可学习参数向量（其中 <cite>C</cite> 是输入大小）。
标准差是通过有偏估计器 (biased estimator) 计算的，相当于 <cite>torch.var(input, unbiased=False)</cite> 。</p>
<p>默认情况下，该层在训练和评估模式下都使用从输入数据计算的实例统计信息。</p>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> 为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，在训练期间，该层会不断计算均值和方差的估计值，
然后在评估期间将其用于归一化 (normalization) 。运行期间 <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 为 0.1 。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> 与优化器 (optimizer) 中使用的参数和传统的动量 (momentum) 概念都不同。
在数学上，这里运行统计的更新规则是 <span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span> ，
其中 <span class="math notranslate nohighlight">\(\hat{x}\)</span> 是估计的统计量，<span class="math notranslate nohighlight">\(x_t\)</span> 是新的观察值。</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>尽管 <a class="reference internal" href="#oneflow.nn.InstanceNorm3d" title="oneflow.nn.InstanceNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a> 与 <a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> 非常相似，但有一些细微的区别。
<a class="reference internal" href="#oneflow.nn.InstanceNorm3d" title="oneflow.nn.InstanceNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a> 应用于具有RGB颜色的3D模型等通道数据的每个通道，但 <a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a>
通常应用于整个样本，并且经常应用于 NLP 任务。此外， <a class="reference internal" href="#oneflow.nn.LayerNorm" title="oneflow.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> 应用逐元素仿射变换，
而 <a class="reference internal" href="#oneflow.nn.InstanceNorm3d" title="oneflow.nn.InstanceNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a> 通常不应用仿射变换。</p>
</div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>num_features</strong> (int): 来自大小为 <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span> 的预期输入的 <span class="math notranslate nohighlight">\(C\)</span></p></li>
<li><p><strong>eps</strong> (float): 为数值稳定性而添加到分母的值。默认：1e-5</p></li>
<li><p><strong>momentum</strong> (float): 用于计算 running_mean 和 running_var 。默认：0.1</p></li>
<li><p><strong>affine</strong> (bool): 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，该模块具有可学习的仿射参数 (learnable affine parameters) ，初始化方式与批量标准化 (batch normalization) 相同。默认： <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>track_running_stats</strong> (bool): 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，该模块记录运行均值和方差，如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，该模块不记录运行均值和方差，并且始终在训练和评估模式下都使用批处理该类统计信息。默认： <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span> （与输入相同）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 没有可学习的参数</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 有可学习的参数</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.KLDivLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">KLDivLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_target</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.KLDivLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>此接口与 PyTorch 一致。文档参考自： <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html?highlight=kldivloss#torch.nn.KLDivLoss">https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html?highlight=kldivloss#torch.nn.KLDivLoss</a> 。</p>
<p>此算子测量 KL 散度。 <a class="reference external" href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler divergence</a> 可用于连续分布中的距离测量，并且在对连续输出分布的空间（离散采样）执行直接回归时通常很有效。与 <code class="xref py py-class docutils literal notranslate"><span class="pre">NLLLoss</span></code> 一样， <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> 应包含 <em>log-probabilities</em> 并且不限于 2D tensor。默认情况下，目标被解释为 <em>probabilities</em> ，但可以将其视为将 <code class="xref py py-attr docutils literal notranslate"><span class="pre">log_target</span></code> 设置为 <code class="docutils literal notranslate"><span class="pre">True</span></code> 的 <em>log-probabilities</em> 。</p>
<p>此 criterion 要求 <cite>target</cite> 、 <cite>Tensor</cite> 的形状与 <cite>input</cite> 、 <cite>Tensor</cite> 一致。未简化 （即 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> 设置为 <code class="docutils literal notranslate"><span class="pre">'none'</span></code> ） 的损失可以描述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[l(x,y) = L = \{ l_1,\dots,l_N \}, \quad
l_n = y_n \cdot \left( \log y_n - x_n \right)\]</div></div>
<p>其中索引 <span class="math notranslate nohighlight">\(N\)</span> span <code class="docutils literal notranslate"><span class="pre">input</span></code> 的所有维度，并且 <span class="math notranslate nohighlight">\(L\)</span> 具有与 <code class="docutils literal notranslate"><span class="pre">input</span></code> 相同的形状。
如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> 不为 <code class="docutils literal notranslate"><span class="pre">'none'</span></code> （默认 <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> ），则：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), &amp; \text{if reduction} = \text{`mean';} \\
    \operatorname{sum}(L),  &amp; \text{if reduction} = \text{`sum'.}
\end{cases}\end{split}\]</div></div>
<p>如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> 为默认值 <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> ，每个 minibatch 的损失在 observations 和维度上取平均值。
如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> 为 <code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> ，可以得到正确的 KL 散度，其中损失仅在批次维度上进行平均。
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code> 模式的行为将在下一个主要版本中更改为与 <code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> 相同。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><dl class="simple">
<dt><strong>reduction</strong> (string, 可选的):  指定应用于输出的简化（可以为 <code class="docutils literal notranslate"><span class="pre">'none'</span></code> 、 <code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> 、 <code class="docutils literal notranslate"><span class="pre">'sum'</span></code> 、 <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> ，默认： <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> ）：</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">'none'</span></code> ：不会进行简化</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> ：输出的总和将除以 batchsize 。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'sum'</span></code> ：将输出求和。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'mean'</span></code> ：输出和将除以输出中的元素数。</p></li>
</ul>
</dd>
</dl>
</li>
<li><p><strong>log_target</strong> (bool, 可选的): 指定是否在 log space 中传递 <cite>target</cite>。默认： <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> 时不会返回真正的 KL 散度值，请使用符合 KL 数学定义的 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = <code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> 。
在下一个主要版本中，<code class="docutils literal notranslate"><span class="pre">'mean'</span></code> 将更改为与 <code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> 相同。</p>
</div>
<dl class="simple">
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, *)\)</span> 其中 <span class="math notranslate nohighlight">\(*\)</span> 表示任意数量的额外维度</p></li>
<li><p><strong>Target</strong> : <span class="math notranslate nohighlight">\((N, *)\)</span>，与输入的形状相同</p></li>
<li><p><strong>Output</strong> : 默认为标量。如果 :attr:<code class="docutils literal notranslate"><span class="pre">reduction</span></code> 为 <code class="docutils literal notranslate"><span class="pre">'none'</span></code> ，则为 <span class="math notranslate nohighlight">\((N, *)\)</span>，形状与输入相同</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.9021705</span><span class="p">,</span> <span class="mf">0.08798598</span><span class="p">,</span> <span class="mf">1.04686249</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.22386942</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.89729659</span><span class="p">,</span> <span class="mf">0.01615712</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">,</span> <span class="n">log_target</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([ 1.3514,  0.0000, -0.0836], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">,</span> <span class="n">log_target</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(0.4226, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">,</span> <span class="n">log_target</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(5.7801, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.L1Loss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">L1Loss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.L1Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>此运算符计算 <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> 中每个元素之间的 L1 Loss 。</p>
<p>公式为：</p>
<p>如果 reduction = “none”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[output = |Target - Input|\]</div></div>
<p>如果 reduction = “mean”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[output = \frac{1}{n}\sum_{i=1}^n|Target_i - Input_i|\]</div></div>
<p>如果 reduction = “sum”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[output = \sum_{i=1}^n|Target_i - Input_i|\]</div></div>
<dl class="simple">
<dt>参数:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor): 输入张量。</p></li>
<li><p><strong>target</strong> (Tensor): 目标张量。</p></li>
<li><p><strong>reduction</strong> (str): 简化类型，可以为 <code class="docutils literal notranslate"><span class="pre">"none"</span></code> 、 <code class="docutils literal notranslate"><span class="pre">"mean"</span></code> 、 <code class="docutils literal notranslate"><span class="pre">"sum"</span></code> 。默认为 “mean” 。</p></li>
</ul>
</dd>
<dt>返回类型：</dt><dd><p>oneflow.tensor</p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">]],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[3., 3., 3.],</span>
<span class="go">        [2., 2., 2.],</span>
<span class="go">        [3., 3., 3.]], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m_mean</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m_mean</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(2.6667, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m_mean</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m_mean</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(24., dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.LayerNorm">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">LayerNorm</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">normalized_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">elementwise_affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.LayerNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>对小批量输入应用层归一化 (Layer Normalization) ，行为如论文 <a class="reference external" href="https://arxiv.org/abs/1607.06450">Layer Normalization</a> 所述。</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div></div>
<p>在特定数字维度上分别计算均值和标准差，这些维度的形状必须由:attr:<cite>normalized_shape</cite> 指定。
如果 <a class="reference internal" href="#oneflow.nn.LayerNorm.elementwise_affine" title="oneflow.nn.LayerNorm.elementwise_affine"><code class="xref py py-attr docutils literal notranslate"><span class="pre">elementwise_affine</span></code></a> 为 <cite>True</cite> ，则 <span class="math notranslate nohighlight">\(\gamma\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span> 是
参数 <a class="reference internal" href="#oneflow.nn.LayerNorm.normalized_shape" title="oneflow.nn.LayerNorm.normalized_shape"><code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized_shape</span></code></a> 的可学习仿射变换参数。标准差是通过有偏估计器 (biased estimator) 计算的。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>与批量归一化 (Batch Normalization) 和实例归一化 (Instance Normalization) 使用 <code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code>
选项为每个完整通道/平面应用标量 scale 和偏差不同，层归一化 (Layer Normalization) 使用 <a class="reference internal" href="#oneflow.nn.LayerNorm.elementwise_affine" title="oneflow.nn.LayerNorm.elementwise_affine"><code class="xref py py-attr docutils literal notranslate"><span class="pre">elementwise_affine</span></code></a>
处理每个元素的 scale 和偏差。</p>
</div>
<p>该层在训练和评估模式下都使用从输入数据计算的统计信息。</p>
<dl>
<dt>参数：</dt><dd><ul>
<li><p><strong>normalized_shape</strong> (int 或 list 或 oneflow.Size): 来自预期大小输入的输入形状</p>
<blockquote>
<div><div class="math-wrapper"><div class="math notranslate nohighlight">
\[[* \times \text{normalized_shape}[0] \times \text{normalized_shape}[1] \times \ldots \times \text{normalized_shape}[-1]]\]</div></div>
<p>如果使用单个整数，则将其视为单例列表，并且此模块将对最后一个维度进行标准化，该维度预计具有该特定大小。</p>
</div></blockquote>
</li>
<li><p><strong>eps</strong> (float, 可选的): 为数值稳定性而添加到分母的值。默认：1e-5</p></li>
<li><dl class="simple">
<dt><strong>elementwise_affine</strong> (bool, 可选的): 如果为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，该模块具有可学习的逐元素仿射参数，</dt><dd><p>并且将他们初始化为 1 （对于权重）和 0（对于偏差）。默认值： <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((N, *)\)</span></p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((N, *)\)</span> （形状与输入相同）</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[[</span><span class="o">-</span><span class="mf">0.16046895</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.03667831</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.34974465</span><span class="p">,</span> <span class="mf">0.26505867</span><span class="p">]],[[</span><span class="o">-</span><span class="mf">1.24111986</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.53806001</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.72426331</span><span class="p">,</span> <span class="mf">0.43572459</span><span class="p">]],],[[[</span><span class="o">-</span><span class="mf">0.77390957</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.42610624</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.16398858</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.35760343</span><span class="p">]],[[</span><span class="mf">1.07541728</span><span class="p">,</span> <span class="mf">0.11008703</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.26361224</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.48663723</span><span class="p">]]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> 
<span class="go">tensor([[[[ 1.0000, -1.0000],</span>
<span class="go">          [-0.9999,  0.9999]],</span>

<span class="go">         [[-1.0000,  1.0000],</span>
<span class="go">          [ 1.0000, -1.0000]]],</span>


<span class="go">        [[[-0.9998,  0.9998],</span>
<span class="go">          [ 1.0000, -1.0000]],</span>

<span class="go">         [[ 1.0000, -1.0000],</span>
<span class="go">          [ 1.0000, -1.0000]]]], dtype=oneflow.float32,</span>
<span class="go">       grad_fn=&lt;broadcast_add_backward&gt;)</span>
</pre></div>
</div>
<dl class="py attribute">
<dt id="oneflow.nn.LayerNorm.elementwise_affine">
<code class="sig-name descname"><span class="pre">elementwise_affine</span></code><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#oneflow.nn.LayerNorm.elementwise_affine" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.LayerNorm.eps">
<code class="sig-name descname"><span class="pre">eps</span></code><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><a class="headerlink" href="#oneflow.nn.LayerNorm.eps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="oneflow.nn.LayerNorm.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.LayerNorm.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.LayerNorm.normalized_shape">
<code class="sig-name descname"><span class="pre">normalized_shape</span></code><em class="property"><span class="pre">:</span> <span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="p"><span class="pre">…</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#oneflow.nn.LayerNorm.normalized_shape" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="oneflow.nn.LayerNorm.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.LayerNorm.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.LeakyReLU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">LeakyReLU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">negative_slope</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.LeakyReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>逐元素地应用如下公式：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\text{LeakyRELU}(x) = \begin{cases}
    x, &amp; \text{ if } x \geq 0 \\
    \text{negative_slope} \times x, &amp; \text{ otherwise }
\end{cases}\end{split}\]</div></div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>negative_slope</strong> - 控制负斜率的角度。默认值为 1e-2。</p></li>
<li><p><strong>inplace</strong> - 可以选择以 in-place 的方式执行操作。默认值为 <code class="docutils literal notranslate"><span class="pre">False</span></code>。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> - <span class="math notranslate nohighlight">\((N, *)\)</span> ，其中 <cite>*</cite> 表示任意数量的附加维度。</p></li>
<li><p><strong>Output</strong> - <span class="math notranslate nohighlight">\((N, *)\)</span> ，与输入的形状相同。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.2000, 0.3000, 3.0000, 4.0000], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.LeakyReLU.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.LeakyReLU.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Linear">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Linear</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_features</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_features</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Linear" title="Permalink to this definition">¶</a></dt>
<dd><p>对输入数据应用线性变换： <span class="math notranslate nohighlight">\(y = xA^T + b\)</span> 。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>in_features</strong> - 每一个输入样本的大小。</p></li>
<li><p><strong>out_features</strong> - 每一个输出样本的大小。</p></li>
<li><p><strong>bias</strong> - 若设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> ，则该层不会学习附加偏差。默认值为 <code class="docutils literal notranslate"><span class="pre">True</span></code>。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *, H_{in})\)</span> ，其中 <cite>*</cite> 表示任意数量的附加维度，且 <span class="math notranslate nohighlight">\(H_{in} = {in\_features}\)</span> 。</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *, H_{out})\)</span> ，其中除了最后一个维度之外的所有维度都与输入的形状相同，且 <span class="math notranslate nohighlight">\(H_{out} = {out\_features}\)</span> 。</p></li>
</ul>
</dd>
<dt>属性：</dt><dd><ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code>: 形状为 <span class="math notranslate nohighlight">\(({out\_features}, {in\_features})\)</span>  的模块的可学习参数。这些值通过 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 初始化，其中 <span class="math notranslate nohighlight">\(k = 1 / {in\_features}\)</span>  。</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code>: 形状为 <span class="math notranslate nohighlight">\(({out\_features})\)</span>  的模块的可学习参数。若 <code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code> = <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则这些值通过 <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> 初始化，其中 <span class="math notranslate nohighlight">\(k = 1 / {in\_features}\)</span>  。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([128, 30])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Linear.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.Linear.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
<dl class="py method">
<dt id="oneflow.nn.Linear.reset_parameters">
<code class="sig-name descname"><span class="pre">reset_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.Linear.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.LogSigmoid">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">LogSigmoid</span></code><a class="headerlink" href="#oneflow.nn.LogSigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>逐元素地应用如下公式：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{LogSigmoid}(x) = \log\left(\frac{ 1 }{ 1 + \exp(-x)}\right)\]</div></div>
<dl class="simple">
<dt>形状：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> ，其中 <cite>*</cite> 表示任意数量的附加维度。</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span> ，与输入的形状相同。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logsigmoid</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LogSigmoid</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">logsigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([-0.9741, -0.6931, -0.4741], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.LogSoftmax">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">LogSoftmax</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.LogSoftmax" title="Permalink to this definition">¶</a></dt>
<dd><p>对一个 n 维输入张量应用 LogSoftmax 公式，它可以被简化为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right) = x_i - \log({ \sum_j \exp(x_j)})\]</div></div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>dim</strong> (int) - LogSoftmax 计算的维度。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> ，其中 <cite>*</cite> 表示任意数量的附加维度。</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span> ，与输入的形状相同。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
<span class="gp">... </span>       <span class="p">[[</span> <span class="mf">0.4296</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1957</span><span class="p">,</span>  <span class="mf">2.5463</span><span class="p">],</span>
<span class="gp">... </span>       <span class="p">[</span> <span class="mf">1.2552</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5747</span><span class="p">,</span>  <span class="mf">0.6923</span><span class="p">]]</span>
<span class="gp">... </span>   <span class="p">)</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[-2.2513, -3.8766, -0.1346],</span>
<span class="go">        [-0.4877, -3.3176, -1.0506]], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.LogSoftmax.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.LogSoftmax.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.MSELoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">MSELoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.MSELoss" title="Permalink to this definition">¶</a></dt>
<dd><p>此接口与 PyTorch 一致，文档参考自： <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html?highlight=mseloss#torch.nn.MSELoss">https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html?highlight=mseloss#torch.nn.MSELoss</a> 。</p>
<p>创建一个指标来测量输入 <span class="math notranslate nohighlight">\(x\)</span> 和目标 <span class="math notranslate nohighlight">\(y\)</span> 中每个元素之间的均方误差（平方 L2 范数）。</p>
<p>未规约的损失（如将 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> 设置为 <code class="docutils literal notranslate"><span class="pre">'none'</span></code>）可被描述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = \left( x_n - y_n \right)^2,\]</div></div>
<p>其中 <span class="math notranslate nohighlight">\(N\)</span> 是每个批量的大小。如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> 不是 <code class="docutils literal notranslate"><span class="pre">'none'</span></code>
（默认值为 <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>），则有</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\ell(x, y) =
\begin{cases}
    \operatorname{mean}(L), &amp;  \text{if reduction} = \text{`mean';}\\
    \operatorname{sum}(L),  &amp;  \text{if reduction} = \text{`sum'.}
\end{cases}\end{split}\]</div></div>
<p><span class="math notranslate nohighlight">\(x\)</span> 和 <span class="math notranslate nohighlight">\(y\)</span> 是任意形状的张量，每个都有 <span class="math notranslate nohighlight">\(n\)</span> 个元素。平均运算对所有元素进行操作，除以 <span class="math notranslate nohighlight">\(n\)</span> 。如果设置 <code class="docutils literal notranslate"><span class="pre">reduction</span> <span class="pre">=</span> <span class="pre">'sum'</span></code>，则可以避免除以 <span class="math notranslate nohighlight">\(n\)</span> 的行为。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>reduction</strong> (string, optional) - 指定应用于输出的规约操作： <code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code> 。若为 <code class="docutils literal notranslate"><span class="pre">'none'</span></code> ：不进行规约；<code class="docutils literal notranslate"><span class="pre">'mean'</span></code> ：输出的和将会除以输出中的元素数量； <code class="docutils literal notranslate"><span class="pre">'sum'</span></code> ：输出将被求和。默认为 <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> ，其中 <cite>*</cite> 表示任意数量的附加维度。</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span> ，与输入的形状相同。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span><span class="p">[[</span><span class="o">-</span><span class="mf">0.02557137</span><span class="p">,</span> <span class="mf">0.03101675</span><span class="p">,</span> <span class="mf">1.37493674</span><span class="p">],</span>
<span class="gp">... </span><span class="p">[</span><span class="mf">0.25599439</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.08372561</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.21006816</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span><span class="p">[[</span><span class="o">-</span><span class="mf">1.53105064</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.68137555</span><span class="p">,</span> <span class="mf">0.5931354</span><span class="p">],</span>
<span class="gp">... </span><span class="p">[</span><span class="o">-</span><span class="mf">0.49158347</span><span class="p">,</span> <span class="mf">0.93673637</span><span class="p">,</span> <span class="mf">0.1324141</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[2.2665, 0.5075, 0.6112],</span>
<span class="go">        [0.5589, 4.0823, 0.1173]], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(1.3573, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(8.1436, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.MarginRankingLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">MarginRankingLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">margin</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.MarginRankingLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>根据给定的输入 <span class="math notranslate nohighlight">\(x1\)</span>, <span class="math notranslate nohighlight">\(x2\)</span> ，两个一维小批量 <cite>Tensors</cite> ，以及一个带标签的一维小批量张量 <span class="math notranslate nohighlight">\(y\)</span> （包含 1 或 -1），创建一个指标来测量损失。</p>
<p>若 <span class="math notranslate nohighlight">\(y = 1\)</span> 则假定第一个输入的 rank 比第二个输入更高， <span class="math notranslate nohighlight">\(y = -1\)</span> 反之亦然。</p>
<p>小批量中每个样本的损失函数为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{loss}(x1, x2, y) = \max(0, -y * (x1 - x2) + \text{margin})\]</div></div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>margin</strong> (float, optional) - 默认为 <span class="math notranslate nohighlight">\(0\)</span>。</p></li>
<li><p><strong>reduction</strong> (string, optional) - 指定应用于输出的规约操作： <code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code> 。若为 <code class="docutils literal notranslate"><span class="pre">'none'</span></code> ：不进行规约；<code class="docutils literal notranslate"><span class="pre">'mean'</span></code> ：输出的和将会除以输出中的元素数量； <code class="docutils literal notranslate"><span class="pre">'sum'</span></code> ：输出将被求和。默认为 <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p><cite>x1</cite> : <span class="math notranslate nohighlight">\((N, D)\)</span> ，其中 <cite>N</cite> 是批量大小， <cite>D</cite> 是样本大小。</p></li>
<li><p><cite>x2</cite> : <span class="math notranslate nohighlight">\((N, D)\)</span> ，其中 <cite>N</cite> 是批量大小， <cite>D</cite> 是样本大小。</p></li>
<li><p>Target : <span class="math notranslate nohighlight">\((N)\)</span></p></li>
<li><p>Output : 若 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> = <code class="docutils literal notranslate"><span class="pre">'none'</span></code> ，那么输出为 <span class="math notranslate nohighlight">\((N)\)</span> ，否则为标量。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MarginRankingLoss</span><span class="p">(</span><span class="n">margin</span> <span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[2., 1., 0.],</span>
<span class="go">        [3., 0., 5.],</span>
<span class="go">        [0., 0., 0.]], dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MarginRankingLoss</span><span class="p">(</span><span class="n">margin</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(8.2000, dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MarginRankingLoss</span><span class="p">(</span><span class="n">margin</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(8.3333, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.MaxPool1d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">MaxPool1d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_indices</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.MaxPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>此接口与 PyTorch 一致，文档参考自： <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d">https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d</a> 。</p>
<p>在一个由多个输入平面组成的输入信号上应用 <strong>1D max pooling</strong> 。</p>
<p>在最简单的情况下，若输入大小为 <span class="math notranslate nohighlight">\((N, C, L)\)</span> 和输出大小为 <span class="math notranslate nohighlight">\((N, C, L_{out})\)</span> ，则该层的输出值可以被准确描述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out(N_i, C_j, k) = \max_{m=0, \ldots, \text{kernel_size} - 1}
        input(N_i, C_j, stride \times k + m)\]</div></div>
<p>若 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 不为零，则在输入的两侧用最小值隐式地填充 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 个点。 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 是滑动窗口中元素之间的跨步。这个 <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">链接</a> 中有 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 的可视化展示。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>若 ceil_mode = True 且滑动窗口从左侧填充区域或输入中开始，则允许其越界。从右侧填充区域开始的滑动窗口将被忽略。</p>
</div>
<dl>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>kernel_size</strong> - 滑动窗口的大小，必须为正。</p></li>
<li><p><strong>stride</strong> - 滑动窗口的步长，必须为正。默认值为 <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>。</p></li>
<li><p><strong>padding</strong> - 填充在输入张量两侧的隐式负无穷，该值必须非负且不大于 kernel_size / 2。</p></li>
<li><p><strong>dilation</strong> - 滑动窗口中元素之间的步幅，必须为正。</p></li>
<li><p><strong>return_indices</strong> - 若设置为 <code class="docutils literal notranslate"><span class="pre">True</span></code> 则返回 argmax 以及最大值，在后续的 <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.MaxUnpool1d</span></code> 中使用。</p></li>
<li><p><strong>ceil_mode</strong> - 若设置为 <code class="docutils literal notranslate"><span class="pre">True</span></code> 则使用 <cite>ceil</cite> 而非 <cite>floor</cite> 来计算输出形状，这确保了输入张量中的每个元素都被滑动窗口覆盖。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, L_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, L_{out})\)</span> ，其中</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[L_{out} = \left\lfloor \frac{L_{in} + 2 \times \text{padding} - \text{dilation}
      \times (\text{kernel_size} - 1) - 1}{\text{stride}} + 1\right\rfloor\]</div></div>
</li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">of_maxpool1d</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">of_maxpool1d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="n">oneflow</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.MaxPool1d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.MaxPool1d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.MaxPool2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">MaxPool2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_indices</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.MaxPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>此接口与 PyTorch 一致，文档参考自： <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d">https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d</a> 。</p>
<p>在一个由多个输入平面组成的输入信号上应用 <strong>2D max pooling</strong> 。</p>
<p>在最简单的情况下，若输入大小为 <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> ，输出大小为 <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> 且 <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> 为 <span class="math notranslate nohighlight">\((kH, kW)\)</span> ，则该层的输出值可以被准确描述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    out(N_i, C_j, h, w) ={} &amp; \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\
                            &amp; \text{input}(N_i, C_j, \text{stride[0]} \times h + m,
                                           \text{stride[1]} \times w + n)
\end{aligned}\end{split}\]</div></div>
<p>若 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 不为零，则在输入的两侧用最小值隐式地填充 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 个点。 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 控制了核点之间的空间。这个 <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">链接</a> 中有 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 的可视化展示。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>若 ceil_mode = True 且滑动窗口从左侧填充区域或输入中开始，则允许其越界。从右侧填充区域开始的滑动窗口将被忽略。</p>
</div>
<dl>
<dt>参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 可以是：</dt><dd><ul class="simple">
<li><p>一个单独的 <code class="docutils literal notranslate"><span class="pre">int</span></code> – 在这种情况下，高度和宽度维度使用相同的值。</p></li>
<li><p>一个由两个 int 组成的 <code class="docutils literal notranslate"><span class="pre">tuple</span></code> – 在这种情况下，第一个整数用于高度维度，第二个整数用于宽度维度。</p></li>
</ul>
</dd>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>kernel_size</strong> - 窗口的最大尺寸。</p></li>
<li><p><strong>stride</strong> - 滑动窗口的步长，必须为正。默认值为 <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>。</p></li>
<li><p><strong>padding</strong> - 要添加在两侧的隐式最小填充。</p></li>
<li><p><strong>dilation</strong> - 滑动窗口中元素之间的步幅，必须为正。</p></li>
<li><p><strong>return_indices</strong> - 若设置为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则返回最大索引和输出，在后续的 <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.MaxUnpool2d</span></code> 中使用。</p></li>
<li><p><strong>ceil_mode</strong> - 若设置为 <code class="docutils literal notranslate"><span class="pre">True</span></code> 则使用 <cite>ceil</cite> 而非 <cite>floor</cite> 来计算输出形状。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> ，其中</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor\frac{H_{in} + 2 * \text{padding[0]} - \text{dilation[0]}
      \times (\text{kernel_size[0]} - 1) - 1}{\text{stride[0]}} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor\frac{W_{in} + 2 * \text{padding[1]} - \text{dilation[1]}
      \times (\text{kernel_size[1]} - 1) - 1}{\text{stride[1]}} + 1\right\rfloor\]</div></div>
</li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="n">oneflow</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.MaxPool2d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.MaxPool2d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.MaxPool3d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">MaxPool3d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_indices</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.MaxPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>此接口与 PyTorch 一致，文档参考自： <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html#torch.nn.MaxPool3d">https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html#torch.nn.MaxPool3d</a> 。</p>
<p>在一个由多个输入平面组成的输入信号上应用 <strong>3D max pooling</strong> 。</p>
<p>在最简单的情况下，若输入大小为 <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span> ，输出大小为 <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span> 且 <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> 为 <span class="math notranslate nohighlight">\((kD, kH, kW)\)</span> ，则该层的输出值可以被准确描述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \text{out}(N_i, C_j, d, h, w) ={} &amp; \max_{k=0, \ldots, kD-1} \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\
                                      &amp; \text{input}(N_i, C_j, \text{stride[0]} \times d + k,
                                                     \text{stride[1]} \times h + m, \text{stride[2]} \times w + n)
\end{aligned}\end{split}\]</div></div>
<p>若 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 不为零，则在输入的两侧用最小值隐式地填充 <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> 个点。 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 控制了核点之间的空间。这个 <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">链接</a> 中有 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 的可视化展示。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>若 ceil_mode = True 且滑动窗口从左侧填充区域或输入中开始，则允许其越界。从右侧填充区域开始的滑动窗口将被忽略。</p>
</div>
<dl>
<dt>参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> 可以是：</dt><dd><ul class="simple">
<li><p>一个单独的 <code class="docutils literal notranslate"><span class="pre">int</span></code> – 在这种情况下，深度、高度和宽度维度使用相同的值。</p></li>
<li><p>一个由三个 int 组成的 <code class="docutils literal notranslate"><span class="pre">tuple</span></code> – 在这种情况下，第一个整数用于深度维度，第二个整数用于高度维度，第三个整数用于宽度维度。</p></li>
</ul>
</dd>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>kernel_size</strong> - 窗口的最大尺寸。</p></li>
<li><p><strong>stride</strong> - 滑动窗口的步长，必须为正。默认值为 <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>。</p></li>
<li><p><strong>padding</strong> - 要添加在三个边上的的隐式最小填充。</p></li>
<li><p><strong>dilation</strong> - 滑动窗口中元素之间的步幅，必须为正。</p></li>
<li><p><strong>return_indices</strong> - 若设置为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则返回最大索引和输出，在后续的 <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.MaxUnpool3d</span></code> 中使用。</p></li>
<li><p><strong>ceil_mode</strong> - 若设置为 <code class="docutils literal notranslate"><span class="pre">True</span></code> 则使用 <cite>ceil</cite> 而非 <cite>floor</cite> 来计算输出形状。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span> ，其中</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times
  (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times
  (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2] \times
  (\text{kernel_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor\]</div></div>
</li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">of_maxpool3d</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">of_maxpool3d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="n">oneflow</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.MaxPool3d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.MaxPool3d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.MinMaxObserver">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">MinMaxObserver</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">quantization_formula</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'google'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_bit</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_scheme</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'symmetric'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_layer_quantization</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.MinMaxObserver" title="Permalink to this definition">¶</a></dt>
<dd><p>计算输入张量的量化参数。</p>
<p>首先计算输入张量的最大值和最小值：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; max\_value = max(input)\\&amp; min\_value = min(input)\end{aligned}\end{align} \]</div></div>
<p>然后用以下等式计算 scale 和 zero_point ：</p>
<blockquote>
<div><p>若 quantization_scheme 为 “symmetric”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; denom = 2^{quantization\_to\_bit - 1} - 1\\&amp; scale = max(|max\_value|,|min\_value|) / denom\\&amp; zero\_point = 0\end{aligned}\end{align} \]</div></div>
<p>若 quantization_scheme 为 “affine”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; denom = 2^{quantization\_to\_bit} - 1\\&amp; scale = (max\_value - min\_value) / denom\\&amp; zero\_point = -min\_value / scale\end{aligned}\end{align} \]</div></div>
</div></blockquote>
<p>若 per_layer_quantization 为 False，则 scale 和 zero_point 的形状将为 <code class="docutils literal notranslate"><span class="pre">(input.shape[0],)</span></code>。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>quantization_bit</strong> (int) - 量化输入为 uintX / intX，X 的值在 [2, 8] 中，默认值为 8。</p></li>
<li><p><strong>quantization_scheme</strong> (str) - “symmetric” 或 “affine”，量化为有符号/无符号整数。默认值为 “symmetric”。</p></li>
<li><p><strong>quantization_formula</strong> (str) - “google” 或 “cambricon”。</p></li>
<li><p><strong>per_layer_quantization</strong> (bool) - 若设置为 True，则表示 per-layer，否则为 per-channel。默认值为 True。</p></li>
</ul>
</dd>
<dt>返回值：</dt><dd><p>Tuple[oneflow.Tensor, oneflow.Tensor]: 输入张量的 scale 和 zero_point。</p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">weight</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_bit</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_scheme</span> <span class="o">=</span> <span class="s2">"symmetric"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_formula</span> <span class="o">=</span> <span class="s2">"google"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">per_layer_quantization</span> <span class="o">=</span> <span class="bp">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">min_max_observer</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MinMaxObserver</span><span class="p">(</span><span class="n">quantization_formula</span><span class="o">=</span><span class="n">quantization_formula</span><span class="p">,</span> <span class="n">quantization_bit</span><span class="o">=</span><span class="n">quantization_bit</span><span class="p">,</span>
<span class="gp">... </span><span class="n">quantization_scheme</span><span class="o">=</span><span class="n">quantization_scheme</span><span class="p">,</span> <span class="n">per_layer_quantization</span><span class="o">=</span><span class="n">per_layer_quantization</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> <span class="n">min_max_observer</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">input_tensor</span><span class="p">,</span> <span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Mish">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Mish</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Mish" title="Permalink to this definition">¶</a></dt>
<dd><p>逐元素地应用如下公式：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{Mish}(x) = x * \text{Tanh}(\text{Softplus}(x))\]</div></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>请参考 <a class="reference external" href="https://arxiv.org/abs/1908.08681">Mish: A Self Regularized Non-Monotonic Neural Activation Function</a> 。</p>
</div>
<dl class="simple">
<dt>形状：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> ，其中 <cite>*</cite> 表示任意数量的附加维度。</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span> ，与输入的形状相同。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mish</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Mish</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">mish</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.8651, 1.9440, 2.9865], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ModuleDict">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ModuleDict</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">modules</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">oneflow.nn.module.Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ModuleDict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ModuleList">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ModuleList</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">modules</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow.nn.module.Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ModuleList" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.MovingAverageMinMaxObserver">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">MovingAverageMinMaxObserver</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_formula</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'google'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_update_after_iters</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_bit</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_scheme</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'symmetric'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.MovingAverageMinMaxObserver" title="Permalink to this definition">¶</a></dt>
<dd><p>根据输入张量的最小值和最大值的移动平均计算量化参数。</p>
<p>首先计算输入张量的 moving_max 和 moving_min：</p>
<blockquote>
<div><p>若 quantization_scheme 为 “symmetric”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; moving\_max = moving\_max * momentum + |max(input)| * (1 - momentum)\\&amp; moving\_min = moving\_max\end{aligned}\end{align} \]</div></div>
<p>若 quantization_scheme 为 “affine”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; moving\_max = moving\_max * momentum + max(input) * (1 - momentum)\\&amp; moving\_min = moving\_min * momentum + min(input) * (1 - momentum)\end{aligned}\end{align} \]</div></div>
</div></blockquote>
<p>最小值和最大值的移动平均值被初始化为第一批输入 <cite>Blob</cite> 的最小值和最大值。</p>
<p>然后用以下等式计算 scale 和 zero_point：</p>
<blockquote>
<div><p>若 quantization_scheme 为 “symmetric”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; denom = 2^{quantization\_to\_bit - 1} - 1\\&amp; scale = moving\_max / denom\\&amp; zero\_point = 0\end{aligned}\end{align} \]</div></div>
<p>若 quantization_scheme 为 “affine”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; denom = 2^{quantization\_to\_bit} - 1\\&amp; scale = (moving\_max - moving\_min) / denom\\&amp; zero\_point = -moving\_min / scale\end{aligned}\end{align} \]</div></div>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">current_train_step</span></code> 可以直接被赋值给一个优化器（例如 SGD）。</p>
</div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>training</strong> (bool) - 模式是否处于训练状态，默认值为 False。</p></li>
<li><p><strong>quantization_bit</strong> (int) - 量化输入为 uintX / intX，X 的值在 [2, 8] 中，默认值为 8。</p></li>
<li><p><strong>quantization_scheme</strong> (str) - “symmetric” 或 “affine”，量化为有符号/无符号整数。默认值为 “symmetric”。</p></li>
<li><p><strong>quantization_formula</strong> (str) - “google” 或 “cambricon”。</p></li>
<li><p><strong>momentum</strong> (float) - 指数移动平均运算的平滑参数，默认值为 0.95。</p></li>
</ul>
</dd>
<dt>返回值：</dt><dd><p>Tuple[oneflow.Tensor, oneflow.Tensor]: 输入张量的 scale 和 zero_point。</p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">weight</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">current_train_step_tensor</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>  <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
<span class="gp">... </span>   <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_bit</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_scheme</span> <span class="o">=</span> <span class="s2">"symmetric"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_formula</span> <span class="o">=</span> <span class="s2">"google"</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">moving_average_min_max_observer</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">(</span><span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">quantization_formula</span><span class="o">=</span><span class="n">quantization_formula</span><span class="p">,</span>
<span class="gp">... </span>                                                                      <span class="n">stop_update_after_iters</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">quantization_bit</span><span class="o">=</span><span class="n">quantization_bit</span><span class="p">,</span>
<span class="gp">... </span>                                                                      <span class="n">quantization_scheme</span><span class="o">=</span><span class="n">quantization_scheme</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span>
<span class="gp">... </span>                                                                      <span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">)</span> <span class="o">=</span> <span class="n">moving_average_min_max_observer</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">input_tensor</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">current_train_step_tensor</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.MovingAverageMinMaxObserver.reset_running_stats">
<code class="sig-name descname"><span class="pre">reset_running_stats</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.MovingAverageMinMaxObserver.reset_running_stats" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.NLLLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">NLLLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_index</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">-</span> <span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.NLLLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>负对数似然损失，在 <cite>C</cite> 类训练分类问题中很有效。</p>
<p>通过前向调用给出的输入将包含每个类的对数概率。在 <cite>K</cite> 维情况下，输入必须是大小为 <span class="math notranslate nohighlight">\((minibatch, C)\)</span> 或 <span class="math notranslate nohighlight">\((minibatch, C, d_1, d_2, ..., d_K)\)</span> 的张量且 <span class="math notranslate nohighlight">\(K \geq 1\)</span></p>
<p>通过在网络的最后一层添加 <cite>LogSoftmax</cite> 层，可以直接得到神经网络中的对数概率。如果不想添加额外的层，可以改用 <cite>CrossEntropyLoss</cite></p>
<p>该函数期望的 <cite>target</cite> 应为 <span class="math notranslate nohighlight">\([0, C-1]\)</span> 范围内的索引，其中 <cite>C = number of classes</cite></p>
<p>未规约的损失（如将 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> 设置为 <code class="docutils literal notranslate"><span class="pre">'none'</span></code>）可被描述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - w_{y_n} x_{n,y_n}, \quad
w_{c} = \mathbb{1},\]</div></div>
<p>其中 <span class="math notranslate nohighlight">\(x\)</span> 是输入，<span class="math notranslate nohighlight">\(y\)</span> 是输出，<span class="math notranslate nohighlight">\(w\)</span> 是权重且
<span class="math notranslate nohighlight">\(N\)</span> 是批的大小。如果 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> 不是 <code class="docutils literal notranslate"><span class="pre">'none'</span></code>
（默认值为 <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>），则有</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\ell(x, y) = \begin{cases}
    \sum_{n=1}^N \frac{1}{N} l_n, &amp;
    \text{if reduction} = \text{`mean';}\\
    \sum_{n=1}^N l_n,  &amp;
    \text{if reduction} = \text{`sum'.}
\end{cases}\end{split}\]</div></div>
<p>该函数也可用于更高维度的输入，例如 2D 图像，通过提供
大小为 <span class="math notranslate nohighlight">\((minibatch, C, d_1, d_2, ..., d_K)\)</span> 且 <span class="math notranslate nohighlight">\(K \geq 1\)</span> 的输
入和一个形状合适的目标，其中 <span class="math notranslate nohighlight">\(K\)</span> 是维度数。在图像的情况下，它计
算每个像素的 NLL 损失。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>reduction</strong> (string, optional) - 指定应用于输出的 reduction：<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code> ：不进行 reduction；<code class="docutils literal notranslate"><span class="pre">'mean'</span></code> ：取输出的加权平均值；<code class="docutils literal notranslate"><span class="pre">'sum'</span></code> ：输出将被求和。默认值为 <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span><span class="p">[[</span><span class="o">-</span><span class="mf">0.1664078</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7256707</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.14690138</span><span class="p">],</span>
<span class="gp">... </span><span class="p">[</span><span class="o">-</span><span class="mf">0.21474946</span><span class="p">,</span> <span class="mf">0.53737473</span><span class="p">,</span> <span class="mf">0.99684894</span><span class="p">],</span>
<span class="gp">... </span><span class="p">[</span><span class="o">-</span><span class="mf">1.135804</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.50371903</span><span class="p">,</span> <span class="mf">0.7645404</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([ 0.1664, -0.5374, -0.7645], dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(-1.1355, dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(-0.3785, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.OFRecordBytesDecoder">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">OFRecordBytesDecoder</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">blob_name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.OFRecordBytesDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>此算子将张量读取为字节，输出取决于下游任务，可能需要进一步的解码过程，比如 cv2.imdecode() 用于图像和解码，以及 decode(“utf-8”) 用于字符。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>blob_name</strong> - OFRecord 目标特征的名称。</p></li>
<li><p><strong>name</strong> - 图中此分量的名称。</p></li>
<li><p><strong>input</strong> - 可能由 OFRecordReader 提供的张量。</p></li>
</ul>
</dd>
<dt>返回值：</dt><dd><p>按字节编码后的张量。</p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">example</span><span class="p">():</span>
<span class="gp">... </span>     <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="gp">... </span>     <span class="n">record_reader</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">OFRecordReader</span><span class="p">(</span>
<span class="gp">... </span>        <span class="s2">"dataset/"</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">part_name_suffix_length</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="gp">... </span>     <span class="p">)</span>
<span class="gp">... </span>     <span class="n">val_record</span> <span class="o">=</span> <span class="n">record_reader</span><span class="p">()</span>

<span class="gp">... </span>     <span class="n">bytesdecoder_img</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">OFRecordBytesDecoder</span><span class="p">(</span><span class="s2">"encoded"</span><span class="p">)</span>

<span class="gp">... </span>     <span class="n">image_bytes_batch</span> <span class="o">=</span> <span class="n">bytesdecoder_img</span><span class="p">(</span><span class="n">val_record</span><span class="p">)</span>

<span class="gp">... </span>     <span class="n">image_bytes</span> <span class="o">=</span> <span class="n">image_bytes_batch</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="gp">... </span>     <span class="k">return</span> <span class="n">image_bytes</span>
<span class="gp">... </span><span class="n">example</span><span class="p">()</span>  
<span class="go">array([255 216 255 ...  79 255 217], dtype=uint8)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.OFRecordImageDecoder">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">OFRecordImageDecoder</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">blob_name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">color_space</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'BGR'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.OFRecordImageDecoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.OFRecordImageDecoderRandomCrop">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">OFRecordImageDecoderRandomCrop</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">blob_name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">color_space</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'BGR'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_attempts</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_seed</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_area</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">[0.08,</span> <span class="pre">1.0]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_aspect_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">[0.75,</span> <span class="pre">1.333333]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.OFRecordImageDecoderRandomCrop" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.OFRecordRawDecoder">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">OFRecordRawDecoder</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">blob_name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">oneflow._oneflow_internal.dtype</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim1_varying_length</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncate</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">auto_zero_padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.OFRecordRawDecoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.OFRecordReader">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">OFRecordReader</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ofrecord_dir</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_part_num</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">part_name_prefix</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'part-'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">part_name_suffix_length</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_shuffle</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle_buffer_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1024</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle_after_epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_seed</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.device</span><span class="p"><span class="pre">,</span> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">placement</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.placement</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sbp</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.sbp.sbp</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.sbp.sbp</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.OFRecordReader" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.PReLU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">PReLU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_parameters</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.PReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>逐元素地应用如下公式：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[PReLU(x) = \max(0,x) + a * \min(0,x)\]</div></div>
<p>这里 <span class="math notranslate nohighlight">\(a\)</span> 是一个可学习的参数。当不带参数调用时， <cite>nn.PReLU()</cite> 在所有输入通道中使用单个参数 <span class="math notranslate nohighlight">\(a\)</span>。
若调用 <cite>nn.PReLU(nChannels)</cite> ，为每个通道使用单独的 <span class="math notranslate nohighlight">\(a\)</span>。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>为了获得良好的性能，在学习 <span class="math notranslate nohighlight">\(a\)</span> 时不应使用权重衰减。</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>通道维度是输入的第二维度。当输入维度不足 2 时，就不存在通道维度且通道数为 1。</p>
</div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>num_parameters</strong> (int) - 需要学习的 <span class="math notranslate nohighlight">\(a\)</span> 的数量尽管它将一个 int 数值作为输入，但只有两类值是合法的： 1 或输入的通道数。默认值为 1。</p></li>
<li><p><strong>init</strong> (float) - <span class="math notranslate nohighlight">\(a\)</span> 的初始值。默认值为 0.25。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> ，其中 <cite>*</cite> 表示任意数量的附加维度。</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span> ，与输入的形状相同。</p></li>
</ul>
</dd>
<dt>属性：</dt><dd><ul class="simple">
<li><p>weight (Tensor) - 形状为 (<code class="xref py py-attr docutils literal notranslate"><span class="pre">num_parameters</span></code>) 的可学习权重。</p></li>
</ul>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[[[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]]]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="go">[[[[ 1.  -0.5]</span>
<span class="go">   [ 3.   4. ]]]]</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.PReLU.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.PReLU.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Parameter">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Parameter</span></code><a class="headerlink" href="#oneflow.nn.Parameter" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ParameterDict">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ParameterDict</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ParameterDict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ParameterList">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ParameterList</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ParameterList" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py attribute">
<dt id="oneflow.nn.PixelShuffle">
<code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">PixelShuffle</span></code><a class="headerlink" href="#oneflow.nn.PixelShuffle" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <a class="reference internal" href="#oneflow.nn.modules.pixelshuffle.PixelShufflev2" title="oneflow.nn.modules.pixelshuffle.PixelShufflev2"><code class="xref py py-class docutils literal notranslate"><span class="pre">oneflow.nn.modules.pixelshuffle.PixelShufflev2</span></code></a></p>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Quantization">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Quantization</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">quantization_formula</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'google'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_bit</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_scheme</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'symmetric'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Quantization" title="Permalink to this definition">¶</a></dt>
<dd><p>在推理时模拟量化操作。</p>
<p>输出将被计算为：</p>
<blockquote>
<div><p>若 quantization_scheme == “symmetric”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; quant\_max = 2^{quantization\_to\_bit - 1} - 1\\&amp; quant\_min = -quant\_max\\&amp; clamp(round(x / scale), quant\_min, quant\_max)\end{aligned}\end{align} \]</div></div>
<p>若 quantization_scheme == “affine”:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp; quant\_max = 2^{quantization\_to\_bit} - 1\\&amp; quant\_min = 0\\&amp; (clamp(round(x / scale + zero\_point), quant\_min, quant\_max) - zero\_point)\end{aligned}\end{align} \]</div></div>
</div></blockquote>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>quantization_bit</strong> (int) - 量化输入为 uintX / intX ， X 的值在 [2, 8] 中，默认值为 8。</p></li>
<li><p><strong>quantization_scheme</strong> (str) - “symmetric” 或 “affine” ， 量化为有符号/无符号整数。 默认值为 “symmetric”。</p></li>
<li><p><strong>quantization_formula</strong> (str) - “google” or “cambricon”。</p></li>
</ul>
</dd>
<dt>返回值：</dt><dd><p>oneflow.Tensor: 经过量化操作后的输入张量。</p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">weight</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_bit</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_scheme</span> <span class="o">=</span> <span class="s2">"symmetric"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_formula</span> <span class="o">=</span> <span class="s2">"google"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">per_layer_quantization</span> <span class="o">=</span> <span class="bp">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">min_max_observer</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MinMaxObserver</span><span class="p">(</span><span class="n">quantization_formula</span><span class="o">=</span><span class="n">quantization_formula</span><span class="p">,</span> <span class="n">quantization_bit</span><span class="o">=</span><span class="n">quantization_bit</span><span class="p">,</span>
<span class="gp">... </span><span class="n">quantization_scheme</span><span class="o">=</span><span class="n">quantization_scheme</span><span class="p">,</span> <span class="n">per_layer_quantization</span><span class="o">=</span><span class="n">per_layer_quantization</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Quantization</span><span class="p">(</span><span class="n">quantization_formula</span><span class="o">=</span><span class="n">quantization_formula</span><span class="p">,</span> <span class="n">quantization_bit</span><span class="o">=</span><span class="n">quantization_bit</span><span class="p">,</span>
<span class="gp">... </span><span class="n">quantization_scheme</span><span class="o">=</span><span class="n">quantization_scheme</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> <span class="n">min_max_observer</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">input_tensor</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">output_tensor</span> <span class="o">=</span> <span class="n">quantization</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">input_tensor</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">scale</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">zero_point</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ReLU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ReLU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>ReLU 激活函数，对张量中的每一个元素做 element-wise 运算，公式如下:</p>
<p><span class="math notranslate nohighlight">\(\text{ReLU}(x) = (x)^+ = \max(0, x)\)</span></p>
<dl class="simple">
<dt>参数:</dt><dd><p>inplace: 是否做 in-place 操作。 默认为 <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
</dd>
<dt>形状:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> 其中 <cite>*</cite> 的意思是，可以指定任意维度</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span> 输入形状与输出形状一致</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relu</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">tensor([1., 0., 3.], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.ReLU.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ReLU.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ReLU6">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ReLU6</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ReLU6" title="Permalink to this definition">¶</a></dt>
<dd><p>逐元素地应用如下公式：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\text{Relu6}(x) = \begin{cases}
    6 &amp; \text{ if } x &gt; 6 \\
    0 &amp; \text{ if } x &lt; 0 \\
    x &amp; \text{ otherwise } \\
\end{cases}\end{split}\]</div></div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>inplace</strong> - 可以选择以 in-place 的方式执行操作。默认值为 <code class="docutils literal notranslate"><span class="pre">False</span></code>。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> ，其中 <cite>*</cite> 表示任意数量的附加维度。</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span> ，与输入的形状相同。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relu6</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">relu6</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.0000, 0.0000, 0.5000], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.ReLU6.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ReLU6.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ReflectionPad2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ReflectionPad2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ReflectionPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>此接口与 PyTorch 一致，文档参考自： <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad2d.html">https://pytorch.org/docs/stable/generated/torch.nn.ReflectionPad2d.html</a> 。</p>
<p>使用输入边界的反射来填充输入张量。</p>
<dl>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>padding</strong> (Union[int,tuple]) - 填充范围的大小或边界。若输入是一个 int，那各个维度上都会填充同样大小的数据。若输入是一个四个元素的元组，那么使用 <span class="math notranslate nohighlight">\((\text{padding}_{\text{left}}, \text{padding}_{\text{right}}, \text{padding}_{\text{top}}, \text{padding}_{\text{bottom}} )\)</span>。</p></li>
</ul>
</dd>
<dt>返回值：</dt><dd><p>Tensor: 返回一个新的张量，这是输入张量的反射填充的结果。</p>
</dd>
<dt>形状：</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H_{\text{in}}, W_{\text{in}})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H_{\text{out}}, W_{\text{out}})\)</span> ，其中</p>
<p><span class="math notranslate nohighlight">\(H_{\text{out}} = H_{\text{in}} + \text{padding}_{\text{top}} + \text{padding}_{\text{bottom}}\)</span></p>
<p><span class="math notranslate nohighlight">\(W_{\text{out}} = W_{\text{in}} + \text{padding}_{\text{left}} + \text{padding}_{\text{right}}\)</span></p>
</li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">18</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReflectionPad2d</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[[[ 5.,  4.,  3.,  4.,  5.,  4.,  3.],</span>
<span class="go">          [ 2.,  1.,  0.,  1.,  2.,  1.,  0.],</span>
<span class="go">          [ 5.,  4.,  3.,  4.,  5.,  4.,  3.],</span>
<span class="go">          [ 8.,  7.,  6.,  7.,  8.,  7.,  6.],</span>
<span class="go">          [ 5.,  4.,  3.,  4.,  5.,  4.,  3.]],</span>

<span class="go">         [[14., 13., 12., 13., 14., 13., 12.],</span>
<span class="go">          [11., 10.,  9., 10., 11., 10.,  9.],</span>
<span class="go">          [14., 13., 12., 13., 14., 13., 12.],</span>
<span class="go">          [17., 16., 15., 16., 17., 16., 15.],</span>
<span class="go">          [14., 13., 12., 13., 14., 13., 12.]]]], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.ReflectionPad2d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.ReflectionPad2d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ReplicationPad2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ReplicationPad2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ReplicationPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>此接口与 PyTorch 一致，文档参考自： <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad2d.html#replicationpad2d">https://pytorch.org/docs/stable/generated/torch.nn.ReplicationPad2d.html#replicationpad2d</a> 。</p>
<p>通过复制输入张量边界元素对输入张量进行填充操作。</p>
<dl>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>padding</strong> (Union[int, tuple, list]) - 填充范围的大小。若输入是一个 int，那各个边界上都会填充上同样大小的数据。若输入是一个四个元素的元组，那么使用 (<span class="math notranslate nohighlight">\(\mathrm{padding_{left}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{right}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{top}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{bottom}}\)</span>)。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> ，其中</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(H_{out} = H_{in} + \mathrm{padding_{top}} + \mathrm{padding_{bottom}}\)</span></p>
<p><span class="math notranslate nohighlight">\(W_{out} = W_{in} + \mathrm{padding_{left}} + \mathrm{padding_{right}}\)</span></p>
</div></blockquote>
</li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad2d</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">18</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_int</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">18</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="go">oneflow.Size([1, 2, 5, 7])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([[[[ 0.,  0.,  0.,  1.,  2.,  2.,  2.],</span>
<span class="go">          [ 0.,  0.,  0.,  1.,  2.,  2.,  2.],</span>
<span class="go">          [ 3.,  3.,  3.,  4.,  5.,  5.,  5.],</span>
<span class="go">          [ 6.,  6.,  6.,  7.,  8.,  8.,  8.],</span>
<span class="go">          [ 6.,  6.,  6.,  7.,  8.,  8.,  8.]],</span>

<span class="go">         [[ 9.,  9.,  9., 10., 11., 11., 11.],</span>
<span class="go">          [ 9.,  9.,  9., 10., 11., 11., 11.],</span>
<span class="go">          [12., 12., 12., 13., 14., 14., 14.],</span>
<span class="go">          [15., 15., 15., 16., 17., 17., 17.],</span>
<span class="go">          [15., 15., 15., 16., 17., 17., 17.]]]], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.ReplicationPad2d.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.ReplicationPad2d.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.SELU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">SELU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.SELU" title="Permalink to this definition">¶</a></dt>
<dd><p>逐元素地应用 SELU 函数，其等式为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{SELU}(x) = \text{scale} * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))\]</div></div>
<p>其中有 <span class="math notranslate nohighlight">\(\alpha = 1.6732632423543772848170429916717\)</span> 和</p>
<p><span class="math notranslate nohighlight">\(\text{scale} = 1.0507009873554804934193349852946\)</span></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>当使用 <code class="docutils literal notranslate"><span class="pre">kaiming_normal</span></code> 或 <code class="docutils literal notranslate"><span class="pre">kaiming_normal_</span></code> 进行初始化时，应当设置
<code class="docutils literal notranslate"><span class="pre">nonlinearity='linear'</span></code> 而非 <code class="docutils literal notranslate"><span class="pre">nonlinearity='selu'</span></code>，这是为了得到 <a class="reference external" href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a> 。
查看 <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.init.calculate_gain()</span></code> 获得更多信息。</p>
</div>
<p>获得更多细节请参考文献 <a class="reference external" href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a> 。</p>
<dl class="simple">
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> - <span class="math notranslate nohighlight">\((N, *)\)</span> ，其中 <cite>*</cite> 表示任意数量的附加维度。</p></li>
<li><p><strong>Output</strong> - <span class="math notranslate nohighlight">\((N, *)\)</span> ，与输入形状相同。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selu</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SELU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">selu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([1.0507, 2.1014, 3.1521], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Sequential">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Sequential</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Sequential" title="Permalink to this definition">¶</a></dt>
<dd><p>一个序列容器。按照 Module 在构造函数中被传递的顺序将其添加到容器中。或者，也可以向构造函数传递 Module 的有序字典。</p>
<p>为了便于理解，这里有一个示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span> 
<span class="go">Sequential(</span>
<span class="go">  (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span>
<span class="go">  (1): ReLU()</span>
<span class="go">  (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span>
<span class="go">  (3): ReLU()</span>
<span class="go">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
<span class="gp">... </span>   <span class="p">(</span><span class="s1">'conv1'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">)),</span>
<span class="gp">... </span>   <span class="p">(</span><span class="s1">'relu1'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
<span class="gp">... </span>   <span class="p">(</span><span class="s1">'conv2'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">5</span><span class="p">)),</span>
<span class="gp">... </span>   <span class="p">(</span><span class="s1">'relu2'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
<span class="gp">... </span><span class="p">]))</span> 
<span class="go">Sequential(</span>
<span class="go">  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span>
<span class="go">  (relu1): ReLU()</span>
<span class="go">  (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span>
<span class="go">  (relu2): ReLU()</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.SiLU">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">SiLU</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.SiLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Swish 激活函数</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{SiLU}(x) = x * sigmoid(x)\]</div></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>SiLU（Sigmoid Linear Unit）在 <a class="reference external" href="https://arxiv.org/abs/1606.08415">Gaussian Error Linear Units (GELUs)</a>
中被提出在`Sigmoid-Weighted Linear Units for Neural Network Function Approximation
in Reinforcement Learning &lt;<a class="reference external" href="https://arxiv.org/abs/1702.03118">https://arxiv.org/abs/1702.03118</a>&gt;`_ 和 <a class="reference external" href="https://arxiv.org/abs/1710.05941v1">Swish:
a Self-Gated Activation Function</a>
中被试验。</p>
</div>
<dl class="simple">
<dt>形状：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> ，其中 <cite>*</cite> 表示任意数量的附加维度。</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span> ，与输入的形状相同。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">silu</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SiLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">silu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.7311, 1.7616, 2.8577], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Sigmoid">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Sigmoid</span></code><a class="headerlink" href="#oneflow.nn.Sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>应用逐元素函数：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)}\]</div></div>
<dl class="simple">
<dt>形状：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> 其中 <cite>*</cite> 表示任意数量的附加维度</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span> 与输入相同的形状</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.81733328</span><span class="p">,</span> <span class="mf">0.43621480</span><span class="p">,</span> <span class="mf">0.10351428</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.6937, 0.6074, 0.5259], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.SmoothL1Loss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">SmoothL1Loss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.SmoothL1Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>若逐元素的绝对误差低于 beta ，则创建一个使用平方项的指标，否则创建一个使用 L1 项的指标。</p>
<p>此接口与 PyTorch 一致，文档参考自： <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html">https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html</a> 。</p>
<p>与 <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.MSELoss</span></code> 相比，它对异常值不太敏感，并在某些场景下可以防止梯度爆炸。比如 Ross Girshick 的论文 <a class="reference external" href="https://openaccess.thecvf.com/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf">Fast R-CNN</a> 。</p>
<p>对于大小为 <span class="math notranslate nohighlight">\(N\)</span> 的批次，未减少的损失可以描述为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\ell(x, y) = L = \{l_1, ..., l_N\}^T\]</div></div>
<p>其中：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}l_n = \begin{cases}
0.5 (x_n - y_n)^2 / beta, &amp; \text{if } |x_n - y_n| &lt; beta \\
|x_n - y_n| - 0.5 * beta, &amp; \text{otherwise }
\end{cases}\end{split}\]</div></div>
<p>若 <cite>reduction</cite> == <cite>none</cite> ，那么：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\begin{split}\ell(x, y) =
\begin{cases}
    \operatorname{mean}(L), &amp;  \text{if reduction} = \text{`mean';}\\
    \operatorname{sum}(L),  &amp;  \text{if reduction} = \text{`sum'.}
\end{cases}\end{split}\]</div></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>平滑 L1 损失可以看作是 <a class="reference internal" href="#oneflow.nn.L1Loss" title="oneflow.nn.L1Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">L1Loss</span></code></a> ，但 <span class="math notranslate nohighlight">\(|x - y| &lt; beta\)</span> 部分被替换为二次函数，使得它的斜率在 <span class="math notranslate nohighlight">\(|x - y| = beta\)</span> 处为 1。二次函数的部分平滑了 <span class="math notranslate nohighlight">\(|x - y| = 0\)</span> 处的 L1 损失。</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>平滑 L1 损失与 <code class="xref py py-class docutils literal notranslate"><span class="pre">HuberLoss</span></code> 密切相关，相当于 <span class="math notranslate nohighlight">\(huber(x, y) / beta\)</span> （注意 Smooth L1 的 beta 超参数相当于 Huber 的 delta）。这导致了以下差异：
* 当 beta 趋向 0，平滑 L1 损失收敛到 <a class="reference internal" href="#oneflow.nn.L1Loss" title="oneflow.nn.L1Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">L1Loss</span></code></a> ，而 <code class="xref py py-class docutils literal notranslate"><span class="pre">HuberLoss</span></code> 收敛到常数 0 损失。
* 当 beta 趋向 <span class="math notranslate nohighlight">\(+\infty\)</span> ，平滑 L1 损失收敛到常数 0 损失，而 <code class="xref py py-class docutils literal notranslate"><span class="pre">HuberLoss</span></code> 收敛到 <a class="reference internal" href="#oneflow.nn.MSELoss" title="oneflow.nn.MSELoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MSELoss</span></code></a>。
* 对于平滑 L1 损失，随着 beta 的变化，损失的 L1 段的斜率恒为 1。而对于 <code class="xref py py-class docutils literal notranslate"><span class="pre">HuberLoss</span></code> ，斜率是 beta。</p>
</div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>size_average</strong> (bool, optional) - 已弃用（参考 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>）。默认情况下，损失是批次中每个损失元素的平均值。请注意，对于某些损失，每个样本有多个元素。若 <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> == <code class="docutils literal notranslate"><span class="pre">False</span></code>，则将每个小批量的损失相加。当 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> == <code class="docutils literal notranslate"><span class="pre">False</span></code> 时忽略。默认值为 <code class="docutils literal notranslate"><span class="pre">True</span></code>。</p></li>
<li><p><strong>reduce</strong> (bool, optional) - 已弃用（参考 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>）。根据 <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> 对每个小批量的损失进行平均或汇总。若 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> == <code class="docutils literal notranslate"><span class="pre">False</span></code>，则返回每个批元素的损失，并忽略 <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>。默认值为 <code class="docutils literal notranslate"><span class="pre">True</span></code>。</p></li>
<li><p><strong>reduction</strong> (string, optional) - 指定应用于输出的 reduction：<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code> ：不进行 reduction；<code class="docutils literal notranslate"><span class="pre">'mean'</span></code> ：输出的和将会除以输出中的元素数量；<code class="docutils literal notranslate"><span class="pre">'sum'</span></code> ：输出将被求和。注意： <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> 正逐渐被弃用，指定这二者的任何一个都将覆盖 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>。默认值为 <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>。</p></li>
<li><p><strong>beta</strong> (float, optional) - 指定在 L1 和 L2 损失之间更改的阈值。该值必须为非负。默认值为 1.0。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> ，其中 <cite>*</cite> 表示任意数量的附加维度。</p></li>
<li><p>Target: <span class="math notranslate nohighlight">\((N, *)\)</span> ，与输入的形状相同。</p></li>
<li><p>Output: 若 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> == <code class="docutils literal notranslate"><span class="pre">'none'</span></code> 则输出为形状为 <span class="math notranslate nohighlight">\((N)\)</span> 的张量，否则是一个标量。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.0200, 0.1250, 1.7000, 0.0050, 0.1800], dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(0.4060, dtype=oneflow.float32)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor(2.0300, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Softmax">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Softmax</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> → <span class="pre">Tensor</span><a class="headerlink" href="#oneflow.nn.Softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>将 Softmax 函数应用于 n 维输入 tensor 并重新缩放 tensor，以便 n 维输出 tensor
的元素位于 [0,1] 范围内并且和为 1。</p>
<p>Softmax 的公式为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}\]</div></div>
<p>当输入张量是稀疏张量时，则未被指定的值将被视为 <code class="docutils literal notranslate"><span class="pre">-inf</span></code> 。</p>
<dl class="simple">
<dt>形状：</dt><dd><ul class="simple">
<li><p><strong>Input</strong> : <span class="math notranslate nohighlight">\((*)\)</span> 其中 <cite>*</cite> 表示任意数量的附加维度</p></li>
<li><p><strong>Output</strong> : <span class="math notranslate nohighlight">\((*)\)</span> ，与输入的形状相同</p></li>
</ul>
</dd>
<dt>返回类型：</dt><dd><p>oneflow.tensor: 与输入具有相同维度和形状的张量，其值在 [0, 1] 范围内</p>
</dd>
<dt>参数：</dt><dd><p><strong>dim</strong> (int): 要进行 Softmax 计算的维度（因此沿 <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> 的每个切片总和为 1）。</p>
</dd>
</dl>
<p>示例：</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="k">as</span> <span class="nn">flow</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.46716809</span><span class="p">,</span>  <span class="mf">0.40112534</span><span class="p">,</span>  <span class="mf">0.61984003</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.31244969</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.42528763</span><span class="p">,</span>  <span class="mf">1.47953856</span><span class="p">]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([[[0.1575, 0.3754, 0.4671],</span>
<span class="go">         [0.0507, 0.1230, 0.8263]]], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Softmax.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Softmax.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Softplus">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Softplus</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">20</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Softplus" title="Permalink to this definition">¶</a></dt>
<dd><p>逐元素地应用公式：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))\]</div></div>
<p>SoftPlus 是 ReLU 函数的平滑近似，可用于将输出约束为始终为正。</p>
<p>为了数值稳定性，当 <span class="math notranslate nohighlight">\(input \times \beta &gt; threshold\)</span> 时，该函数的实现恢复为线性函数。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>beta</strong> - 公式中 <span class="math notranslate nohighlight">\(\beta\)</span> 的值，默认为 1</p></li>
<li><p><strong>threshold</strong> - 高于此值的函数将恢复为线性函数，默认为 20</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> ，其中 <cite>*</cite> 表示任意数量的附加维度。</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span> ，与输入的形状相同。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softplus</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Softplus</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.4741, 0.6931, 0.9741], dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Softplus.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Softplus.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Softsign">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Softsign</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Softsign" title="Permalink to this definition">¶</a></dt>
<dd><p>SoftSign 激活函数</p>
<p>公式为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[SoftSign(x) = \frac{x}{1 + |x|}\]</div></div>
<dl class="simple">
<dt>形状：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> ，其中 <cite>*</cite> 表示任意数量的附加维度。</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span> ，与输入的形状相同。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softsign</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Softsign</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">softsign</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([0.5000, 0.6667, 0.7500], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Tanh">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Tanh</span></code><a class="headerlink" href="#oneflow.nn.Tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>此算子计算张量的双曲正切值。</p>
<p>等式为：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[out = \frac{e^x-e^{-x}}{e^x+e^{-x}}\]</div></div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>input</strong> (oneflow.Tensor) - 张量。</p></li>
</ul>
</dd>
<dt>返回值：</dt><dd><p>oneflow.Tensor: 运算结果。</p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tanh</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="go">tensor([-0.7616,  0.0000,  0.7616], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.TripletMarginLoss">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">TripletMarginLoss</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">margin</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">swap</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size_average</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.TripletMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>根据给定的输入张量 <span class="math notranslate nohighlight">\(x1\)</span>, <span class="math notranslate nohighlight">\(x2\)</span>, <span class="math notranslate nohighlight">\(x3\)</span> 以及大于 <span class="math notranslate nohighlight">\(0\)</span> 的边界值，创建一个测量三元组损失的指标来测量样本之间的相对相似性。三元组由 <cite>a</cite>, <cite>p</cite> 和 <cite>n</cite> 组成（即分别为锚点、正例和负例）。所有输入张量的形状应为 <span class="math notranslate nohighlight">\((N, D)\)</span> 。</p>
<p>在 V. Balntas, E. Riba 等人的 <a class="reference external" href="http://www.bmva.org/bmvc/2016/papers/paper119/index.html">Learning local feature descriptors with triplets and shallow convolutional neural networks</a> 中详细描述了距离交换。</p>
<p>小批量中每个样本的损失函数是：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}\]</div></div>
<p>其中</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p\]</div></div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>margin</strong> (float, optional) - 默认值为 <span class="math notranslate nohighlight">\(1\)</span></p></li>
<li><p><strong>p</strong> (float, optional) - 成对距离的范数，默认值为 <span class="math notranslate nohighlight">\(2.0\)</span></p></li>
<li><p><strong>swap</strong> (bool, optional) - 默认值为 <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>reduction</strong> (string, optional) - 指定对输出应用的 reduction，可以为：<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>。其中 <code class="docutils literal notranslate"><span class="pre">'none'</span></code> ：不进行 reduction；<code class="docutils literal notranslate"><span class="pre">'mean'</span></code> ：输出的和将会除以输出中的元素数量；<code class="docutils literal notranslate"><span class="pre">'sum'</span></code> ：输出将被求和。默认值为 <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>。注意： <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> 和 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> 正逐渐被弃用，指定这二者的任何一个都将覆盖 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>。默认值为 <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, D)\)</span> ，其中 <span class="math notranslate nohighlight">\(D\)</span> 是向量维度。</p></li>
<li><p>Output: 若 <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> == <code class="docutils literal notranslate"><span class="pre">'none'</span></code> 则输出为形状为 <span class="math notranslate nohighlight">\((N)\)</span> 的张量，否则是一个标量。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">triplet_loss</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">TripletMarginLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">anchor</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">positive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">negative</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">triplet_loss</span><span class="p">(</span><span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">anchor</span><span class="p">),</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">positive</span><span class="p">),</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">negative</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor(6.2971, dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.Upsample">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">Upsample</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="p"><span class="pre">…</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_factor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="p"><span class="pre">…</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'nearest'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">align_corners</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.Upsample" title="Permalink to this definition">¶</a></dt>
<dd><p>此接口与 PyTorch 一致，文档参考自： <a class="reference external" href="https://pytorch.org/docs/1.9.0/_modules/torch/nn/modules/upsampling.html#Upsample">https://pytorch.org/docs/1.9.0/_modules/torch/nn/modules/upsampling.html#Upsample</a> 。</p>
<p>对给定的多通道 1D（时间）、2D（空间）或 3D（体积）数据进行上采样。</p>
<p>假定输入数据的形式为 <cite>批量大小 x 通道 x [可选深度] x [可选高度] x 宽度</cite>。 因此，对于空间输入，我们期待一个 4D 张量；对于体积输入，我们期待一个 5D 张量。</p>
<p>可用于上采样的算法分别是 3D、4D 和 5D 输入张量的最近邻和线性、双线性、双三次和三线性算法。</p>
<p>可以用 <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code> 或目标输出大小来计算输出大小（由于其不确定性，不能同时给出）。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>size</strong> (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int], optional) - 输出空间大小。</p></li>
<li><p><strong>scale_factor</strong> (float or Tuple[float] or Tuple[float, float] or Tuple[float, float, float], optional) - 空间大小的乘数。若是 tuple 则需要匹配输入大小。</p></li>
<li><p><strong>mode</strong> (str, optional) - 上采样算法： <code class="docutils literal notranslate"><span class="pre">'nearest'</span></code>，<code class="docutils literal notranslate"><span class="pre">'linear'</span></code>，<code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code>，<code class="docutils literal notranslate"><span class="pre">'bicubic'</span></code> 和 <code class="docutils literal notranslate"><span class="pre">'trilinear'</span></code>。默认值为：<code class="docutils literal notranslate"><span class="pre">'nearest'</span></code>。</p></li>
<li><p><strong>align_corners</strong> (bool, optional) - 若设置为 <code class="docutils literal notranslate"><span class="pre">True</span></code> ，则若为 True，则输入和输出张量的角像素对齐，从而保留这些像素的值。这仅在模式为 <code class="docutils literal notranslate"><span class="pre">'linear'</span></code>，<code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code> 或 <code class="docutils literal notranslate"><span class="pre">'trilinear'</span></code> 时有效。默认值为 False。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, W_{in})\)</span>, <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span> 或 <span class="math notranslate nohighlight">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, W_{out})\)</span>, <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span>
or <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span> ，其中</p></li>
</ul>
</dd>
</dl>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[D_{out} = \left\lfloor D_{in} \times \text{scale_factor} \right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor H_{in} \times \text{scale_factor} \right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor W_{in} \times \text{scale_factor} \right\rfloor\]</div></div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>若 <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">True</span></code> ，线性插值模式（线性、双线性、双三次和三线性）不会按比例对齐输出和输入像素，因此输出值可能取决于输入大小。这是 0.3.1 版本之前这些模式的默认行为。 0.3.1 版本之后的默认值行为是 <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">False</span></code>。有关其如何影响输出的具体示例，请参见下文。</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>若需要下采样或者一般性的调整大小，应该使用 <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code>。</p>
</div>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"nearest"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> 
<span class="go">tensor([[[[1., 1., 2., 2.],</span>
<span class="go">          ...</span>
<span class="go">          [3., 3., 4., 4.]]]], device='cuda:0', dtype=oneflow.float32)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="oneflow.nn.Upsample.extra_repr">
<code class="sig-name descname"><span class="pre">extra_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> → <span class="pre">str</span><a class="headerlink" href="#oneflow.nn.Upsample.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>设置此模块的其他表现方式。若要打印自定义的额外信息，应在此模块中重新实现此方法。接受单行和多行字符串。</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.UpsamplingBilinear2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">UpsamplingBilinear2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_factor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.UpsamplingBilinear2d" title="Permalink to this definition">¶</a></dt>
<dd><p>对由多个输入通道组成的输入信号应用 2D bilinear upsampling。</p>
<p>若要指定比例，需要 <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> 或 <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code> 作为它的构造函数参数。</p>
<p>若给定 <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> ，则它也是图像 <cite>(h, w)</cite> 的大小。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>size</strong> (int or Tuple[int, int], optional) - 输出空间大小。</p></li>
<li><p><strong>scale_factor</strong> (float or Tuple[float, float], optional) - 空间大小的乘数。</p></li>
</ul>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>对这个类的维护已经停止，请使用 <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code>。它等同于 <code class="docutils literal notranslate"><span class="pre">nn.functional.interpolate(...,</span> <span class="pre">mode='bilinear',</span> <span class="pre">align_corners=True)</span></code></p>
</div>
<dl class="simple">
<dt>形状：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> ，其中</p></li>
</ul>
</dd>
</dl>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor H_{in} \times \text{scale_factor} \right\rfloor\]</div></div>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[W_{out} = \left\lfloor W_{in} \times \text{scale_factor} \right\rfloor\]</div></div>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="o">&gt;</span> <span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="o">&gt;</span> <span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">UpsamplingBilinear2d</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="n">output</span> <span class="c1">#doctest: +ELLIPSIS</span>
<span class="n">tensor</span><span class="p">([[[[</span><span class="mf">1.0000</span><span class="p">,</span> <span class="mf">1.3333</span><span class="p">,</span> <span class="mf">1.6667</span><span class="p">,</span> <span class="mf">2.0000</span><span class="p">],</span>
          <span class="o">...</span>
          <span class="p">[</span><span class="mf">3.0000</span><span class="p">,</span> <span class="mf">3.3333</span><span class="p">,</span> <span class="mf">3.6667</span><span class="p">,</span> <span class="mf">4.0000</span><span class="p">]]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">'cuda:0'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">oneflow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.UpsamplingNearest2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">UpsamplingNearest2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_factor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.UpsamplingNearest2d" title="Permalink to this definition">¶</a></dt>
<dd><p>对由多个输入通道组成的输入信号应用 2D nearest neighbor upsampling。</p>
<p>若要指定比例，需要 <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> 或 <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code> 作为它的构造函数参数。</p>
<p>若给定 <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> ，则它也是图像 <cite>(h, w)</cite> 的大小。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>size</strong> (int or Tuple[int, int], optional) - 输出空间大小。</p></li>
<li><p><strong>scale_factor</strong> (float or Tuple[float, float], optional) - 空间大小的乘数。</p></li>
</ul>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>对这个类的维护已经停止，请使用 <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code>。</p>
</div>
<dl class="simple">
<dt>形状：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> ，其中</p></li>
</ul>
</dd>
</dl>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[H_{out} = \left\lfloor H_{in} \times \text{scale_factor} \right\rfloor
W_{out} = \left\lfloor W_{in} \times \text{scale_factor} \right\rfloor\]</div></div>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">UpsamplingNearest2d</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> 
<span class="go">tensor([[[[1., 1., 2., 2.],</span>
<span class="go">          ...</span>
<span class="go">          [3., 3., 4., 4.]]]], device='cuda:0', dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py class">
<dt id="oneflow.nn.ZeroPad2d">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">oneflow.nn.</span></code><code class="sig-name descname"><span class="pre">ZeroPad2d</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">tuple</span><span class="p"><span class="pre">,</span> </span><span class="pre">list</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.ZeroPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>此接口与 PyTorch 一致，文档参考自： <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad2d.html">https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad2d.html</a> 。</p>
<p>用零填充输入张量边界。用户可以通过设置参数 <cite>paddings</cite> 来设置填充量。</p>
<dl>
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>padding</strong> (Union[int, tuple]) -  填充量的大小。若是 <cite>int</cite> 类型，则在所有边界中使用相同的填充。若是 4-<cite>tuple</cite> 则使用 (<span class="math notranslate nohighlight">\(\mathrm{padding_{left}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{right}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{top}}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{padding_{bottom}}\)</span>)</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> ，其中</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(H_{out} = H_{in} + \mathrm{padding_{top}} + \mathrm{padding_{bottom}}\)</span></p>
<p><span class="math notranslate nohighlight">\(W_{out} = W_{in} + \mathrm{padding_{left}} + \mathrm{padding_{right}}\)</span></p>
</div></blockquote>
</li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m1</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ZeroPad2d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m2</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ZeroPad2d</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">18</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m1</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="go">oneflow.Size([1, 2, 7, 7])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([[[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  1.,  2.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  3.,  4.,  5.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  6.,  7.,  8.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]],</span>

<span class="go">         [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  9., 10., 11.,  0.,  0.],</span>
<span class="go">          [ 0.,  0., 12., 13., 14.,  0.,  0.],</span>
<span class="go">          [ 0.,  0., 15., 16., 17.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]]]], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m2</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([[[[ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  1.,  2.,  0.,  0.],</span>
<span class="go">          [ 0.,  3.,  4.,  5.,  0.,  0.],</span>
<span class="go">          [ 0.,  6.,  7.,  8.,  0.,  0.]],</span>

<span class="go">         [[ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">          [ 0.,  9., 10., 11.,  0.,  0.],</span>
<span class="go">          [ 0., 12., 13., 14.,  0.,  0.],</span>
<span class="go">          [ 0., 15., 16., 17.,  0.,  0.]]]], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py function">
<dt id="oneflow.nn.modules.pixelshuffle.PixelShufflev2">
<code class="sig-prename descclassname"><span class="pre">oneflow.nn.modules.pixelshuffle.</span></code><code class="sig-name descname"><span class="pre">PixelShufflev2</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">upscale_factor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h_upscale_factor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">w_upscale_factor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> → <span class="pre">None</span><a class="headerlink" href="#oneflow.nn.modules.pixelshuffle.PixelShufflev2" title="Permalink to this definition">¶</a></dt>
<dd><p>部分文档参考自： <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle">https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle</a> 。</p>
<p>将形状为 <span class="math notranslate nohighlight">\((*, C \times r_h \times r_w, H, W)\)</span> 的张量中的元素重新排列为形状为 <span class="math notranslate nohighlight">\((*, C, H \times r_h, W \times r_w)\)</span> 的张量，其中 r_h 和 r_w 是放大因子。</p>
<p>这对于实现步幅为 <span class="math notranslate nohighlight">\(1/r\)</span> 的高效亚像素卷积很有效。</p>
<p>更多细节请参考这篇论文：
<a class="reference external" href="https://arxiv.org/abs/1609.05158">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</a> 。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>upscale_factor</strong> (int, optional) - 增加空间分辨率的因子，仅在空间高度和宽度因子相同时使用。</p></li>
<li><p><strong>h_upscale_factor</strong> (int, optional) - 增加高度空间分辨率的因子，只能使用 h_upscale_factor 和 upscale_factor 的其中一个。</p></li>
<li><p><strong>w_upscale_factor</strong> (int, optional) - 增加宽度空间分辨率的因子，只能使用 w_upscale_factor 和 upscale_factor 的其中一个。</p></li>
</ul>
</dd>
<dt>形状：</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((*, C_{in}, H_{in}, W_{in})\)</span>，其中 * 是零个或多个批次维度</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((*, C_{out}, H_{out}, W_{out})\)</span>，其中</p></li>
</ul>
</dd>
</dl>
<p>如果使用 upscale_factor：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}C_{out} = C_{in} \div \text{h_upscale_factor}^2\\H_{out} = H_{in} \times \text{upscale_factor}\\W_{out} = W_{in} \times \text{upscale_factor}\end{aligned}\end{align} \]</div></div>
<p>如果使用 h_upscale_factor 和 w_upscale_factor：</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}C_{out} = C_{in} \div \text{h_upscale_factor} \div \text{w_upscale_factor}\\H_{out} = H_{in} \times \text{h_upscale_factor}\\W_{out} = W_{in} \times \text{w_upscale_factor}\end{aligned}\end{align} \]</div></div>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">PixelShuffle</span><span class="p">(</span><span class="n">upscale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="go">oneflow.Size([3, 1, 10, 10])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">PixelShuffle</span><span class="p">(</span><span class="n">h_upscale_factor</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">w_upscale_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="go">oneflow.Size([1, 2, 6, 8])</span>
</pre></div>
</div>
</dd></dl>
<dl class="py function">
<dt id="oneflow.nn.parallel.DistributedDataParallel">
<code class="sig-prename descclassname"><span class="pre">oneflow.nn.parallel.</span></code><code class="sig-name descname"><span class="pre">DistributedDataParallel</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">oneflow.nn.module.Module</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">broadcast_buffers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bucket_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#oneflow.nn.parallel.DistributedDataParallel" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="py function">
<dt id="oneflow.nn.utils.clip_grad_norm_">
<code class="sig-prename descclassname"><span class="pre">oneflow.nn.utils.</span></code><code class="sig-name descname"><span class="pre">clip_grad_norm_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">oneflow._oneflow_internal.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_norm</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">error_if_nonfinite</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> → <span class="pre">oneflow._oneflow_internal.Tensor</span><a class="headerlink" href="#oneflow.nn.utils.clip_grad_norm_" title="Permalink to this definition">¶</a></dt>
<dd><p>裁剪可迭代参数的梯度范数。范数是在所有梯度上一起计算的，就像它们被连接成一个向量一样。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>parameters</strong> (Iterable[Tensor] 或 Tensor) - 一个可迭代的张量或一个将梯度归一化的单个张量。</p></li>
<li><p><strong>max_norm</strong> (float 或 int) - 梯度的最大范数。</p></li>
<li><p><strong>norm_type</strong> (float 或 int) - 使用的 p-norm 的类型。对于无穷范数，可以是  <code class="docutils literal notranslate"><span class="pre">'inf'</span></code>。</p></li>
<li><p><strong>error_if_nonfinite</strong> (bool) - 当为 True 时，如果来自 :attr:<code class="docutils literal notranslate"><span class="pre">parameters</span></code> 的梯度的总范数为 <code class="docutils literal notranslate"><span class="pre">nan</span></code> 、 <code class="docutils literal notranslate"><span class="pre">inf</span></code> 或 <code class="docutils literal notranslate"><span class="pre">-inf</span></code> 会出现 error 。默认：True。</p></li>
</ul>
</dd>
<dt>返回类型：</dt><dd><p>裁剪梯度范数后的参数。参数的总范数（视为单个向量）。</p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">3.7</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m1</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out1</span> <span class="o">=</span> <span class="n">m1</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out1</span> <span class="o">=</span> <span class="n">out1</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out1</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">norm1</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">norm1</span>
<span class="go">tensor(6., dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([[0.1000, 0.1000, 0.1000],</span>
<span class="go">        [0.1000, 0.1000, 0.1000]], dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out2</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">atan</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out2</span> <span class="o">=</span> <span class="n">out2</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out2</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">norm2</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">norm2</span>
<span class="go">tensor(1.0394, dtype=oneflow.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([[0.0962, 0.0481, 0.0283],</span>
<span class="go">        [0.0663, 0.4810, 0.0428]], dtype=oneflow.float32)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py function">
<dt id="oneflow.nn.utils.weight_norm">
<code class="sig-prename descclassname"><span class="pre">oneflow.nn.utils.</span></code><code class="sig-name descname"><span class="pre">weight_norm</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">T_module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'weight'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> → <span class="pre">T_module</span><a class="headerlink" href="#oneflow.nn.utils.weight_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>对给定模块中的参数应用权重归一化 (Weight Normalization)。</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\mathbf{w}=g \frac{\mathbf{v}}{\|\mathbf{v}\|}\]</div></div>
<p>权重归一化 (Weight Normalization) 是一种重新参数化 (Reparameterization)，
它将权重张量的大小从其方向解耦。此操作将用两个参数替换由 <code class="xref py py-attr docutils literal notranslate"><span class="pre">name</span></code> 指定的参数：
一个指定大小（例如 <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>），另一个指定方向（例如 <code class="docutils literal notranslate"><span class="pre">'weight_v'</span></code>)。
权重归一化 (Weight Normalization) 是通过一个 hook 实现的，该 hook 在每个
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> 调用之前从大小和方向重新计算权重张量。</p>
<p>默认情况下，当 <code class="docutils literal notranslate"><span class="pre">dim=0</span></code> 时，每个输出通道/平面独立计算范数。
要计算整个权重张量的范数，请使用 <code class="docutils literal notranslate"><span class="pre">dim=None</span></code>。</p>
<p>参见 <a class="reference external" href="https://arxiv.org/abs/1602.07868">https://arxiv.org/abs/1602.07868</a> 。本文档说明参考 Pytorch 文档：<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html">https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html</a> 。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>module</strong> (Module): 包含模块</p></li>
<li><p><strong>name</strong> (str, 可选的): 权重参数名称</p></li>
<li><p><strong>dim</strong> (int, 可选的): 计算范数的维度</p></li>
</ul>
</dd>
<dt>返回类型：</dt><dd><p>带有权重范数 hook 的原始模块</p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">weight_norm</span><span class="p">(</span><span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">'weight'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span>
<span class="go">Linear(in_features=20, out_features=40, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">weight_g</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([40, 1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">weight_v</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">oneflow.Size([40, 20])</span>
</pre></div>
</div>
</dd></dl>
<dl class="py function">
<dt id="oneflow.nn.utils.remove_weight_norm">
<code class="sig-prename descclassname"><span class="pre">oneflow.nn.utils.</span></code><code class="sig-name descname"><span class="pre">remove_weight_norm</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'weight'</span></span></em><span class="sig-paren">)</span> → <span class="pre">T_module</span><a class="headerlink" href="#oneflow.nn.utils.remove_weight_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>从模块中删除 Weight Normalization Reparameterization。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>module</strong> (Module): 包含模块</p></li>
<li><p><strong>name</strong> (str, 可选的): 权重参数名称</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">oneflow</span> <span class="kn">as</span> <span class="nn">flow</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">weight_norm</span><span class="p">(</span><span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">flow</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">remove_weight_norm</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="go">Linear(in_features=20, out_features=40, bias=True)</span>
</pre></div>
</div>
</dd></dl>
</div>
</div>

      </article>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="functional.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">oneflow.nn.functional</div>
              </div>
              <svg><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="tensor_attributes.html">
              <svg><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Tensor Attributes</div>
                
              </div>
            </a>
        </div>

        <div class="related-information">
              Copyright &#169; 2020, OneFlow
            |
            Built with <a href="https://www.sphinx-doc.org/">Sphinx</a>
              and
              <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
              <a href="https://github.com/pradyunsg/furo">Furo theme</a>.
            |
            <a class="muted-link" href="_sources/nn.rst.txt"
               rel="nofollow">
              Show Source
            </a>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            Contents
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">oneflow.nn</a><ul>
<li><a class="reference internal" href="#module-oneflow.nn">Operators for neural networks</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </main>
</div>
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="_static/scripts/main.js?digest=e931d09b2a40c1bb82b542effe772014573baf67"></script></body>
</html>